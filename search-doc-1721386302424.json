{"searchDocs":[{"title":"GraphQL Conf 2023","type":0,"sectionRef":"#","url":"/blog/graphql-conf-2023/","content":"","keywords":"","version":null},{"title":"Workshops & Talks Highlights​","type":1,"pageTitle":"GraphQL Conf 2023","url":"/blog/graphql-conf-2023/#workshops--talks-highlights","content":" Is GraphQL BFF Necessary: An electrifying discussion led by Tanmay from Hasura, as he unravels the significance of the BFF layer in the era after ReactJS. One profound takeaway? GraphQL isn't just a fleeting tactic for instant gains—it's a visionary strategy that propels businesses toward unparalleled success! Interactive GraphQL with Envoy &amp; Kubernetes: The team from solo.io showcased the magic of adding GraphQL to an envoy gateway. It's all about giving clients more power while retaining essential gateway features. The Future of Efficiency: Benjie Gillam's talk was a rollercoaster! He introduced grafast, a new GraphQL execution engine that optimizes data loading through query planning. One to watch! Rethinking Rate Limiting: Meenakshi Dhanani from Postman took us on a journey through the intricacies of rate-limiting GraphQL queries. Traditional methods? Not so effective. Enter query cost analysis! GraphQL Fusion Unveiled: Michael Staib from ChilliCream introduced GraphQL Fusion, a revolutionary approach to building distributed GraphQL APIs. The future of federating GraphQL APIs is looking bright! The Null Saga: Stephen Spalding from Netflix delved into the history of 'null' and introduced the Client Controlled Nullability proposal. A game-changer for GraphQL clients, we are definitely looking forward to this one! The Right Size for GraphQL: Theo Browne's presentation was an eye-opener. He introduced us to scenarios where tRPC might be a better fit than GraphQL. Data Load 3.0: Jens from Wundergraph talked about the massive performance gains one could potentially get by using a BFS algorithm in data loaders.  ","version":null,"tagName":"h3"},{"title":"Unconference Session: Where Everyone's a Speaker!​","type":1,"pageTitle":"GraphQL Conf 2023","url":"/blog/graphql-conf-2023/#unconference-session-where-everyones-a-speaker","content":" This was our first time to such a thing. The conference kicked off with a dynamic unconference session. Everyone in attendance brainstormed discussion topics grouped them, and then dove deep into discussions. Our table delved into the multifaceted world of &quot;Federation&quot; - merging multiple GraphQL graphs into a supergraph. The consensus? The journey towards a supergraph is filled with challenges, but with tools like the Open Federation spec and GraphQL Fusion, the future looks promising!  ","version":null,"tagName":"h3"},{"title":"Networking & Global Connections​","type":1,"pageTitle":"GraphQL Conf 2023","url":"/blog/graphql-conf-2023/#networking--global-connections","content":" One of the highlights of GraphQLConf 23 was the global representation. Meeting tech enthusiasts from the Netherlands, New Zealand, Poland, Romania, and more was truly inspiring. Special shoutout to Gerard Klijs from AxonIQ for his unique take on CQRS and GraphQL!  ","version":null,"tagName":"h3"},{"title":"Conclusion​","type":1,"pageTitle":"GraphQL Conf 2023","url":"/blog/graphql-conf-2023/#conclusion","content":" Did you miss out on some sessions? No worries! All the talks are available on the GraphQL Foundation's YouTube channel. Dive in and get inspired!  GraphQLConf 2023 was more than just a conference for us; it was an experience. Here's to the future of GraphQL and the endless possibilities it holds! 🎉 ","version":null,"tagName":"h3"},{"title":"Are Hackers Using Your Own GraphQL API Against You?","type":0,"sectionRef":"#","url":"/blog/graphql-introspection-security/","content":"","keywords":"","version":null},{"title":"Understanding GraphQL Introspection​","type":1,"pageTitle":"Are Hackers Using Your Own GraphQL API Against You?","url":"/blog/graphql-introspection-security/#understanding-graphql-introspection","content":" Imagine having a magical lens that lets you peek into the very structure of a GraphQL server. That's essentially what introspection does! It's like having a detailed map of a treasure trove, showing you every nook and cranny of the API's capabilities. This self-documenting capability is incredibly useful for developers, enabling tools like GraphiQL and GraphQL Playground to provide rich, interactive documentation and auto-completion features.  A basic introspection query might look like this:  { __schema { types { name fields { name type { name } } } } }   This query asks the server to return information about all the types in the schema, including their fields and field types. The server's response provides a comprehensive map of its structure, which can be invaluable during development.  ","version":null,"tagName":"h2"},{"title":"The Security Implications of Introspection​","type":1,"pageTitle":"Are Hackers Using Your Own GraphQL API Against You?","url":"/blog/graphql-introspection-security/#the-security-implications-of-introspection","content":" While introspection is a goldmine for developers, it can also be a treasure map for attackers. Let's put on our black hat for a moment and see how a malicious actor might exploit this feature.  ","version":null,"tagName":"h2"},{"title":"Schema Reconnaissance​","type":1,"pageTitle":"Are Hackers Using Your Own GraphQL API Against You?","url":"/blog/graphql-introspection-security/#schema-reconnaissance","content":" One of the primary risks of introspection is schema reconnaissance. An attacker who gains access to a GraphQL endpoint can use introspection to explore the schema and identify potential targets for further attacks. This includes discovering sensitive types and fields, as well as understanding the relationships between different parts of the schema. Armed with this knowledge, an attacker can craft more effective queries to exploit vulnerabilities in the system.  For instance, an attacker might discover a 'User' type with fields like 'email', 'password', and 'isAdmin'. They could then craft a query to exploit this:  query { allUsers { email password isAdmin } }   If not properly secured, this query could potentially expose sensitive user data. The attacker might also notice an 'updateUser' mutation, which could be a target for privilege escalation attempts.  ","version":null,"tagName":"h3"},{"title":"Information Disclosure​","type":1,"pageTitle":"Are Hackers Using Your Own GraphQL API Against You?","url":"/blog/graphql-introspection-security/#information-disclosure","content":" Another significant risk is information disclosure. The introspection feature can inadvertently reveal implementation details that should remain hidden. This includes internal types, deprecated fields, and administrative functionalities. Such exposure can give attackers clues about the underlying system architecture and any potential weaknesses.  ","version":null,"tagName":"h3"},{"title":"Attack Surface Expansion​","type":1,"pageTitle":"Are Hackers Using Your Own GraphQL API Against You?","url":"/blog/graphql-introspection-security/#attack-surface-expansion","content":" By using introspection, attackers can significantly expand their attack surface. They can identify entry points for various attacks, including SQL injection, cross-site scripting (XSS), and denial of service (DoS) attacks. For instance, if introspection reveals that certain fields accept user input, an attacker might probe these fields for injection vulnerabilities.  ","version":null,"tagName":"h3"},{"title":"Mitigating Introspection Risks​","type":1,"pageTitle":"Are Hackers Using Your Own GraphQL API Against You?","url":"/blog/graphql-introspection-security/#mitigating-introspection-risks","content":" Now, let's switch gears and become the defenders of our GraphQL realm. Here are some battle-tested strategies to keep your API safe from prying eyes:  ","version":null,"tagName":"h2"},{"title":"Disable Introspection in Production​","type":1,"pageTitle":"Are Hackers Using Your Own GraphQL API Against You?","url":"/blog/graphql-introspection-security/#disable-introspection-in-production","content":" Disabling introspection in production is crucial because it significantly reduces the information available to potential attackers. Without introspection, they can't easily map out your API's structure or discover hidden fields and types. This forces attackers to rely on guesswork or prior knowledge, making their job much more difficult. However, it's important to note that this is not a silver bullet—determined attackers may still attempt to reverse-engineer your API through trial and error.  In many GraphQL implementations, disabling introspection is straightforward. For example, in Tailcall, you can disable introspection by setting the introspection option to false:  schema @server(introspection: false) { query: Query mutation: Mutation }   This configuration ensures that introspection is disabled.  ","version":null,"tagName":"h3"},{"title":"Implement Authentication and Authorization​","type":1,"pageTitle":"Are Hackers Using Your Own GraphQL API Against You?","url":"/blog/graphql-introspection-security/#implement-authentication-and-authorization","content":" Another critical measure is to implement robust authentication and authorization mechanisms. By ensuring that only authenticated and authorized users can access your GraphQL endpoint, you can reduce the risk of unauthorized introspection queries. Use industry-standard authentication protocols such as OAuth2 or JWT to secure your endpoints.  Imagine a GraphQL API for a banking application. You might implement role-based access control where only users with an 'ADMIN' role can access certain fields or mutations.  In Tailcall, you can achieve this by using the @protected directive.  Tailcall supports a variety of authentication and authorization mechanisms, including JWT, OAuth2, and custom authentication strategies.  This ensures that even if an attacker gains access to a regular user account, they can't use it to access sensitive admin-only data or operations.  ","version":null,"tagName":"h3"},{"title":"Rate Limiting and Throttling​","type":1,"pageTitle":"Are Hackers Using Your Own GraphQL API Against You?","url":"/blog/graphql-introspection-security/#rate-limiting-and-throttling","content":" Rate limiting and throttling can also help mitigate the risks of introspection. By limiting the number of queries a client can execute within a given timeframe, you can reduce the likelihood of an attacker using introspection to gather information about your schema. Implementing these controls can also help protect your server from DoS attacks.  ","version":null,"tagName":"h3"},{"title":"Query Allow Lists​","type":1,"pageTitle":"Are Hackers Using Your Own GraphQL API Against You?","url":"/blog/graphql-introspection-security/#query-allow-lists","content":" Query allow lists work by pre-registering all valid queries that your application needs. This is typically done during the build process of your frontend application. Each query is hashed, and these hashes are stored on the server. When a query comes in, its hash is checked against the allow list.  For example, you might have a client-side query like this:  query GetUserProfile($id: ID!) { user(id: $id) { name email } }   This query would be hashed and stored on the server. When executed, the server checks if the incoming query's hash matches any in its allow list. If not, it's rejected.  This approach is powerful because it completely prevents arbitrary queries, including introspection queries, from being executed. It does require more setup and maintenance, especially in applications where queries change frequently, but it provides a very high level of security.  ","version":null,"tagName":"h3"},{"title":"Monitor and Log Introspection Queries​","type":1,"pageTitle":"Are Hackers Using Your Own GraphQL API Against You?","url":"/blog/graphql-introspection-security/#monitor-and-log-introspection-queries","content":" Monitoring and logging introspection queries can provide valuable insights into potential security threats. By tracking when and how introspection queries are executed, you can identify suspicious activity and respond accordingly. Implement logging at both the application and network levels to capture detailed information about each query.  ","version":null,"tagName":"h3"},{"title":"Use a Web Application Firewall (WAF)​","type":1,"pageTitle":"Are Hackers Using Your Own GraphQL API Against You?","url":"/blog/graphql-introspection-security/#use-a-web-application-firewall-waf","content":" A WAF can be particularly effective for GraphQL APIs because it can be configured to understand GraphQL-specific threats. For instance, you can set up rules to:  Limit query depth: Prevent deeply nested queries that could overload your server.Restrict field counts: Avoid overly broad queries that request too many fields at once.Block known malicious patterns: Such as attempts to inject malicious code into queries.  For example, a WAF rule might look like this:  SecRule ARGS_POST:query &quot;@contains __schema&quot; \\ &quot;id:1000,\\ phase:2,\\ t:none,\\ block,\\ msg:'GraphQL introspection query detected'&quot;   This rule would block any POST request containing '__schema' in the query parameter, which is typically indicative of an introspection query.  By implementing these kinds of rules, a WAF adds an extra layer of protection, catching many potential attacks before they even reach your GraphQL server.  ","version":null,"tagName":"h3"},{"title":"Conclusion​","type":1,"pageTitle":"Are Hackers Using Your Own GraphQL API Against You?","url":"/blog/graphql-introspection-security/#conclusion","content":" Securing GraphQL is like playing a high-stakes game of chess. You need to think several moves ahead, anticipating potential threats while leveraging the strengths of your position. By implementing these strategies, you're not just protecting your API—you're ensuring that GraphQL's power remains in the right hands. Stay vigilant, keep learning, and may your queries be ever secure!  By prioritizing security in your GraphQL implementation, you can harness the power of this modern query language while safeguarding your data and maintaining the trust of your users. Securing GraphQL is an ongoing process that requires vigilance and a proactive approach. Stay informed about the latest security developments, regularly review and update your security measures, and ensure that your development and security teams are aligned in their efforts to protect your applications. ","version":null,"tagName":"h2"},{"title":"GraphQL vs REST vs gRPC - an unfair comparison","type":0,"sectionRef":"#","url":"/blog/graphql-vs-rest-vs-grpc/","content":"Since its inception, GraphQL has steadily gained popularity, often finding itself at the center of comparisons with other data query and manipulation languages such as REST and gRPC. The internet is replete with articles debating the merits and demerits of each, with some even questioning the viability of GraphQL. However, this discourse misses a crucial point: the unique strengths of GraphQL. This article aims to illuminate the distinct advantages GraphQL offers, particularly in addressing a common but complex challenge known as impedance mismatch. Impedance mismatch refers to the discordance between the capabilities of an existing API and the ideal features required for a specific use case. From the perspective of a platform engineer, the goal is to develop APIs that cater to a broad range of needs. Yet, crafting a unique API for every conceivable requirement is neither practical nor efficient. Consequently, engineers often end up creating generalized APIs. However, as a consumer, you might find these APIs lacking in some respects while being superfluous in others. Furthermore, as your needs evolve, so does your notion of the ideal API, exacerbating this mismatch. Herein lies the brilliance of GraphQL: it offers a framework for structuring data exposure and queries that significantly mitigates this issue. The GraphQL specification introduces the concept of viewing data as a graph composed of nodes, which represent domain entities for a business, interconnected by relationships that define their interactions. For instance, in the development of a social network, a user entity might have the ability to create a post, which in turn could receive comments, illustrating the interconnected nature of data entities. With the data conceptualized as a graph, GraphQL advocates for a method of querying that allows for precise data retrieval. This selective querying capability enables developers to request exactly the data they need, distinguishing GraphQL from REST and gRPC which aren't truly a &quot;queryable&quot;. The precision of GraphQL extends to the granularity of specifying individual fields within entities, facilitating extremely efficient and targeted queries. Notably, the GraphQL specification does not prescribe any specific data storage methodologies but focuses on the manner in which data is queried, hence the designation &quot;Graph Query Language.&quot; This approach allows for queries tailored to specific requirements, such as obtaining posts by the current user along with comments on those posts. By enabling precise data queries, GraphQL helps in avoiding the inefficiencies associated with over-fetching or under-fetching data, thereby enhancing overall system performance. The impedance mismatch is not solely a technical issue pertaining to the differences in API schemas. It extends into the realm of development processes as well. GraphQL significantly ameliorates this aspect by allowing the consumers of an API to begin their work even before the actual API is fully implemented. This is made possible through the agreement on a schema upfront. By decoupling the dependency between the consumer and the provider of the API, GraphQL facilitates a more efficient and flexible development process. Comparing GraphQL with REST or gRPC on this front might not do justice to their distinct objectives. REST and gRPC are primarily designed as lightweight RPC protocols, not specifically to address impedance mismatch for which a full fledged query language is more suitable. A more apt comparison would be with OpenAPI, which also allows for API composition. However, OpenAPI's capabilities in fine-tuning what an API delivers are somewhat constrained compared to GraphQL's flexible querying capabilities. Beyond the technical resolution of impedance mismatch, GraphQL addresses a critical business problem: the inefficiency in software development that arises from this gap between actual and ideal APIs. This inefficiency leads to developers spending excessive time on API orchestration—time that could be better spent on core application development. They find themselves constantly writing, revising, and optimizing APIs and their orchestration, as well as managing the fallout from breaking changes. By leveraging GraphQL, developers can significantly reduce these frictions, streamlining the development process and enhancing productivity. In essence, GraphQL not only solves a technical problem but also delivers substantial business value by enabling more efficient and flexible software development practices. GraphQL offers an excellent developer experience for API consumption with its intuitive query language that allows for retrieving deeply nested data independently of the upstream source. However, it does have some limitations. At Tailcall, we are dedicated to making GraphQL more accessible and easier to work with. If you like what you just read, please do subscribe and share on twitter and linkedin 🙏","keywords":"","version":null},{"title":"No one talks about API Orchestration","type":0,"sectionRef":"#","url":"/blog/no-one-talks-about-api-orchestration/","content":"","keywords":"","version":null},{"title":"Microservice​","type":1,"pageTitle":"No one talks about API Orchestration","url":"/blog/no-one-talks-about-api-orchestration/#microservice","content":" Microservices architecture is a design pattern in which a large application is built as a suite of modular services, each of which runs its process and communicates with other services through well-defined interfaces, typically using a lightweight messaging protocol. This approach has several benefits over a monolithic architecture, including improved scalability, resilience, and maintainability. In a microservices architecture, each service has a specific role and is independently deployable, so developers can work on different services in parallel and deploy them independently of each other. This can make the development process more agile and allow for faster deployment of new features.    An API gateway is a server that acts as a single point of entry for certain types of requests. It can receive requests from the client, route them to the appropriate backend service, and then return the response from the backend service to the client. An API gateway can also perform tasks such as authentication, rate limiting, and caching. This makes it a useful component in a microservices architecture, where each service has its API and the API gateway acts as the &quot;front door&quot; for clients to access the services.  ","version":null,"tagName":"h2"},{"title":"API Composition​","type":1,"pageTitle":"No one talks about API Orchestration","url":"/blog/no-one-talks-about-api-orchestration/#api-composition","content":" API composition refers to the process of combining multiple APIs to create a new API or a new functionality. This can be done by sending requests to multiple APIs and combining the results, or by creating a new API that acts as a façade for the underlying APIs.  💡 API Composition is also known as API Orchestration. This is however vastly different from Microservice Orchestration.  For example, consider a scenario where a client application wants to display a user's profile information and recent posts on a social media platform. In this case, the client can send two separate requests to two different APIs: one to retrieve the user's profile information, and another to retrieve their recent posts. The client can then combine the results from these two APIs to create a single response that contains all the required information. This new response can be considered as the output of the composed API.  To build a rich user interface, API composition is necessary on the client side. One of the main challenges with API composition on the client side is that it can lead to increased complexity in the client application. This is because the client needs to handle the process of sending requests to multiple APIs and combining the results, which can add to the overall size and complexity of the client code.  Another challenge with API composition on the client side is that it can result in reduced performance and increased latency. This is because the client needs to make multiple separate requests to different APIs, which can take more time and result in a slower response from the composed API.  In addition, API composition on the client side can also lead to increased security risks. This is because the client needs to handle sensitive information, such as API keys and authentication credentials, which can be vulnerable to attacks if not properly secured. The client doesn't have access to powerful CPUs or a reliable network either. This makes the composition problem even more challenging to implement and manage. It is therefore often more efficient and effective to perform API composition on the server side instead.  ","version":null,"tagName":"h2"},{"title":"Backend For Frontend​","type":1,"pageTitle":"No one talks about API Orchestration","url":"/blog/no-one-talks-about-api-orchestration/#backend-for-frontend","content":" A BFF layer can help to solve the challenges of API composition by providing a separate backend service that is optimized for each specific frontend client. This can enable the BFF to perform API composition on behalf of the client, which can help to improve the performance and reliability of the composed API. The BFF layer typically sits as a separate component in the overall architecture, between the frontend client and the microservices. It can communicate with both the frontend client and the microservices using well-defined interfaces and protocols, such as REST or gRPC.  The BFF can take advantage of a powerful CPU and access to a fast network to improve the performance and reliability of the composed API. It can also provide added flexibility and control over the composition process. This can make it a useful tool for developers who want to create new APIs by combining the functionality of multiple underlying APIs.    BFFs truly solve the problems mentioned above to a great extent, however they introduce new set of challenges viz.  ","version":null,"tagName":"h2"},{"title":"Highly Specialized​","type":1,"pageTitle":"No one talks about API Orchestration","url":"/blog/no-one-talks-about-api-orchestration/#highly-specialized","content":" One of the challenges with using a BFF layer is that it is a highly specialized solution that requires a significant amount of hand-written code. Unlike an API gateway, there is no standard BFF solution that can be deployed out-of-the-box, and each BFF implementation must be custom-tailored to the specific requirements of the frontend client. This lack of standardization and reusability can make the BFF solution more complex and difficult to maintain.  ","version":null,"tagName":"h3"},{"title":"Fragile​","type":1,"pageTitle":"No one talks about API Orchestration","url":"/blog/no-one-talks-about-api-orchestration/#fragile","content":" Another challenge with using a BFF layer is that it can be fragile and susceptible to failure. The BFF solution is dependent on the developers to follow best practices and handle all error scenarios, and if these steps are not taken, the solution can be prone to bugs and performance issues. Additionally, the BFF solution must be thoroughly tested, including performance testing, unit testing, and integration testing, to ensure that it is reliable and performs well in production. This can require significant effort and expertise, and if these steps are not properly followed, the BFF solution can be fragile and prone to failure. Also, it's worth mentioning that a BFF layer is an entry point to all your backend, it going down basically means nothing is accessible for the user so this layer needs to be robust and resilient to exceptions.  ","version":null,"tagName":"h3"},{"title":"Performance​","type":1,"pageTitle":"No one talks about API Orchestration","url":"/blog/no-one-talks-about-api-orchestration/#performance","content":" Because BFF layers are typically custom-written for each use case, it can be difficult to predict the performance impact of a small code change. Issues such as unoptimized algorithms, inefficient caching, and unnecessary downstream requests can go unnoticed and only be discovered very late in the development cycle. Typically companies perform thorough benchmarking and load testing before anything goes live. This results in a very high time to market even for minor changes.  ","version":null,"tagName":"h3"},{"title":"Monolith​","type":1,"pageTitle":"No one talks about API Orchestration","url":"/blog/no-one-talks-about-api-orchestration/#monolith","content":" Eventually, this layer turns out to be a big monolith touching every service in your backend. The layer contains a lot of handwritten spaghetti code that's hard to maintain. Onboarding new engineers also becomes harder and upgrading libraries or architecture gets costlier. Any tiny change requires a full-fledged deployment on your infrastructure.  ","version":null,"tagName":"h3"},{"title":"Canary Support (or lack thereof)​","type":1,"pageTitle":"No one talks about API Orchestration","url":"/blog/no-one-talks-about-api-orchestration/#canary-support-or-lack-thereof","content":" Every change that happens in the backend requires the deployment of the BFF layer. In fact, any feature that is built on the client also requires changes on the BFF layer. Such frequent changes can not be exposed to 100% of users because the reliability and performance of this system are unknown. A common way to solve this problem is to use blue-green deployments. This requires additional infrastructure and complex routing mechanisms. First-class support to do canary releases is very important and should be part of a modern BFF layer, however, most companies rely on DevOps for its support.  ","version":null,"tagName":"h3"},{"title":"Coupled Release​","type":1,"pageTitle":"No one talks about API Orchestration","url":"/blog/no-one-talks-about-api-orchestration/#coupled-release","content":" BFF layers can't be deployed independently since they act as a bridge between the clients and the services. Generally, the services need to go live first, and they need to make sure that the change is compatible with the current version of the BFF layer running in production. The interesting problem is in case there is a bug in the microservice and it needs to be reverted, even the BFF layer needs to be reverted. This kind of coupling makes it operationally very expensive to manage.  ","version":null,"tagName":"h3"},{"title":"Organizational Friction​","type":1,"pageTitle":"No one talks about API Orchestration","url":"/blog/no-one-talks-about-api-orchestration/#organizational-friction","content":" The Backends for Frontends (BFF) pattern is designed to create a tailor-made backend service for each user interface (e.g., desktop, mobile, etc.), with the aim of simplifying the client-side and improving the user experience.  However, in practice, this architecture sometimes creates friction within the organization, particularly when BFFs are developed and maintained by the backend team. Here are a few reasons why:  Communication and Responsiveness: As the backend team is typically in charge of the BFF, front-end teams often have to wait for them to make necessary changes. This slows down the development process, especially when backlogs are high or priorities differ. Different Skillsets: Backend and frontend developers often specialize in different programming languages and paradigms. If the backend team is in charge of the BFF, they might not be as comfortable or efficient at dealing with issues that are more closely related to the frontend. Lack of Ownership: Frontend teams often feel that they lack ownership and control over the part of the system that directly impacts their work. This leads to decreased motivation and productivity.  One potential solution to these issues is to shift the ownership of the BFFs to the front-end teams. Since these teams are the primary consumers of the BFFs, they could be better placed to design, implement, and maintain them. This would not only empower the front-end teams but also free up backend teams to focus on their core responsibilities.  However, this solution is not without its own challenges. For one, front-end teams would need to upskill to handle their new responsibilities. Also, the organization would need to ensure that there are clear lines of communication between the front-end and backend teams, so that any changes to shared resources can be coordinated effectively.  ","version":null,"tagName":"h3"},{"title":"Legacy Gateway​","type":1,"pageTitle":"No one talks about API Orchestration","url":"/blog/no-one-talks-about-api-orchestration/#legacy-gateway","content":" BFF layers often end up implementing some of the cross-cutting concerns of an API gateway such as rate limiting, authentication, throttling, etc. This makes its purpose quite confusing in the sense that do we need an API gateway if we are using a BFF layer. Moreover, it's not very clear if we use an API gateway with a BFF layer, where should we place it? Should we place it between the clients and the BFF layer or the BFF layer and the service mesh? These are subjective decisions that each company ends up making as there is no standard way of doing this. However, it's worth mentioning that legacy gateways do introduce a gap that's being attempted to be filled by a BFF layer.  BFF, Presentation Layer, Facade, Middleware, UI Layer, Orchestration Layer, API Adapter — Are all different nomenclatures used for the same thing.  To summarize, BFFs do indeed address the issues of API orchestration to a significant extent; however, they also present a new set of challenges for organizations to tackle. Clearly, there is more to the story. In our next blog post, we will discuss some of the solutions that large organizations with unlimited budgets have implemented to overcome this problem. So, please subscribe if you haven't already. ","version":null,"tagName":"h3"},{"title":"The truth about scaling Automatic Persisted Queries","type":0,"sectionRef":"#","url":"/blog/the-truth-about-scaling-automatic-persisted-queries/","content":"","keywords":"","version":null},{"title":"The Problem​","type":1,"pageTitle":"The truth about scaling Automatic Persisted Queries","url":"/blog/the-truth-about-scaling-automatic-persisted-queries/#the-problem","content":" Large Queries​  Clients send queries to a GraphQL server as HTTP requests that include the query as the body. When these queries become large, they can lead to increased latency and network usage, degrading client performance.  For example, a normal GraphQL query might look like this:  curl -X POST -H &quot;Content-Type: application/json&quot; \\ --data '{&quot;query&quot;: &quot;{ largeQuery { field1 field2 ... } }&quot;}' \\ http://your-graphql-server.com/graphql   Each GraphQL query is parsed every time the server receives it. If it's large, the parsing can take a significant amount of time, increasing latency even further.  Legacy Infrastructure​  Existing CDN infrastructure is designed to cache only GET calls. To make a GraphQL request, one must make a POST call. This limits the usage of CDNs for caching purposes.  ","version":null,"tagName":"h3"},{"title":"Solution: Persisted Queries (PQ)​","type":1,"pageTitle":"The truth about scaling Automatic Persisted Queries","url":"/blog/the-truth-about-scaling-automatic-persisted-queries/#solution-persisted-queries-pq","content":" Definition and Benefits​  To enhance network performance for large query strings, GraphQL server supports Persisted Queries (PQ). A PQ is a GraphQL query cached server-side, identified by its SHA-256 hash. Clients send this identifier instead of the query, dramatically reducing request sizes (without affecting response), saving parsing time, and enabling GET calls instead of POST.  A PQ request might look like this:  curl -X GET -H &quot;Content-Type: application/json&quot; \\ --data-urlencode 'extensions={&quot;persistedQuery&quot;:{&quot;version&quot;:1,&quot;sha256Hash&quot;:&quot;&lt;SHA 256&gt;&quot;}}' \\ http://your-graphql-server.com/graphql   Application with CDNs​  Using the PQ link automatically sends short hashed queries as GET requests, enabling CDNs to serve them.  Latency Reduction​  No Parsing Overhead: Since the query isn't sent to the server, the parsing stage, which can be computationally expensive, is eliminated. This saves valuable server processing time, directly reducing client latency. Network Efficiency: By transmitting only the hash instead of the full query, the request size is dramatically reduced, leading to faster network transmission and lower latency.  Security Enhancements​  Control Over Allowed Queries: The server can start with a finite set of &quot;allowed&quot; queries, ensuring that unauthorized or unoptimized GraphQL requests cannot be made. This control is a significant safeguard for production environments, preventing potential abuse or inefficiencies. Reduction in Attack Surface: By limiting the queries to a pre-defined set, the risk of malicious queries is reduced, enhancing the security profile of the application.  Problem​  While PQs provide remarkable benefits, they are not without challenges:  Schema Rigidity: If you aim to keep the schema open and queries dynamic, supporting any possible query becomes complex. Maintenance of Cached Queries: Managing the cache of allowed queries and keeping them in sync with evolving client needs can become a maintenance burden, especially in a fast-changing environment.  ","version":null,"tagName":"h3"},{"title":"Automatic Persisted Queries (APQs)​","type":1,"pageTitle":"The truth about scaling Automatic Persisted Queries","url":"/blog/the-truth-about-scaling-automatic-persisted-queries/#automatic-persisted-queries-apqs","content":" APQs vs PQs​  APQs are a supposed improvement over PQs. In a PQ setup, the server runs with a known set of queries, meaning client changes require server updates. This has implications for maintenance costs, particularly in supporting multiple versions of queries and making a server deployment for every change in the client query. APQs were introduced to overcome these challenges.  How APQs Work​  The APQ process is a two-step approach:  Hash Request: The client sends a request with the hash of the query. If the server recognizes the hash, it returns the corresponding response: curl -X GET -H &quot;Content-Type: application/json&quot; \\ --data-urlencode 'extensions={&quot;persistedQuery&quot;:{&quot;version&quot;:1,&quot;sha256Hash&quot;:&quot;&lt;SHA 256&gt;&quot;}}' \\ http://your-graphql-server.com/graphql Full Query Request: If the server does not recognize the hash, it returns an error. The client then sends a new request that includes both the hash and the full query string: curl --get http://localhost:4000/graphql \\ --header 'content-type: application/json' \\ --data-urlencode '{&quot;query&quot;: &quot;{ largeQuery { field1 field2 ... } }&quot;}' \\ --data-urlencode 'extensions={&quot;persistedQuery&quot;:{&quot;version&quot;:1,&quot;sha256Hash&quot;:&quot;&lt;HASH&gt;&quot;}}' The server parses the full query, caches it for future use, and returns the GraphQL response. Subsequent requests use the hash.  This process optimizes network performance while allowing flexibility in the queries that can be run. You can read more about APQ here  ","version":null,"tagName":"h3"},{"title":"Problems with APQs​","type":1,"pageTitle":"The truth about scaling Automatic Persisted Queries","url":"/blog/the-truth-about-scaling-automatic-persisted-queries/#problems-with-apqs","content":" Thundering Herd Problem​  Consider a situation where a server has just been deployed or restarted, and the cache is empty. Now, multiple clients send hash requests for queries that are not yet cached.  Massive Error Responses: Since the cache is empty, the server returns errors for all hash requests, signaling the clients to send the full query strings. Simultaneous Full Query Requests: All clients now simultaneously send full query requests, causing a sudden surge in demand. Server Strain: The server must parse and cache each unique query, placing significant strain on its resources. This can lead to increased latency and even server failure if the demand is too high. Repeated Pattern: If the server struggles to cache the queries quickly enough, the clients may continue to receive errors and retry the full query requests, perpetuating the problem.  In an environment with many clients and dynamically changing queries, the system can become vulnerable to sudden surges in demand. This vulnerability can undermine the performance benefits APQs are designed to provide, leading to potential system instability.  Cache Limitations​  Queries are typically cached in memory, requiring cache warmup on each instance, hindering deployment on server-less solutions. An alternative could be using a centralized cache, but it typically nullifies performance gains due to serialization, deserialization, and IO call overhead.  Security Concerns​  Automatically persisting queries can cause memory leaks, as clients can send varying query combinations, exhausting server memory. Mitigation through cache size limits and eviction mechanisms may lead to frequent cache misses, leading to doubling request numbers.  ","version":null,"tagName":"h3"},{"title":"Possible Solution​","type":1,"pageTitle":"The truth about scaling Automatic Persisted Queries","url":"/blog/the-truth-about-scaling-automatic-persisted-queries/#possible-solution","content":" Persistent queries are a great improvement over regular queries. They clearly improve performance and are more secure. APQs on the other hand though try to give more flexibility they can become quite messy to deal with as you scale. One alternative that is significantly more effective, is to run GraphQL on Edge itself. Essentially write your own CDN layer that is smart enough to understand that it's a graphQL and deploy it on edge with caching and whatnot! This is hard, and that's exactly what Tailcall helps solve.  ","version":null,"tagName":"h3"},{"title":"Conclusion​","type":1,"pageTitle":"The truth about scaling Automatic Persisted Queries","url":"/blog/the-truth-about-scaling-automatic-persisted-queries/#conclusion","content":" Automatic persisted queries, while offering some advantages in network performance, reveal significant challenges when it comes to scaling. The complexities of caching, potential security risks, and the inherent problems with automatic persistence highlight that persisted queries may not be the one-size-fits-all solution they are often portrayed as.  The question of whether to implement PQ or APQ must be approached with caution, taking into account the specific requirements and potential scalability issues of your system. While they may serve as a useful tool in certain scenarios, understanding the limitations and conducting thorough analysis is vital to avoid falling into the trap of a solution that doesn't truly scale. This blog post has aimed to shed light on these complexities, encouraging a more nuanced perspective on a topic that is often oversimplified. ","version":null,"tagName":"h3"},{"title":"Unraveling the Challenges of BFF Federation","type":0,"sectionRef":"#","url":"/blog/unraveling-the-challenges-of-bff-federation/","content":"","keywords":"","version":null},{"title":"Using a BFF Federation​","type":1,"pageTitle":"Unraveling the Challenges of BFF Federation","url":"/blog/unraveling-the-challenges-of-bff-federation/#using-a-bff-federation","content":" Federation, as a concept, is not exclusive to GraphQL. In essence, it's about abstracting multiple data sources or services into a unified, single API interface that can be consumed by clients. This approach is not unique to any particular technology or framework and can be implemented with various tools and languages.  However, GraphQL has played a significant role in popularizing the concept of federation. With its strong typing, introspective capabilities, and its natural fit for defining schemas across distributed services, it has provided an elegant solution to the challenge of federating APIs.  While this blog discusses federation in the context of GraphQL, it's essential to understand that the core principles and challenges of the federation can be applied beyond GraphQL. Remember, the implementation of federation is not about a specific technology, but about the architectural approach to create a unified interface from multiple data sources.  With this in mind, let's delve into the pros and cons of the federation, using GraphQL as our main context for the discussion. As you'll see, the benefits and pitfalls of federation are relevant, whether you're using GraphQL or not.  Federation is a concept that originates from the philosophy of microservices. This approach promotes the partitioning of large monolithic systems into smaller, more manageable components. In a federated architecture, instead of having a monolithic Backend-for-Frontend (BFF) handling all requests, you have multiple smaller BFFs that handle different aspects of the request.  Imagine a client makes a request to your system. This request still goes through an API gateway, which serves as the entry point to your system. However, instead of hitting a monolithic BFF, it now meets a BFF Router, specifically designed to understand and route requests to the appropriate BFFs.  The Router is smart. It understands the client's request and can break it down into smaller parts. It then delegates these smaller tasks to the appropriate services, each responsible for a specific aspect of the request. These services work in parallel, handling their part of the request, which often involves calling downstream microservices and orchestrating their responses.  Once the BFFs have finished their tasks, they send their responses back to the Router. The Router, in turn, takes these individual responses, combines them into a single response that fulfils the original request, and sends it back to the client.  This system, where individual services handle specific parts of a request in a coordinated manner, is often referred to as a Federation. The term &quot;Apollo Federation&quot; or &quot;Super Graph&quot; is commonly used to describe this setup when it's implemented with Apollo, a popular GraphQL implementation, but the concept is not limited to any specific technology or tool.    ","version":null,"tagName":"h2"},{"title":"Federation Benefits​","type":1,"pageTitle":"Unraveling the Challenges of BFF Federation","url":"/blog/unraveling-the-challenges-of-bff-federation/#federation-benefits","content":" Many large organizations using GraphQL in production have transitioned to this architecture to accommodate their scaling needs. The primary selling points of this architecture are:  Enhanced Team Ownership: GraphQL Federation fosters a sense of ownership among teams by allowing each team to own and maintain its GraphQL service. With Federation, teams can operate independently, focusing on their specific domain without worrying about the overall schema. This separation of concerns leads to more maintainable code, faster development cycles, and increased productivity. It empowers teams to work in parallel, each owning a piece of the larger schema while ensuring that the entire system operates as a cohesive whole. This significantly enhances team efficiency and collaboration, particularly in larger organizations with multiple teams working on different services. This alone is by far the most significant aspect of using GraphQL Federation. Incremental Adoption: A major advantage of GraphQL Federation is its ability to support incremental adoption. This means teams can gradually wrap their domain-specific microservices with a GraphQL layer, one at a time, and integrate it into the federated schema without disrupting the entire system. This flexible approach minimizes the impact on existing workflows and reduces the risks associated with large-scale changes. From the frontend perspective, GraphQL Federation offers a unified interface for querying the data. This simplifies the frontend code and enables the development of rich, interactive UIs with less effort. As soon as the first services are federated, frontend developers can begin transitioning their queries to the federated schema, reducing disruption and allowing for a smoother adoption process. This incremental approach also allows teams to evaluate and demonstrate the value of federation at each step, building confidence and promoting buy-in across the organization. It ensures teams are not overwhelmed by the complexity of new technology or architecture and can adjust their practices as they learn.  ","version":null,"tagName":"h2"},{"title":"Federation vs BFF​","type":1,"pageTitle":"Unraveling the Challenges of BFF Federation","url":"/blog/unraveling-the-challenges-of-bff-federation/#federation-vs-bff","content":"   It's not hard to see that GraphQL Federation carries some serious muscle over its monolithic adversary, the BFF. But before we declare a champion, let's take a few rounds to scrutinize the limitations we've come across in our BFF solution, and see how the GraphQL Federation stands up under pressure. It's time for a head-to-head comparison!  Specialization: Both BFF and GraphQL Federation require a certain amount of manual intervention. In the BFF approach, the entire layer is custom-built, meaning there's no ready-to-use solution, which necessitates significant manual management. On the other hand, GraphQL Federation provides an open-source, ready-to-use Apollo Router. However, it's not an all-inclusive solution, as the individual GraphQL services still need to be manually maintained and written by hand for specific use cases. While it's still a tough fight, the federation manages to land a jab here and gets a few extra points in this round. Fragility: Federation offers an enhanced strategy. In a federated architecture, when a GraphQL service malfunction, only its segment of the graph becomes inaccessible to the user. This results in a more resilient system, less prone to total failure, demonstrating the ability to continue the fight even after taking a hit. In this round, Federation steps up and delivers a solid punch. Performance: When it comes to Performance, the Federation architecture introduces an extra journey for each packet. The request has to travel through the router, then to the individual GraphQL services, before finally reaching the domain service. This journey can add a few milliseconds of latency, a difference that may not be perceptible to the end user. However, this seemingly small delay has a broader impact on the performance of the entire architecture. The addition of the router introduces a requirement for more infrastructure and increases the frequency of data serialization and deserialization. This increased complexity affects both the system's throughput and infrastructure costs. In this round, the Federation architecture might not be the clear knockout winner we were hoping for. Monolith Tendency: It's a clear knockout in this round for the Federation! It elegantly sidesteps the monolith tendency, keeping the architecture agile and modular. BFF, in contrast, takes a heavy fall with its tendency to become a monolithic layer over time. Canary Release: Federation, unlike BFF, reveals graph dependencies and compatibility issues at runtime, not compile time. This amps up the need for first-class canary releases. However, when it comes to canary support, it's a draw. Both fighters are still in the ring, each showing resilience in their own way. No knockout here, folks! Coupled Release: In the Federation architecture, each GraphQL service operates independently, significantly reducing the coupling between services and the router. This independence allows each team to manage its own release cycles, putting an end to the wide-scale halts that were commonplace with the BFF layer. However, it's important to note that each GraphQL service still maintains a tight connection with its corresponding downstream domain service. While this is a form of coupling, it's considerably less invasive than the BFF approach, where the entire layer was intertwined. Despite this necessary connection to the domain services, the Federation architecture proves to be more agile. In this round, GraphQL Federation edges out the BFF approach. Organizational Friction: With Federation, the responsibility for managing the architecture often falls squarely on the shoulders of backend engineers. The reason is simple: the complexity of the architecture and its intimate ties with downstream domain services necessitate a deep level of technical understanding. This is a departure from the BFF paradigm, where frontend teams could claim ownership of this part of the infrastructure. The intricate nature of the Federation, however, makes this almost untenable. When put head-to-head with BFFs, the Federation seems to take a step back in this regard. The power to control abstraction slips away from the consumer. In this round, BFFs manage to hold their ground. Legacy Gateway: Just as in the BFF model, the API gateway maintains its place in the Federation architecture, and rightfully so. However, we find ourselves reestablishing a substantial amount of resiliency and caching logic within these new layers, duplicating efforts previously expended on a traditional gateway. This redundancy marks a lack of efficiency in the Federation approach, signaling a tie in this round.  Overall, this round favors GraphQL Federation. It proves to be a significantly more robust architecture when evaluated based on the aforementioned criteria. Let's explore further and assess how it performs in isolation and as the company expands.  ","version":null,"tagName":"h2"},{"title":"Pitfalls of GraphQL Federation​","type":1,"pageTitle":"Unraveling the Challenges of BFF Federation","url":"/blog/unraveling-the-challenges-of-bff-federation/#pitfalls-of-graphql-federation","content":" While GraphQL Federation has numerous benefits, it is not without its downsides. Here are some points of caution that should be considered when deciding to use this architecture:  Cost and Complexity: GraphQL Federation introduces significant complexity into the architecture. Setting up, maintaining, and testing a federated graph can be a challenge as it requires a deep understanding of both GraphQL and distributed systems. Additionally, this architecture demands more infrastructure and a larger team for maintenance. As such, the adoption and migration process can be complex and costly. Typically, only large organizations with platform teams, robust budgets, and a governing body to maintain the schema's sanctity and system reliability, tend to consider adopting this approach. The inherent complexity and cost implications make the Federation a less likely choice for small to medium-sized organizations. Ownership Challenges: It is often argued that domain service owners maintain their individual GraphQL layers. However, this doesn't always reflect the reality. Services can frequently be divided or merged, leading to uncertainty about how to modify the GraphQL layer. This results in a complex web of requests from GraphQL services to domain services outside the team's control. Infrastructure Scaling: Each subgraph in a federated architecture operates on a separate piece of infrastructure, scaling independently. This brings its own set of challenges. For instance, when a subgraph is divided or merged, computing and scaling requirements need to be re-evaluated. Moreover, a deployment in another subgraph can trigger a substantial increase in load from the router on your subgraph, potentially causing unexpected stress on your infrastructure. This underscores the need for robust scaling and load-balancing strategies within a federated architecture.  While GraphQL Federation has the potential to solve some issues of traditional BFF architecture, it brings in its own set of challenges. Therefore, it's important to evaluate these considerations based on the specific requirements and constraints of your project before deciding to implement this architecture.  ","version":null,"tagName":"h2"},{"title":"We are onto something​","type":1,"pageTitle":"Unraveling the Challenges of BFF Federation","url":"/blog/unraveling-the-challenges-of-bff-federation/#we-are-onto-something","content":" When examining the underlying issue, the debate essentially revolves around microservices and monoliths. Undoubtedly, the federated solution offers better scalability compared to a monolithic architecture; however, it also introduces a myriad of distinct challenges related to maintenance and costs that warrant careful consideration. This is not the end of the discussion, as client requests pass through CDNs and the Gateway before reaching the router, and we have yet to explore those components of the infrastructure. In the following sections, we will delve into these components and further investigate how they interact, as well as delve deeper into GraphQL. ","version":null,"tagName":"h2"},{"title":"Writing a GraphQL Backend by Hand is Long Gone","type":0,"sectionRef":"#","url":"/blog/writing-a-graphql-backend-by-hand-is-long-gone/","content":"","keywords":"","version":null},{"title":"Complexity with GraphQL​","type":1,"pageTitle":"Writing a GraphQL Backend by Hand is Long Gone","url":"/blog/writing-a-graphql-backend-by-hand-is-long-gone/#complexity-with-graphql","content":" If you see, most of the concerns with GraphQL are around building a robust GraphQL backend. It's rarely about consuming GraphQL, because if you look closely at the GraphQL spec, you will find that it's focused on how to elegantly consume data. As long as the output of your backend matches what's expected in the query, the specification doesn't care about how the backend is implemented.  Hence, the main complexity with GraphQL comes with how GraphQL is built. One of the major hurdles in hand-coding a GraphQL backend is managing performance. Issues like batching, incorrect usage of data loaders, caching, and the notorious N+1 problem can cripple your application.  Manually implementing batching mechanisms and data loaders can be incredibly tedious. While libraries like DataLoader can assist, integrating them seamlessly into your system requires a deep understanding of both your data and the GraphQL query patterns. Overuse of data loaders is so common with most GraphQL implementations that ultimately it becomes the main culprit for high latency.  Secondly, traditional caching doesn't work with GraphQL, so you have to resort to all sorts of solutions, using persisted queries or some vendor-specific implementation of caching. Implementing effective caching strategies is essential for performance but it's tricky. Developers must decide what to cache, when to invalidate the cache, and how to manage cache consistency, which adds another layer of complexity.  The N+1 issue, boy, that's perhaps everyone's favorite issue with GraphQL. It arises when executing multiple upstream requests that could have been combined into one, leading to massive performance degradation. Detecting and solving this requires meticulous analysis of query patterns and database access, which requires developers to have the context of the whole query at once, generate a query plan, translate it to appropriate upstream calls, and then execute! That's a lot of complex engineering effort; building a general-purpose query engine is not for the faint-hearted, and in the midst of all this complex yet interesting work, I need to ship features!  Grafast is an upcoming generalized query planner that could make query-planning in JS a bit more tamed.  GraphQL’s flexibility can be a double-edged sword when it comes to security, necessitating robust mechanisms for authentication and authorization. Like caching, traditional route-based API access doesn't work with GraphQL. Implementing these security layers correctly involves ensuring that only authenticated users can access the GraphQL entity and that they can only access data or fields that they are authorized to see. This requires fine-grained control and often custom logic and the invention of a new standard that works just for you.  Lastly, but most importantly, ensuring your GraphQL API is reliable means tackling error handling, propagation, and telemetry. Proper error handling in GraphQL is crucial for providing meaningful feedback to clients and maintaining the integrity of your application. The GraphQL team recently started working on a standard for serving GraphQL over HTTP, which won't be easy to integrate if you already have a GraphQL API running in production. Moreover, integrating telemetry within a GraphQL backend isn't easy either; it is a very involved process to integrate spans to trace GraphQL resolvers. And, if you have written your GraphQL layer by hand in JavaScript, be ready for some significant performance degradation.  ","version":null,"tagName":"h2"},{"title":"GraphQL is more like SQL and less like REST​","type":1,"pageTitle":"Writing a GraphQL Backend by Hand is Long Gone","url":"/blog/writing-a-graphql-backend-by-hand-is-long-gone/#graphql-is-more-like-sql-and-less-like-rest","content":" We talked about it in our previous blog why GraphQL isn't like REST or gRPC. I would argue that SQL is a closer elder sibling of GraphQL than REST or gRPC. Writing a GraphQL backend can be likened to building an SQL engine manually. Imagine if every time you wanted to interact with a database, you had to write the SQL engine from scratch. Every time you made a database change, you would need to rewrite your engine so that it can work with the new schema or indexes. It’s inefficient and impractical; no one does that. Fortunately, modern databases come with embedded, high-performance SQL engines such as Apache Calcite that adhere to the SQL specification but abstract away the complexities around building it. These databases allow developers to focus on writing queries and managing data without worrying about the underlying mechanics, thanks to their sophisticated query engines.  GraphQL, much like SQL, is a query language designed to allow clients to request exactly the data they need. Unlike REST, which relies on fixed endpoints, or gRPC, which focuses on remote procedure calls, GraphQL provides a flexible, hierarchical way to fetch and manipulate data, making it a closer analog to SQL in terms of expressiveness and precision. And I believe the future of GraphQL is going to be like the journey of this elder sibling.  ","version":null,"tagName":"h2"},{"title":"The future of GraphQL​","type":1,"pageTitle":"Writing a GraphQL Backend by Hand is Long Gone","url":"/blog/writing-a-graphql-backend-by-hand-is-long-gone/#the-future-of-graphql","content":" The future of GraphQL development is moving towards generalized automated solutions built on modern, low-level system stacks like Rust and Zig, and moving away from the prevalent hand-written Node.js-based solutions of today.    These engines will connect to data sources of any type and build a GraphQL endpoint on top of them. They will find connections between other data sources, sometimes completely automatically and sometimes using hints given by the developer, creating a unified GraphQL experience. Similar to SQL engines, which use JIT techniques to identify performance optimizations at runtime, GraphQL engines will become extremely smart about performance. My hope is that GraphQL will eventually move away from its dependency on the JSON protocol, into something more efficient such as protobuf. There is definitely going to be a lot of work put into the standardization of the loose ends. GraphQL engines will eventually converge on error handling and error propagation strategies. GraphQL on HTTP is the first step in that direction. Authentication and Authorization too will very quickly become standard features of GraphQL, so you won't need to worry about inventing a new way of authentication. This will all be packed into a GraphQL standard. This might be a stretch, but if the standards team gets together, I think even GraphQL caching will be consistent across all GraphQL engines, and you will be able to switch from one caching solution to another without locking into a vendor-specific implementation.  You might have already seen a wave of open-source solutions that build GraphQL on top of existing data sources. One such solution paving the way is Tailcall. Tailcall’s platform is designed to automate the creation, validation, and optimization of GraphQL backends. Sticking to standards and ensuring developers don't ever have to pay the heavy tax of using GraphQL that they do today, do check it out!  Lastly, if you are reading this today and thinking of writing a GraphQL server by hand, I urge you to reconsider and use something that does this for you. Before you know it, your handwritten solution will be deprecated in favor of something faster, easier, and more secure: an automatic GraphQL solution. ","version":null,"tagName":"h2"},{"title":"Design a GraphQL Schema So Good, It'll Make REST APIs Cry","type":0,"sectionRef":"#","url":"/blog/graphql-schema/","content":"","keywords":"","version":null},{"title":"The Power of GraphQL Schemas​","type":1,"pageTitle":"Design a GraphQL Schema So Good, It'll Make REST APIs Cry","url":"/blog/graphql-schema/#the-power-of-graphql-schemas","content":" A well-designed GraphQL schema serves as the blueprint for your entire API. It defines:  The types of data availableThe relationships between those typesThe operations clients can perform (queries, mutations, subscriptions)The structure of requests and responses  Your schema acts as a contract between your backend and frontend teams. Once published, clients can rely on its structure, enabling them to build UIs with confidence. A thoughtful schema design upfront can save significant refactoring down the road.  ","version":null,"tagName":"h2"},{"title":"Our Example Application: TechTalent​","type":1,"pageTitle":"Design a GraphQL Schema So Good, It'll Make REST APIs Cry","url":"/blog/graphql-schema/#our-example-application-techtalent","content":" To illustrate schema design principles, let's imagine we're building TechTalent - a platform connecting tech companies with job seekers. Our application will allow:  Companies to post job listingsCandidates to create profiles and apply to jobsRecruiters to search candidates and manage applications  We'll design our schema step-by-step to support these core features.  ","version":null,"tagName":"h2"},{"title":"Step 1: Identify Core Types​","type":1,"pageTitle":"Design a GraphQL Schema So Good, It'll Make REST APIs Cry","url":"/blog/graphql-schema/#step-1-identify-core-types","content":" The first step is to identify the main entities in our domain. For TechTalent, our core types might include:  CompanyJobListingCandidateApplicationRecruiter  Let's start by defining these as object types in our schema:  type Company { id: ID! name: String! description: String # More fields to come } type JobListing { id: ID! title: String! description: String! # More fields to come } type Candidate { id: ID! name: String! email: String! # More fields to come } type Application { id: ID! # More fields to come } type Recruiter { id: ID! name: String! email: String! # More fields to come }   Notice we've only included a few basic fields at this stage. We'll flesh these out as we progress.  ","version":null,"tagName":"h2"},{"title":"Step 2: Model Relationships​","type":1,"pageTitle":"Design a GraphQL Schema So Good, It'll Make REST APIs Cry","url":"/blog/graphql-schema/#step-2-model-relationships","content":" Next, we need to consider how these types relate to each other. In GraphQL, we model relationships by adding fields that reference other types. Let's update our types:  type Company { id: ID! name: String! description: String jobListings: [JobListing!]! recruiters: [Recruiter!]! } type JobListing { id: ID! title: String! description: String! company: Company! applications: [Application!]! } type Candidate { id: ID! name: String! email: String! applications: [Application!]! } type Application { id: ID! jobListing: JobListing! candidate: Candidate! status: ApplicationStatus! } type Recruiter { id: ID! name: String! email: String! company: Company! } enum ApplicationStatus { PENDING REVIEWED REJECTED ACCEPTED }   We've now established the core relationships:  Companies have job listings and recruitersJob listings belong to a company and have applicationsCandidates have applicationsApplications link a candidate to a job listingRecruiters belong to a company  Note the use of the ApplicationStatus enum to represent the fixed set of possible statuses.  ","version":null,"tagName":"h2"},{"title":"Step 3: Plan Query Operations​","type":1,"pageTitle":"Design a GraphQL Schema So Good, It'll Make REST APIs Cry","url":"/blog/graphql-schema/#step-3-plan-query-operations","content":" With our core types defined, let's consider what query operations our clients will need. We'll start with some basic CRUD (Create, Read, Update, Delete) operations:  type Query { company(id: ID!): Company jobListing(id: ID!): JobListing candidate(id: ID!): Candidate # List operations companies: [Company!]! jobListings(filters: JobListingFilters): [JobListing!]! candidates(filters: CandidateFilters): [Candidate!]! } input JobListingFilters { companyId: ID title: String # Add more filter options } input CandidateFilters { skills: [String!] experienceYears: Int # Add more filter options }   We've added basic queries to fetch individual entities by ID, as well as list queries for our main types. Notice the use of input types for filters - this allows for more flexible and extensible querying.  ","version":null,"tagName":"h2"},{"title":"Step 4: Plan Mutation Operations​","type":1,"pageTitle":"Design a GraphQL Schema So Good, It'll Make REST APIs Cry","url":"/blog/graphql-schema/#step-4-plan-mutation-operations","content":" Next, let's define some mutation operations to allow clients to modify data:  type Mutation { # Company mutations createCompany( input: CreateCompanyInput! ): CreateCompanyPayload! updateCompany( id: ID! input: UpdateCompanyInput! ): UpdateCompanyPayload! # Job Listing mutations createJobListing( input: CreateJobListingInput! ): CreateJobListingPayload! updateJobListing( id: ID! input: UpdateJobListingInput! ): UpdateJobListingPayload! # Candidate mutations createCandidate( input: CreateCandidateInput! ): CreateCandidatePayload! updateCandidate( id: ID! input: UpdateCandidateInput! ): UpdateCandidatePayload! # Application mutations submitApplication( input: SubmitApplicationInput! ): SubmitApplicationPayload! updateApplicationStatus( id: ID! status: ApplicationStatus! ): UpdateApplicationStatusPayload! } # Input and Payload types for each mutation...   Notice the pattern we're using for mutations:  Each mutation has a corresponding input typeEach mutation returns a payload type  This structure offers several benefits:  Input types allow for easy addition of new fields in the futurePayload types can include both the modified entity and any errors or metadataIt provides a consistent structure across all mutations  Let's look at an example input and payload type:  input CreateJobListingInput { companyId: ID! title: String! description: String! requirements: [String!]! salary: SalaryInput } input SalaryInput { min: Int! max: Int! currency: String! } type CreateJobListingPayload { jobListing: JobListing errors: [Error!] } type Error { message: String! path: [String!] }   This structure allows for detailed error reporting and future extensibility.  ","version":null,"tagName":"h2"},{"title":"Step 5: Consider Authentication and Authorization​","type":1,"pageTitle":"Design a GraphQL Schema So Good, It'll Make REST APIs Cry","url":"/blog/graphql-schema/#step-5-consider-authentication-and-authorization","content":" In a production application, we need to consider authentication and authorization. Let's add some operations for user management:  type Mutation { # ... previous mutations signup(input: SignupInput!): AuthPayload! login(input: LoginInput!): AuthPayload! logout: Boolean! } input SignupInput { email: String! password: String! name: String! role: UserRole! } input LoginInput { email: String! password: String! } type AuthPayload { token: String! user: User! } type User { id: ID! email: String! name: String! role: UserRole! } enum UserRole { CANDIDATE RECRUITER ADMIN }   We've added basic authentication operations and a User type to represent authenticated users. In a real-world scenario, you'd likely want to implement more robust authentication and authorization mechanisms.  ","version":null,"tagName":"h2"},{"title":"Step 6: Implement Pagination​","type":1,"pageTitle":"Design a GraphQL Schema So Good, It'll Make REST APIs Cry","url":"/blog/graphql-schema/#step-6-implement-pagination","content":" As our application grows, we'll need to implement pagination for our list queries. Let's update our jobListings query to use cursor-based pagination:  type Query { # ... other queries jobListings( first: Int after: String filters: JobListingFilters ): JobListingConnection! } type JobListingConnection { edges: [JobListingEdge!]! pageInfo: PageInfo! } type JobListingEdge { node: JobListing! cursor: String! } type PageInfo { hasNextPage: Boolean! endCursor: String }   This implementation follows the Relay connection specification, which provides a standardized way to handle pagination in GraphQL.  ","version":null,"tagName":"h2"},{"title":"Step 7: Plan for Real-time Updates​","type":1,"pageTitle":"Design a GraphQL Schema So Good, It'll Make REST APIs Cry","url":"/blog/graphql-schema/#step-7-plan-for-real-time-updates","content":" For certain features, we might want to provide real-time updates. Let's add a subscription to notify when new job listings are posted:  type Subscription { newJobListing: JobListing! }   Clients can subscribe to this operation to receive updates whenever a new job listing is created.  ","version":null,"tagName":"h2"},{"title":"Step 8: Implement Custom Scalars​","type":1,"pageTitle":"Design a GraphQL Schema So Good, It'll Make REST APIs Cry","url":"/blog/graphql-schema/#step-8-implement-custom-scalars","content":" Our schema might benefit from some custom scalar types for specific data formats. For example, let's add a DateTime scalar:  scalar DateTime type JobListing { # ... other fields postedAt: DateTime! applicationDeadline: DateTime }   We'll need to implement the serialization/deserialization logic for this scalar in our resolvers.  ","version":null,"tagName":"h2"},{"title":"Step 9: Use Interfaces for Shared Fields​","type":1,"pageTitle":"Design a GraphQL Schema So Good, It'll Make REST APIs Cry","url":"/blog/graphql-schema/#step-9-use-interfaces-for-shared-fields","content":" As our schema grows, we might notice some types sharing common fields. We can use interfaces to model this shared structure:  interface Node { id: ID! } interface Timestamped { createdAt: DateTime! updatedAt: DateTime! } type Company implements Node &amp; Timestamped { id: ID! createdAt: DateTime! updatedAt: DateTime! # ... other fields } type JobListing implements Node &amp; Timestamped { id: ID! createdAt: DateTime! updatedAt: DateTime! # ... other fields }   This approach promotes consistency and can make it easier to implement features that work across multiple types.  ","version":null,"tagName":"h2"},{"title":"Step 10: Document Your Schema​","type":1,"pageTitle":"Design a GraphQL Schema So Good, It'll Make REST APIs Cry","url":"/blog/graphql-schema/#step-10-document-your-schema","content":" Finally, it's crucial to document your schema thoroughly. GraphQL allows for built-in documentation:  &quot;&quot;&quot; Represents a company on the TechTalent platform. &quot;&quot;&quot; type Company implements Node &amp; Timestamped { &quot;&quot;&quot; Unique identifier for the company. &quot;&quot;&quot; id: ID! &quot;&quot;&quot; The name of the company. &quot;&quot;&quot; name: String! # ... other fields }   Good documentation helps both your team and API consumers understand the purpose and usage of each type and field.  ","version":null,"tagName":"h2"},{"title":"Visualizing the Schema​","type":1,"pageTitle":"Design a GraphQL Schema So Good, It'll Make REST APIs Cry","url":"/blog/graphql-schema/#visualizing-the-schema","content":" To better understand the relationships in our schema, let's visualize the core types:    This diagram illustrates the key relationships between our main entities, helping us ensure our schema accurately represents our domain.  To visualize your schema, you can use tools like GraphQL Voyager.  ","version":null,"tagName":"h2"},{"title":"Best Practices and Considerations​","type":1,"pageTitle":"Design a GraphQL Schema So Good, It'll Make REST APIs Cry","url":"/blog/graphql-schema/#best-practices-and-considerations","content":" As we've designed our schema, we've touched on several best practices. Let's recap some key points and add a few more considerations:  Start with the UI in mind: Design your schema based on how the data will be used in your UI, not just how it's stored in your database. Use clear, consistent naming: Adopt a naming convention (e.g., PascalCase for types, camelCase for fields) and stick to it. Leverage GraphQL features: Make use of enums, interfaces, and unions to create a rich, expressive schema. Plan for change: Use input types for mutations and consider versioning strategies for evolving your schema over time. Optimize for performance: Be mindful of N+1 query problems and consider implementing DataLoader or similar batching mechanisms. Secure your schema: Implement proper authentication and authorization. Consider using directives for field-level permissions. Validate input: Use non-nullable fields and custom scalars to enforce data integrity at the schema level. Provide meaningful errors: Return detailed error information in your mutation payloads to help clients handle failures gracefully. Monitor and analyze: Implement logging and monitoring to understand how your schema is being used and where optimizations can be made. Keep it DRY: Use interfaces and abstract types to reduce duplication in your schema.  ","version":null,"tagName":"h2"},{"title":"Conclusion​","type":1,"pageTitle":"Design a GraphQL Schema So Good, It'll Make REST APIs Cry","url":"/blog/graphql-schema/#conclusion","content":" Designing a production-grade GraphQL schema is an iterative process that requires careful thought and planning. By starting with core types and relationships, then gradually adding queries, mutations, and advanced features, we can build a schema that's both powerful and maintainable.  Remember, your schema is a living document. As your application evolves, so too will your schema. By following these principles and best practices, you'll be well-equipped to design and maintain a GraphQL schema that can grow with your needs.  The TechTalent example we've explored here demonstrates many real-world considerations, but every application will have its unique requirements. Always design with your specific use cases in mind, and don't be afraid to iterate as you learn more about how your API is used in practice.  By investing time in thoughtful schema design upfront, you'll create a solid foundation for your GraphQL API, enabling efficient development and a great experience for your API consumers. ","version":null,"tagName":"h2"},{"title":"Guidelines","type":0,"sectionRef":"#","url":"/docs/contribution-guidelines/","content":"","keywords":"","version":"Next"},{"title":"The Basics​","type":1,"pageTitle":"Guidelines","url":"/docs/contribution-guidelines/#the-basics","content":" Fork and Clone: Fork the repository on GitHub and clone your fork locally. git clone https://github.com/yourusername/tailcall.git Set Up Your Environment: Install Rust: Use rustup to install Rust and the nightly toolchain.Install Prettier: Required for linting, install Prettier.Build the Application: Navigate to the project directory and execute cargo build.Start the Server: Run cargo run -- start ./examples/jsonplaceholder.graphql to start the server and access the GraphiQL interface at https://tailcall.run/playground.  ","version":"Next","tagName":"h2"},{"title":"Making and Discussing Changes​","type":1,"pageTitle":"Guidelines","url":"/docs/contribution-guidelines/#making-and-discussing-changes","content":" Create a New Branch: Always work on a new branch created from the latest main branch. git checkout -b feature/your-feature-name Develop Incrementally: Use small, stacked PRs for complex features. Break down large tasks into smaller, manageable pieces, each with its own PR. If you are working on a large bounty item add the bounty on your main PR and create stacked PRs wrt to your main PR. Discuss on Discord: For real-time discussions, use the #contributors channel on Discord. Create a thread for each PR to facilitate focused discussions.  ","version":"Next","tagName":"h2"},{"title":"Pull Requests and Code Quality​","type":1,"pageTitle":"Guidelines","url":"/docs/contribution-guidelines/#pull-requests-and-code-quality","content":" Keep PRs Small: Focus each PR on a single topic to simplify review and potential reverts. Describe your changes clearly in the PR description, explaining the solution and linking to any relevant discussions or issues. Commit Clearly: Write concise, descriptive commit messages. Each commit should represent a self-contained change. Submit PRs: Push your branch to GitHub and open a PR against the main branch. In the PR description, detail the purpose of your changes and any additional context needed. Code Review: Engage with reviewers on GitHub and address feedback promptly. Use discussions on Discord to resolve complex issues or debates efficiently.  ","version":"Next","tagName":"h2"},{"title":"Community Engagement​","type":1,"pageTitle":"Guidelines","url":"/docs/contribution-guidelines/#community-engagement","content":" Star and Share: Star the repository if you find it helpful and share your contributions on social media using #tailcall and tagging @tailcallhq.  ","version":"Next","tagName":"h2"},{"title":"Final Notes​","type":1,"pageTitle":"Guidelines","url":"/docs/contribution-guidelines/#final-notes","content":" Tailcall thrives through your contributions. We aim to maintain a respectful and inclusive community. Thank you for helping to enhance Tailcall for everyone! ","version":"Next","tagName":"h2"},{"title":"Bounty","type":0,"sectionRef":"#","url":"/docs/contributors/bounty/","content":"","keywords":"","version":"Next"},{"title":"Our Philosophy​","type":1,"pageTitle":"Bounty","url":"/docs/contributors/bounty/#our-philosophy","content":" We’re all about meritocracy here. That means the best ideas and implementations win! We love seeing your quality work and fast moves. And yes, we recognize and appreciate your efforts when you exceed expectations.  ","version":"Next","tagName":"h2"},{"title":"Quick & Quality​","type":1,"pageTitle":"Bounty","url":"/docs/contributors/bounty/#quick--quality","content":" Speedy Gonzalez: We like fast results! Quick feedback, quick updates. The faster, the better!A+ Quality: But hey, don’t rush it if it means cutting corners. We want your best – make it shine!  ","version":"Next","tagName":"h2"},{"title":"Teamwork Makes the Dream Work​","type":1,"pageTitle":"Bounty","url":"/docs/contributors/bounty/#teamwork-makes-the-dream-work","content":" Join Us on Discord: Our Discord server is THE place to collaborate, get tips, and find your next bestie. Let’s make magic together.Share the Love: Inspired by someone’s PR? Working together? Feel free to /split that bounty – sharing is caring!  ","version":"Next","tagName":"h2"},{"title":"How to Dive In​","type":1,"pageTitle":"Bounty","url":"/docs/contributors/bounty/#how-to-dive-in","content":" Pick a Challenge: Use /attempt in the comments to call dibs on an issue. It’s like saying “I got this!”Show Your Work: Got an issue? Great! Now, whip up a draft PR within 24 hours to show you’re on it.Go for Gold: Once you’re ready, switch that draft to Ready for Review. Make sure it’s polished and gleaming!Extra Mile Alert: We’ve got bonuses for those who add that special touch. Clean up, optimize, or fix something extra? We’re here for it!  ","version":"Next","tagName":"h2"},{"title":"The Rules of the Game​","type":1,"pageTitle":"Bounty","url":"/docs/contributors/bounty/#the-rules-of-the-game","content":" Be Quick or Be… Late: No PR within 24 hours? Then it’s open season for that issue again.There Can Be One Winner: Multiple folks can try, but it's the top PR that emerges victorious. If there's a tie, the first submission takes the prize.No Copycats: Be original; be yourself. Avoid merely copying someone else's hard work.Consider Before You Contribute: If there’s already a PR in progress, perhaps take a moment to review it before submitting your own.  ","version":"Next","tagName":"h2"},{"title":"Identifying Plagiarism​","type":1,"pageTitle":"Bounty","url":"/docs/contributors/bounty/#identifying-plagiarism","content":" To keep our Bounty Program fair and fun, we have a strict no-plagiarism policy. Here’s how we keep it original:  Manual Review: Our team manually checks all submissions to spot any sneaky copy-pasting.Raise the Alarm: If you think someone’s trying to pull a fast one, let us know ASAP! Before the copycat's PR gets merged, shout it out in our Discord channel and drop a comment on their PR.  Caught cheating? That means disqualification from the current bounty, a possible ban from future bounties, and a heads-up to the community. So, keep it real and let’s make this program a blast for everyone!  ","version":"Next","tagName":"h2"},{"title":"Wrapping Up​","type":1,"pageTitle":"Bounty","url":"/docs/contributors/bounty/#wrapping-up","content":" We’re stoked to have you! This program is your chance to shine and get rewarded while at it. Stick to these friendly guidelines, and let’s make something awesome together. We look forward to seeing what you bring to the table. ","version":"Next","tagName":"h2"},{"title":"gRPC Decoded: The API Protocol That's Changing Everything","type":0,"sectionRef":"#","url":"/blog/what-is-grpc/","content":"","keywords":"","version":null},{"title":"The Evolution of API Communication​","type":1,"pageTitle":"gRPC Decoded: The API Protocol That's Changing Everything","url":"/blog/what-is-grpc/#the-evolution-of-api-communication","content":"   Request–response protocols date back to early distributed computing in the late 1960s. Theoretical proposals of remote procedure calls as the model of network operations date to the 1970s, with practical implementations emerging in the early 1980s. Traditional RPC mechanisms had limitations in terms of performance, language independence, and flexibility. gRPC addresses these issues by leveraging modern protocols and technologies.  gRPC was initially created by Google, which used a single general-purpose RPC infrastructure called Stubby to connect its numerous microservices. In 2015, Google decided to build the next version of Stubby and make it open source.  ","version":null,"tagName":"h2"},{"title":"Understanding gRPC​","type":1,"pageTitle":"gRPC Decoded: The API Protocol That's Changing Everything","url":"/blog/what-is-grpc/#understanding-grpc","content":" gRPC is a high-performance, language-neutral RPC framework. It uses Protobuf for serialization and HTTP/2 for transport, offering features like streaming, multiplexing, and bidirectional communication. It uses HTTP/2 for transport, Protocol Buffers as the interface description language, and provides features such as authentication, bidirectional streaming and flow control, blocking or nonblocking bindings, and cancellation and timeouts.  ","version":null,"tagName":"h2"},{"title":"Key components of gRPC​","type":1,"pageTitle":"gRPC Decoded: The API Protocol That's Changing Everything","url":"/blog/what-is-grpc/#key-components-of-grpc","content":" Protocol Buffers (Protobuf): A language-neutral, platform-neutral, extensible mechanism for serializing structured data. It is used to define the structure of messages (request and response payloads) that gRPC services exchange. HTTP/2: Provides additional capabilities such as multiplexing, header compression, and server push, which are not as efficient and reliable in HTTP/1.1  ","version":null,"tagName":"h3"},{"title":"How gRPC works (step-by-step process)​","type":1,"pageTitle":"gRPC Decoded: The API Protocol That's Changing Everything","url":"/blog/what-is-grpc/#how-grpc-works-step-by-step-process","content":"   gRPC (Remote Procedure Call) works using a straightforward yet powerful mechanism that facilitates communication between clients and servers in a distributed system.  ","version":null,"tagName":"h3"},{"title":"1. Service Definition​","type":1,"pageTitle":"gRPC Decoded: The API Protocol That's Changing Everything","url":"/blog/what-is-grpc/#1-service-definition","content":" Protocol Buffers (Protobuf): The starting point for using gRPC is defining a service and its methods using Protocol Buffers (Protobuf). Protobuf is a language-neutral, platform-neutral, extensible mechanism for serializing structured data. The structure of data and services is defined in a .proto file.  An Example Snippet From Linkerd  Example of a simple .proto file :  syntax = &quot;proto3&quot;; package calculator; service CalculatorService { rpc Add (AddRequest) returns (AddResponse); } message AddRequest { double number1 = 1; double number2 = 2; } message AddResponse { double result = 1; }   ","version":null,"tagName":"h3"},{"title":"2. Code Generation​","type":1,"pageTitle":"gRPC Decoded: The API Protocol That's Changing Everything","url":"/blog/what-is-grpc/#2-code-generation","content":" Once a service is defined in a .proto file, the Protocol Buffer compiler (protoc) is used to generate client and server code in your chosen programming languages. This step is crucial as it automates the creation of the boilerplate code needed for the gRPC service and client to communicate effectively. The generated code includes:  Service Stubs: These are classes with methods that correspond to the service methods defined in the .proto file. They handle the marshalling and unmarshalling of request and response messages, abstracting away the complexities of network communication. Client-Side Stubs: These are used by the client application to make remote procedure calls to the server. The client stubs handle the creation and sending of requests, as well as receiving and processing responses.  For Example if the calculator example is converted to python, it would look something like this:  # -*- coding: utf-8 -*- # Generated by the protocol buffer compiler. DO NOT EDIT! # source: calculator &quot;&quot;&quot;Generated protocol buffer code.&quot;&quot;&quot; from google.protobuf import descriptor as _descriptor from google.protobuf import descriptor_pool as _descriptor_pool from google.protobuf import message as _message from google.protobuf import reflection as _reflection from google.protobuf import symbol_database as _symbol_database # @@protoc_insertion_point(imports) _sym_db = _symbol_database.Default() DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\\n\\ncalculator\\x12\\ncalculator\\&quot;.\\n\\nAddRequest\\x12\\x0f\\n\\x07number1\\x18\\x01 \\x01(\\x01\\x12\\x0f\\n\\x07number2\\x18\\x02 \\x01(\\x01\\&quot;\\x1d\\n\\x0b\\x41\\x64\\x64Response\\x12\\x0e\\n\\x06result\\x18\\x01 \\x01(\\x01\\x32K\\n\\x11\\x43\\x61lculatorService\\x12\\x36\\n\\x03\\x41\\x64\\x64\\x12\\x16.calculator.AddRequest\\x1a\\x17.calculator.AddResponseb\\x06proto3') _ADDREQUEST = DESCRIPTOR.message_types_by_name['AddRequest'] _ADDRESPONSE = DESCRIPTOR.message_types_by_name['AddResponse'] AddRequest = _reflection.GeneratedProtocolMessageType('AddRequest', (_message.Message,), { 'DESCRIPTOR' : _ADDREQUEST, '__module__' : 'calculator_pb2' # @@protoc_insertion_point(class_scope:calculator.AddRequest) }) _sym_db.RegisterMessage(AddRequest) AddResponse = _reflection.GeneratedProtocolMessageType('AddResponse', (_message.Message,), { 'DESCRIPTOR' : _ADDRESPONSE, '__module__' : 'calculator_pb2' # @@protoc_insertion_point(class_scope:calculator.AddResponse) }) _sym_db.RegisterMessage(AddResponse) _CALCULATORSERVICE = DESCRIPTOR.services_by_name['CalculatorService'] if _descriptor._USE_C_DESCRIPTORS == False: DESCRIPTOR._options = None _ADDREQUEST._serialized_start=26 _ADDREQUEST._serialized_end=72 _ADDRESPONSE._serialized_start=74 _ADDRESPONSE._serialized_end=103 _CALCULATORSERVICE._serialized_start=105 _CALCULATORSERVICE._serialized_end=180 # @@protoc_insertion_point(module_scope)   ","version":null,"tagName":"h3"},{"title":"3. Client-Server Communication​","type":1,"pageTitle":"gRPC Decoded: The API Protocol That's Changing Everything","url":"/blog/what-is-grpc/#3-client-server-communication","content":" Transmission: When a gRPC client initiates a request to a gRPC server, it sends an HTTP/2 request containing the service name, specific method, and serialized parameters using Protobuf. HTTP/2's advantages include multiplexing, enabling concurrent handling of multiple streams over a single connection, binary framing that minimizes overhead and accelerates data exchange, efficient header compression via HPACK, and integrated flow control mechanisms.  ","version":null,"tagName":"h3"},{"title":"4. Serialization and Deserialization​","type":1,"pageTitle":"gRPC Decoded: The API Protocol That's Changing Everything","url":"/blog/what-is-grpc/#4-serialization-and-deserialization","content":" Protobuf Serialization: Data exchanged between gRPC clients and servers is serialized and deserialized using Protobuf.  ","version":null,"tagName":"h3"},{"title":"gRPC Service Methods​","type":1,"pageTitle":"gRPC Decoded: The API Protocol That's Changing Everything","url":"/blog/what-is-grpc/#grpc-service-methods","content":"   Unary RPC: This is the simplest form where the client sends a single request to the server and receives a single response: service MyService { rpc UnaryExample(MyRequest) returns (MyResponse); } Server Streaming RPC: The client sends a request to the server and receives a stream of responses: service MyService { rpc UnaryExample(MyRequest) returns (stream MyResponse); } Client Streaming RPC: The client sends a stream of requests to the server and receives a single response: service MyService { rpc UnaryExample(stream MyRequest) returns (MyResponse); } Bidirectional Streaming RPC: Both the client and server send a stream of messages to each other, establishing a persistent connection: service MyService { rpc UnaryExample(stream MyRequest) returns (stream MyResponse); }   ","version":null,"tagName":"h2"},{"title":"gRPC vs. REST: Basic Comparison​","type":1,"pageTitle":"gRPC Decoded: The API Protocol That's Changing Everything","url":"/blog/what-is-grpc/#grpc-vs-rest-basic-comparison","content":"   A comparison of payload sizes: REST JSON vs gRPC binary checkout full comparison   ","version":null,"tagName":"h2"},{"title":"Communication model​","type":1,"pageTitle":"gRPC Decoded: The API Protocol That's Changing Everything","url":"/blog/what-is-grpc/#communication-model","content":" gRPC: RPC-based, strong typing, and allows unary and bi-directional streaming, making it feasible for modern-day applications and use-cases.REST: Stateless, used for CRUD-based operations over HTTP, follows a simple unary request/response cycle.  ","version":null,"tagName":"h3"},{"title":"Data format and serialization​","type":1,"pageTitle":"gRPC Decoded: The API Protocol That's Changing Everything","url":"/blog/what-is-grpc/#data-format-and-serialization","content":" gRPC: Uses Protobuf for efficient binary serialization.REST: Uses a plain-text format like JSON and XML, which requires more processing in order to parse.  ","version":null,"tagName":"h3"},{"title":"Use cases for each​","type":1,"pageTitle":"gRPC Decoded: The API Protocol That's Changing Everything","url":"/blog/what-is-grpc/#use-cases-for-each","content":" gRPC: Suitable for internal microservices, real-time applications, and situations needing high-performance and time-sensitive communication.REST: Better for public APIs, browser-based applications, and situations requiring stateless operations where ease of use is a priority.  ","version":null,"tagName":"h3"},{"title":"Advantages of gRPC​","type":1,"pageTitle":"gRPC Decoded: The API Protocol That's Changing Everything","url":"/blog/what-is-grpc/#advantages-of-grpc","content":" ","version":null,"tagName":"h2"},{"title":"Efficiency and performance​","type":1,"pageTitle":"gRPC Decoded: The API Protocol That's Changing Everything","url":"/blog/what-is-grpc/#efficiency-and-performance","content":"   Protobuf efficiently serializes messages on both the server and client sides, ensuring that data is transmitted in a compact binary format. This results in smaller message payloads, which are quicker to transmit over the network compared to the verbose JSON format used in REST APIs.  In addition, HTTP/2 uses features like header-compression, multiplexing and server-push which significantly reduce the payload size, as well as make response faster.  These features collectively contribute to significant performance gains, making gRPC 7-10 times faster than traditional REST APIs using JSON.  ","version":null,"tagName":"h3"},{"title":"Language-agnostic nature​","type":1,"pageTitle":"gRPC Decoded: The API Protocol That's Changing Everything","url":"/blog/what-is-grpc/#language-agnostic-nature","content":"   gRPC uses Protocol Buffers (Protobuf) as its (IDL) for describing both the structure and the semantics of the messages sent between clients and servers. Protobuf is independent of programming languages, meaning you can define your API once using Protobuf and then generate code in various languages to interact with it. This allows seamless integration of sub-systems API specification, while also enhancing the DX.  ","version":null,"tagName":"h3"},{"title":"Strong typing and code generation​","type":1,"pageTitle":"gRPC Decoded: The API Protocol That's Changing Everything","url":"/blog/what-is-grpc/#strong-typing-and-code-generation","content":" Protocol Buffers (Protobuf) defines both the structure and the types of messages exchanged between clients and servers within a .proto file, thereby establishing a clear and standardized API contract. This contract specifies the fields and their data types for each message, ensuring consistency and predictability in communication. By enforcing strong typing, Protobuf enhances code reliability by detecting type-related errors during compilation rather than at runtime. This approach not only prevents type mismatches and potential bugs but also saves developers time that would otherwise be spent implementing manual type-checking. Additionally, Protobuf's built-in type safety simplifies the development process, allowing developers to focus more on business logic and less on handling data integrity issues, thus improving the developer experience.  ","version":null,"tagName":"h3"},{"title":"Bidirectional streaming capabilities​","type":1,"pageTitle":"gRPC Decoded: The API Protocol That's Changing Everything","url":"/blog/what-is-grpc/#bidirectional-streaming-capabilities","content":" Unlike traditional RPC methods that are unidirectional (either client-to-server or server-to-client), gRPC's bidirectional streaming allows both parties to establish a persistent connection and send a sequence of messages asynchronously.  Bidirectional streaming is particularly beneficial for applications requiring interactive and responsive communication, such as chat systems, collaborative tools, multiplayer games, and real-time data feeds.  ","version":null,"tagName":"h3"},{"title":"Extensibility and backward compatibility​","type":1,"pageTitle":"gRPC Decoded: The API Protocol That's Changing Everything","url":"/blog/what-is-grpc/#extensibility-and-backward-compatibility","content":" gRPC using Protobuf as the IDL opens support for extensibility by allowing new fields, messages, and services to be added to the .proto file definitions. As services evolve, these changes can be propagated through automated code generation using the protoc compiler, which produces language-specific stubs and serializers/deserializers.  Moreover, explicit versioning and API contracts defined in the .proto files help manage compatibility between different versions of services. During the RPC connection handshake, gRPC allows clients and servers to negotiate capabilities, ensuring that both parties can communicate effectively even if they support different versions or extensions.  Example:  syntax = &quot;proto3&quot;; package greet.v1; service Greeter { rpc SayHello (HelloRequest) returns (HelloReply); } message HelloRequest { string name = 1; } message HelloReply { string message = 1; }   ","version":null,"tagName":"h3"},{"title":"Challenges and Considerations​","type":1,"pageTitle":"gRPC Decoded: The API Protocol That's Changing Everything","url":"/blog/what-is-grpc/#challenges-and-considerations","content":" ","version":null,"tagName":"h2"},{"title":"Learning curve​","type":1,"pageTitle":"gRPC Decoded: The API Protocol That's Changing Everything","url":"/blog/what-is-grpc/#learning-curve","content":" gRPC has a much steeper learning curve compared to the traditional REST, mainly due to some new concepts like HTTP/2 and Protobuf which require significant practice and experience.  ","version":null,"tagName":"h3"},{"title":"Debugging complexity​","type":1,"pageTitle":"gRPC Decoded: The API Protocol That's Changing Everything","url":"/blog/what-is-grpc/#debugging-complexity","content":" Debugging gRPC applications can be really challenging compared to traditional REST APIs. The binary nature of Protobuf messages makes it difficult to inspect and manipulate payloads directly. Tools for debugging and tracing gRPC calls are available, but they often require additional setup and expertise.  ","version":null,"tagName":"h3"},{"title":"Ecosystem maturity​","type":1,"pageTitle":"gRPC Decoded: The API Protocol That's Changing Everything","url":"/blog/what-is-grpc/#ecosystem-maturity","content":" While gRPC has gained significant traction and support, its ecosystem is still maturing compared to REST. Some languages and frameworks may have limited or incomplete support for gRPC features. Additionally, there is less developer support on the internet, less browser-support and very few articles published which makes it challenging to learn especially for beginners.  ","version":null,"tagName":"h3"},{"title":"Browser support limitations​","type":1,"pageTitle":"gRPC Decoded: The API Protocol That's Changing Everything","url":"/blog/what-is-grpc/#browser-support-limitations","content":" Current browser limitations prevent direct implementation of the HTTP/2 gRPC specification. Browsers lack the necessary APIs to provide fine-grained control over requests. For instance:  There's no way to enforce the use of HTTP/2.Even if HTTP/2 could be enforced, browsers can't access raw HTTP/2 frames.  To address these limitations, the gRPC-Web specification was developed. It builds upon the HTTP/2 spec but introduces key differences:  Support for both HTTP/1.1 and HTTP/2 protocols.A new method for handling gRPC trailers: Trailers are sent at the very end of request/response bodies.A new bit in the gRPC message header indicates the presence of trailers. Requirement of a proxy server: This proxy translates between gRPC-Web requests and standard gRPC HTTP/2 responses.It's a mandatory component in the gRPC-Web architecture.  These adaptations allow gRPC-like functionality in web browsers while working within current browser constraints.  gRPC is powerful for service to service communication, but it may not be the best choice for public APIs or browser-based applications where REST/GraphQL is more prevalent.  tip To seamlessly integrate the benefits of both gRPC and GraphQL, you can easily generate GraphQL from gRPC using Tailcall. Check out the documentation here: gRPC to GraphQL .Automated gRPC to GraphQL  ","version":null,"tagName":"h3"},{"title":"Implementing gRPC: Best Practices​","type":1,"pageTitle":"gRPC Decoded: The API Protocol That's Changing Everything","url":"/blog/what-is-grpc/#implementing-grpc-best-practices","content":" ","version":null,"tagName":"h2"},{"title":"Designing Effective Protobuf Schemas​","type":1,"pageTitle":"gRPC Decoded: The API Protocol That's Changing Everything","url":"/blog/what-is-grpc/#designing-effective-protobuf-schemas","content":" Creating efficient and maintainable Protobuf schemas is crucial. Use meaningful field names and provide clear comments for each field, otherwise you may end up in a nested jargon of types! Versioning schemas properly ensures backward and forward compatibility makes it easier to evolve your API without breaking existing clients.  ","version":null,"tagName":"h3"},{"title":"Error Handling and Status Codes​","type":1,"pageTitle":"gRPC Decoded: The API Protocol That's Changing Everything","url":"/blog/what-is-grpc/#error-handling-and-status-codes","content":" Define and document all possible error codes your service can return. Consistent and informative error messages aid in debugging and provide a better experience for developers integrating with your API, if the API is not verbose about the error, the developer trying to integrate the API on the other side may get frustrated:  info A bad API is like a traffic jam - frustrating, confusing, and costly. - Joshua Bloch  ","version":null,"tagName":"h3"},{"title":"Security Considerations (Authentication, Encryption)​","type":1,"pageTitle":"gRPC Decoded: The API Protocol That's Changing Everything","url":"/blog/what-is-grpc/#security-considerations-authentication-encryption","content":" Secure your gRPC services by implementing authentication and encryption. Use Transport Layer Security (TLS) to encrypt communication between clients and servers. Leverage gRPC's support for various authentication mechanisms, such as OAuth, JWT, or custom tokens, to ensure that only authorized clients can access your services.  ","version":null,"tagName":"h3"},{"title":"Performance Optimization Techniques​","type":1,"pageTitle":"gRPC Decoded: The API Protocol That's Changing Everything","url":"/blog/what-is-grpc/#performance-optimization-techniques","content":" gRPC API performance can be boosted in many ways. The channels are expensive to make, and reusing them instead of remaking has a significant impact.  Example:  const grpc = require(&quot;grpc&quot;) // Singleton instance for the gRPC channel let channel = null function getGrpcChannel() { if (!channel) { // Create a new channel if it doesn't exist channel = new grpc.Client(&quot;localhost:50051&quot;, grpc.credentials.createInsecure()) } return channel } // Example usage: const myChannel = getGrpcChannel() // Use `myChannel` to make gRPC calls   Alongside with reusing channels, many other ways can improve performance like implementing load-balancers and using streaming instead of unary where needed.  ","version":null,"tagName":"h3"},{"title":"gRPC Use Cases and Real-World Examples​","type":1,"pageTitle":"gRPC Decoded: The API Protocol That's Changing Everything","url":"/blog/what-is-grpc/#grpc-use-cases-and-real-world-examples","content":" ","version":null,"tagName":"h2"},{"title":"Microservices Architecture​","type":1,"pageTitle":"gRPC Decoded: The API Protocol That's Changing Everything","url":"/blog/what-is-grpc/#microservices-architecture","content":" gRPC is well-suited for microservices architectures, enabling efficient communication between services. Companies like Netflix and Google use gRPC to connect their microservices, benefiting from its performance and strong typing. It ensures reliable, low-latency communication, which is crucial for maintaining responsive and scalable microservices.  ","version":null,"tagName":"h3"},{"title":"Real-Time Communication Systems​","type":1,"pageTitle":"gRPC Decoded: The API Protocol That's Changing Everything","url":"/blog/what-is-grpc/#real-time-communication-systems","content":"   gRPC is ideal for real-time communication systems such as chat applications, online gaming, and live streaming services. Its support for bidirectional streaming allows for seamless and efficient data exchange between clients and servers, enabling real-time interactions and reducing latency.  A snippet from Open-Match, a gaming framework:   // Tickets in matches returned by FetchMatches are moved from active to // pending, and will not be returned by query. rpc FetchMatches(FetchMatchesRequest) returns (stream FetchMatchesResponse) { option (google.api.http) = { post: &quot;/v1/backendservice/matches:fetch&quot; body: &quot;*&quot; }; }   ","version":null,"tagName":"h3"},{"title":"IoT and Edge Computing​","type":1,"pageTitle":"gRPC Decoded: The API Protocol That's Changing Everything","url":"/blog/what-is-grpc/#iot-and-edge-computing","content":" In IoT and edge computing scenarios, gRPC's low overhead and efficient communication make it suitable for resource-constrained devices. It enables reliable communication between edge devices and central servers, facilitating data collection, processing, and command execution in real time.  ","version":null,"tagName":"h3"},{"title":"Mobile and Web Applications​","type":1,"pageTitle":"gRPC Decoded: The API Protocol That's Changing Everything","url":"/blog/what-is-grpc/#mobile-and-web-applications","content":" gRPC is increasingly used in mobile and web applications to improve performance and reduce bandwidth usage. For example, companies like Lyft use gRPC to enhance the efficiency of their mobile apps, ensuring faster response times and a smoother user experience.  ","version":null,"tagName":"h3"},{"title":"Tools and Frameworks for gRPC Development​","type":1,"pageTitle":"gRPC Decoded: The API Protocol That's Changing Everything","url":"/blog/what-is-grpc/#tools-and-frameworks-for-grpc-development","content":" ","version":null,"tagName":"h2"},{"title":"Popular gRPC Libraries for Different Languages​","type":1,"pageTitle":"gRPC Decoded: The API Protocol That's Changing Everything","url":"/blog/what-is-grpc/#popular-grpc-libraries-for-different-languages","content":" gRPC has libraries and tooling support for various programming languages:  gRPC Core - C, C++, Ruby, Node.js, Python, PHP, C#, Objective-CgRPC Java - The Java gRPC implementation. HTTP/2 based RPCgRPC Node.js - gRPC for Node.jsgRPC Go - The Go language implementation of gRPC. HTTP/2 based RPCgRPC C# - The C# language implementation of gRPCgRPC Web - gRPC for Web Clients  ","version":null,"tagName":"h3"},{"title":"Testing and Debugging Tools​","type":1,"pageTitle":"gRPC Decoded: The API Protocol That's Changing Everything","url":"/blog/what-is-grpc/#testing-and-debugging-tools","content":" ghzgatling-grpc  ","version":null,"tagName":"h3"},{"title":"API Management Platforms​","type":1,"pageTitle":"gRPC Decoded: The API Protocol That's Changing Everything","url":"/blog/what-is-grpc/#api-management-platforms","content":" Postmanletmegrpc  ","version":null,"tagName":"h3"},{"title":"Future of gRPC and API Communication​","type":1,"pageTitle":"gRPC Decoded: The API Protocol That's Changing Everything","url":"/blog/what-is-grpc/#future-of-grpc-and-api-communication","content":" ","version":null,"tagName":"h2"},{"title":"Emerging Trends in API Design​","type":1,"pageTitle":"gRPC Decoded: The API Protocol That's Changing Everything","url":"/blog/what-is-grpc/#emerging-trends-in-api-design","content":" The future of API design is moving towards more efficient and flexible communication protocols like gRPC. With the rise of microservices, IoT, and real-time applications, gRPC's performance advantages make it a compelling choice. Trends like GraphQL and RESTful JSON APIs will continue to coexist, but gRPC will gain traction for specific use cases requiring high efficiency and low latency.  ","version":null,"tagName":"h3"},{"title":"gRPC's Role in Cloud-Native Applications​","type":1,"pageTitle":"gRPC Decoded: The API Protocol That's Changing Everything","url":"/blog/what-is-grpc/#grpcs-role-in-cloud-native-applications","content":" gRPC is becoming a cornerstone of cloud-native applications, facilitating communication in containerized environments orchestrated by platforms like Kubernetes. Its ability to handle high-performance, low-latency communication is essential for the scalability and reliability of cloud-native architectures.  ","version":null,"tagName":"h3"},{"title":"Potential Improvements and Extensions​","type":1,"pageTitle":"gRPC Decoded: The API Protocol That's Changing Everything","url":"/blog/what-is-grpc/#potential-improvements-and-extensions","content":" The gRPC ecosystem is continuously evolving, with potential improvements and extensions on the horizon. Enhancements in tooling, support for more languages, better integration with existing frameworks, and increased adoption of gRPC-Web are some areas of expected growth. The community's efforts to address current limitations will make gRPC more accessible and robust for a wider range of applications.  ","version":null,"tagName":"h3"},{"title":"Conclusion​","type":1,"pageTitle":"gRPC Decoded: The API Protocol That's Changing Everything","url":"/blog/what-is-grpc/#conclusion","content":" Recap of gRPC's key features and benefits​  In summary, gRPC offers efficient, low-latency communication, strong typing through Protobuf, and support for multiple languages. Its bidirectional streaming and multiplexing capabilities make it ideal for real-time and microservices-based applications. The performance and reliability of gRPC provide significant advantages over traditional REST APIs in many scenarios, mainly because of the new HTTP/2 and its binary nature.  Considerations for Adopting gRPC in Projects​  When considering gRPC for your projects, ensure that your team is prepared to handle the challenges and leverage the best practices discussed to design, implement, and maintain robust gRPC services. Make sure you have enough support resources and officials, as gRPC doesn't have a community as large as REST.  ","version":null,"tagName":"h2"},{"title":"Further Resources​","type":1,"pageTitle":"gRPC Decoded: The API Protocol That's Changing Everything","url":"/blog/what-is-grpc/#further-resources","content":" Official documentation and tutorials​  gRPC Official Documentation  Community forums and support​  gRPC Twitter handlegRPC StackOverflow taggRPC Gitter roomgRPC Google Group  Books for in-depth learning​  gRPC: Up and RunninggRPC Microservices in Go ","version":null,"tagName":"h3"},{"title":"Micro Benchmarks","type":0,"sectionRef":"#","url":"/docs/contributors/micro-benchmark/","content":"","keywords":"","version":"Next"},{"title":"Running Benchmarks​","type":1,"pageTitle":"Micro Benchmarks","url":"/docs/contributors/micro-benchmark/#running-benchmarks","content":" Install cargo-criterion and rust-script: cargo install cargo-criterion rust-script Execute the benchmarks: cargo bench This command will run all benchmarks and display the results. To run a specific benchmark you could modify the command and pass a pattern to the command: cargo bench -- 'foo.*bar'   ","version":"Next","tagName":"h2"},{"title":"Comparing Benchmarks​","type":1,"pageTitle":"Micro Benchmarks","url":"/docs/contributors/micro-benchmark/#comparing-benchmarks","content":" To facilitate benchmark comparison, we have developed a Rust script capable of contrasting the outcomes of two benchmarks.  # Checkout the base branch git checkout main # Run the benchmarks for the main branch and store the result in a file cargo bench --message-format=json &gt; main.json # Checkout the feature branch git checkout feature # Run the benchmarks again in your feature branch cargo bench --message-format=json &gt; feature.json # Perform a comparison check between the two branches ./scripts/criterion_compare.rs main.json feature.json table   If the benchmarks indicate a degradation exceeding 10%, the script will terminate with an error. You can refer to the automatically generated benches/benchmark.md file to identify which benchmarks underperformed and investigate the corresponding code changes before submitting a pull request. ","version":"Next","tagName":"h2"},{"title":"Telemetry","type":0,"sectionRef":"#","url":"/docs/contributors/telemetry/","content":"Telemetry At Tailcall, we adhere to high observability standards in line with the OpenTelemetry specification. Our implementation utilizes several key Rust crates: rust-opentelemetry and associated crates are used to support the collection and export of telemetry data.tracing and tracing-opentelemetry facilitate the definition of logs and traces. Integration with OpenTelemetry allows for the automatic transfer of this data to the OpenTelemetry system. This layered approach ensures that the tracing library, which is effective across various scenarios, can also function as a standalone telemetry system for logging when OpenTelemetry integration is not required. When developing any features that necessitate observability, consider the following guidelines: Implement traces for tasks that represent a significant operation. This practice aids in the efficient diagnosis of issues and performance bottlenecks.Name spans clearly and specifically, adhering to the guidelines outlined in the OpenTelemetry specifications. Avoid names that introduce a high cardinality of potential values.Due to the constraints of tracing libraries, span names must be static strings. This limitation can be overcome by adding an extra field named otel.name to provide more dynamic descriptions (see the tracing-opentelemetry documentation for more details).Attribute naming should follow OpenTelemetry's semantic conventions. Utilize constants available in the opentelemetry_semantic_conventions crate for standardized attribute names.","keywords":"","version":"Next"},{"title":"Getting Started with GraphQL","type":0,"sectionRef":"#","url":"/docs/","content":"","keywords":"","version":"Next"},{"title":"Installing the Tailcall CLI​","type":1,"pageTitle":"Getting Started with GraphQL","url":"/docs/#installing-the-tailcall-cli","content":"  You can install the latest version - by using NPM.   ","version":"Next","tagName":"h2"},{"title":"NPM​","type":1,"pageTitle":"Getting Started with GraphQL","url":"/docs/#npm","content":" If you don't already have nodejs installed, you can find the instructions here. Install Tailcall by running the following command in your terminal: npm i -g @tailcallhq/tailcall To verify the correct installation of Tailcall, run: tailcall note Do not use the --force flag during npm installations, as it ignores installing platform-specific builds.  ","version":"Next","tagName":"h3"},{"title":"Yarn​","type":1,"pageTitle":"Getting Started with GraphQL","url":"/docs/#yarn","content":" Install Tailcall by running the following command in your terminal: yarn global add @tailcallhq/tailcall To verify the correct installation of Tailcall, run: tailcall   ","version":"Next","tagName":"h3"},{"title":"Homebrew​","type":1,"pageTitle":"Getting Started with GraphQL","url":"/docs/#homebrew","content":" If you don't already have Homebrew installed, you can find the instructions here. Add the Tailcall repository to Homebrew by running the following command in your terminal: brew tap tailcallhq/tailcall brew install tailcall To verify the correct installation of Tailcall, run: tailcall After completing the installation, perform upgrades with: brew update brew upgrade tailcall   ","version":"Next","tagName":"h3"},{"title":"Curl​","type":1,"pageTitle":"Getting Started with GraphQL","url":"/docs/#curl","content":" Follow the steps below to manually install the cli on your system:  curl -sSL https://tailcall.run/install.sh | bash -s --   This command fetches and executes the Tailcall installation script. The ~/.tailcall directory contains the installed files.  Upon completion of the installation, extend your PATH environment variable to include the ~/.tailcall/bin directory:  # export PATH=$PATH:~/.tailcall/bin   ","version":"Next","tagName":"h3"},{"title":"Docker​","type":1,"pageTitle":"Getting Started with GraphQL","url":"/docs/#docker","content":" To install Tailcall with Docker, follow the steps below. Please note that currently, this installation method only works on Linux/amd64 systems. Before starting, make sure you have Docker installed on your system. If not, download it from here.  Pull the latest Tailcall Docker image using the following command: docker pull tailcall.docker.scarf.sh/tailcallhq/tailcall/tc-server: This command fetches the latest version of the Tailcall Docker image from the Docker registry. Run the Tailcall Docker container with the following command: docker run -d --name graphql-server -p 8000:8000 \\ -v /path/to/your/configs:/etc/tailcall \\ --entrypoint &quot;/bin/sh&quot; \\ ghcr.io/tailcallhq/tailcall/tc-server: \\ -c &quot;export PATH=$PATH:~/.tailcall/bin &amp;&amp; tailcall start /etc/tailcall/config.graphql&quot; This command launches the GraphQL server in a Docker container, exposing the GraphQL endpoint on port 8080.  ","version":"Next","tagName":"h3"},{"title":"Initializing a GraphQL project​","type":1,"pageTitle":"Getting Started with GraphQL","url":"/docs/#initializing-a-graphql-project","content":" Once you have installed the Tailcall binaries, you can simply use the init command to initialize your GraphQL project.  tailcall init &lt;directory&gt;   The command will ask you a few questions and based on your input bootstrap a new GraphQL project with a few files:  .tailcallrc.schema.json: Provides autocomplete in your editor when the configuration is written in json or yml format..graphqlrc.yml: An IDE configuration that references your GraphQL configuration (if it's in .graphql format) and the following .tailcallrc.graphql..tailcallrc.graphql: Contains Tailcall specific auto-completions for .graphql format.main.graphql: This is your root configuration that contains  ","version":"Next","tagName":"h2"},{"title":"Writing a GraphQL Configuration​","type":1,"pageTitle":"Getting Started with GraphQL","url":"/docs/#writing-a-graphql-configuration","content":" For our first example, we are going to compose a GraphQL schema from the REST APIs at https://jsonplaceholder.typicode.com, a free online REST API with some fake data. We will use the API at /users to get a list of users, and /users/:id/posts to get the posts for each user, and compose them into a single GraphQL schema.  We can use the following formats to define our GraphQL schema: .graphql, .yml, .json.  Create one of the following files and paste the contents into it.  graphqlymljson schema # Specify server configuration: Start GraphQL server at 0.0.0.0:8000 @server(port: 8000) # Specify a base url for all http requests @upstream(baseURL: &quot;http://jsonplaceholder.typicode.com&quot;) { query: Query } type Query { # Specify the http path for the users query users: [User] @http(path: &quot;/users&quot;) } # Create a user type with the fields returned by the users api type User { id: Int! name: String! username: String! email: String! # Extend the user type with the posts field # Use the current user's id to construct the path posts: [Post] @http(path: &quot;/users/{{.value.id}}/posts&quot;) } # Create a post type with the fields returned by the posts api type Post { id: Int! title: String! body: String! }   The above file is a standard .graphQL file, with some minor additions such as @upstream and @http directives. Basically we specify the GraphQL schema and how to resolve that GraphQL schema in the same file, without having to write any code!  ","version":"Next","tagName":"h2"},{"title":"Starting the GraphQL server​","type":1,"pageTitle":"Getting Started with GraphQL","url":"/docs/#starting-the-graphql-server","content":" Now, run the following command to start the server with the full path to the file that you created earlier.  graphqlymljson tailcall start ./jsonplaceholder.graphql   If the command succeeds, you should see logs like the following below.  INFO File read: ./jsonplaceholder.graphql ... ok INFO N + 1 detected: 0 INFO 🚀 Tailcall launched at [0.0.0.0:8000] over HTTP/1.1 INFO 🌍 Playground: https://tailcall.run/playground/?u=http://127.0.0.1:8000/graphql   The server starts with the schema provided and prints out a load of meta information. We will cover those in detail in a bit. For now, open the playground URL in a new tab in your browser and try it out for yourself!  ","version":"Next","tagName":"h2"},{"title":"Making GraphQL requests to the server​","type":1,"pageTitle":"Getting Started with GraphQL","url":"/docs/#making-graphql-requests-to-the-server","content":" Open a web browser and go to https://tailcall.run/playground/?u=http://127.0.0.1:8000/graphql. This should load the GraphiQL interface. In the query editor of GraphiQL, enter the following query query { users { id name posts { title } } } After running the query in GraphiQL, expect to see a JSON response structured like this: { &quot;data&quot;: { &quot;users&quot;: [ { &quot;id&quot;: 1, &quot;name&quot;: &quot;Leanne Graham&quot;, &quot;posts&quot;: [ { &quot;title&quot;: &quot;sunt aut facere repellat provident occaecati excepturi option reprehenderit&quot; } // Posts truncated for brevity ] }, { &quot;id&quot;: 2, &quot;name&quot;: &quot;Ervin Howell&quot;, &quot;posts&quot;: [ { &quot;title&quot;: &quot;et ea vero quia laudantium autem&quot; }, { &quot;title&quot;: &quot;in quibusdam tempore odit est dolorem&quot; } // Posts truncated for brevity ] } // Users truncated for brevity ] } }   ","version":"Next","tagName":"h2"},{"title":"Deploying GraphQL on Production​","type":1,"pageTitle":"Getting Started with GraphQL","url":"/docs/#deploying-graphql-on-production","content":" Now that you have a running GraphQL server, you can follow our Github Actions Guide to deploy the application on one of the following cloud providers.  AWS LambdaFly.io ","version":"Next","tagName":"h2"},{"title":"Macro Benchmarks","type":0,"sectionRef":"#","url":"/docs/contributors/wrk-benchmark/","content":"","keywords":"","version":"Next"},{"title":"Prerequisites​","type":1,"pageTitle":"Macro Benchmarks","url":"/docs/contributors/wrk-benchmark/#prerequisites","content":" Rust and Cargo (https://rustup.rs/)wrk benchmarking tool (Installation instructions: https://github.com/wg/wrk)  ","version":"Next","tagName":"h2"},{"title":"Step 1: Build Tailcall​","type":1,"pageTitle":"Macro Benchmarks","url":"/docs/contributors/wrk-benchmark/#step-1-build-tailcall","content":" Ensure you are on the desired branch you want to benchmark, and then build Tailcall in release mode to optimize performance:  cargo build --release   ","version":"Next","tagName":"h2"},{"title":"Step 2: Start the Server​","type":1,"pageTitle":"Macro Benchmarks","url":"/docs/contributors/wrk-benchmark/#step-2-start-the-server","content":" Start the Tailcall server by setting the appropriate environment variable to control log output and using the release binary:  export TC_LOG_LEVEL=error cargo run --release -- start ./jsonplaceholder.graphql   This command sets the log level to error to minimize logging output, which can affect performance during benchmarks.  ","version":"Next","tagName":"h2"},{"title":"Step 3: Verify Server is Running​","type":1,"pageTitle":"Macro Benchmarks","url":"/docs/contributors/wrk-benchmark/#step-3-verify-server-is-running","content":" Before running wrk, verify that the server is responsive. Use curl to send a request:  curl -X POST -H &quot;Content-Type: application/json&quot; \\ -d '{&quot;operationName&quot;:null,&quot;variables&quot;:{},&quot;query&quot;:&quot;{posts{title}}&quot;}' \\ http://127.0.0.1:8000/graphql   Repeat this a couple of times to ensure the server is handling requests correctly.  ","version":"Next","tagName":"h2"},{"title":"Step 4: Customize WRK Setup with Lua Script​","type":1,"pageTitle":"Macro Benchmarks","url":"/docs/contributors/wrk-benchmark/#step-4-customize-wrk-setup-with-lua-script","content":" To customize the wrk setup, create a Lua script named wrk_script.lua and paste the following content:  wrk.method = &quot;POST&quot; wrk.body = '{&quot;operationName&quot;:null,&quot;variables&quot;:{},&quot;query&quot;:&quot;{posts{title}}&quot;}' wrk.headers[&quot;Connection&quot;] = &quot;keep-alive&quot; wrk.headers[&quot;Content-Type&quot;] = &quot;application/json&quot;   This script configures wrk to send POST requests with a specific JSON body and headers.  ","version":"Next","tagName":"h2"},{"title":"Step 5: Run the Benchmark​","type":1,"pageTitle":"Macro Benchmarks","url":"/docs/contributors/wrk-benchmark/#step-5-run-the-benchmark","content":" Open another terminal window and execute wrk to start the benchmark. Here is a basic example:  wrk -t12 -c400 -d30s -s wrk_script.lua http://127.0.0.1:8000/graphql   This command uses 12 threads and maintains 400 open HTTP connections over a duration of 30 seconds, targeting the server running on localhost port 8000.  ","version":"Next","tagName":"h2"},{"title":"Step 6: Interpreting Results​","type":1,"pageTitle":"Macro Benchmarks","url":"/docs/contributors/wrk-benchmark/#step-6-interpreting-results","content":" wrk will output statistics about the tests, which include:  Total number of requests completedThroughput, measured in requests per secondLatency distribution  These metrics help assess the performance capabilities and robustness of your server under high load conditions. ","version":"Next","tagName":"h2"},{"title":"Integration Testing","type":0,"sectionRef":"#","url":"/docs/contributors/integration-testing/","content":"","keywords":"","version":"Next"},{"title":"How does it work?​","type":1,"pageTitle":"Integration Testing","url":"/docs/contributors/integration-testing/#how-does-it-work","content":" Execution Spec implements a custom markdown-based testing framework for Tailcall. The framework is designed to help write integration tests for GraphQL configs.  ","version":"Next","tagName":"h2"},{"title":"Run all tests​","type":1,"pageTitle":"Integration Testing","url":"/docs/contributors/integration-testing/#run-all-tests","content":" The integration tests are executed as usual integration test so you can use test options and filters like with usual test.  cargo test   To run integration tests skipping other tests run following command:  cargo test --test execution_spec   After running you will get an output of all executed integration tests.  ","version":"Next","tagName":"h3"},{"title":"Run a single test​","type":1,"pageTitle":"Integration Testing","url":"/docs/contributors/integration-testing/#run-a-single-test","content":" Similar to filtering unit tests to execute a single markdown configuration you can pass it's name to the test command:  cargo test --test execution_spec grpc Compiling tailcall-fixtures v0.1.0 (/Users/tushar/Documents/Projects/tailcall/tailcall-fixtures) Compiling tailcall v0.1.0 (/Users/tushar/Documents/Projects/tailcall) Finished `test` profile [unoptimized + debuginfo] target(s) in 15.96s Running tests/execution_spec.rs (target/debug/deps/execution_spec-6779d7c5c29b9b0b) running 18 tests test run_execution_spec::test-grpc-invalid-method-format.md ... ok test run_execution_spec::test-grpc-invalid-proto-id.md ... ok test run_execution_spec::test-grpc-group-by.md ... ok test run_execution_spec::test-grpc-missing-fields.md ... ok test run_execution_spec::test-grpc-nested-optional.md ... ok test run_execution_spec::test-grpc-nested-data.md ... ok test run_execution_spec::test-grpc-proto-path.md ... ok test run_execution_spec::grpc-proto-with-same-package.md ... ok test run_execution_spec::grpc-reflection.md ... ok test run_execution_spec::test-grpc-optional.md ... ok test run_execution_spec::test-grpc-service-method.md ... ok test run_execution_spec::test-grpc-service.md ... ok test run_execution_spec::grpc-error.md ... ok test run_execution_spec::grpc-simple.md ... ok test run_execution_spec::grpc-batch.md ... ok test run_execution_spec::grpc-url-from-upstream.md ... ok test run_execution_spec::grpc-override-url-from-upstream.md ... ok test run_execution_spec::test-grpc.md ... ok   In the above command all tests with the name grpc will be executed.  ","version":"Next","tagName":"h3"},{"title":"Skipping a test​","type":1,"pageTitle":"Integration Testing","url":"/docs/contributors/integration-testing/#skipping-a-test","content":" Skipping the test is also possible by passing the --skip parameter:  cargo test --test execution_spec -- --skip grpc   Sometimes, you might want to skip the test per permanently for everyone and the CI. You could achieve it by setting the skip configuration in your markdown:  --- skip: true --- &lt;!-- Rest of the configurations --&gt;   ","version":"Next","tagName":"h3"},{"title":"Folder Structure​","type":1,"pageTitle":"Integration Testing","url":"/docs/contributors/integration-testing/#folder-structure","content":" All execution_spec tests are located in tests/execution. The results generated by these tests are stored as snapshots in tests/core/snapshots. An execution_spec test is always a markdown file with a .md extension.  ","version":"Next","tagName":"h2"},{"title":"File Structure​","type":1,"pageTitle":"Integration Testing","url":"/docs/contributors/integration-testing/#file-structure","content":" Each .md file runs in its own scope, so no two tests can interfere with each other. The file structure is as follows:  ","version":"Next","tagName":"h2"},{"title":"Heading​","type":1,"pageTitle":"Integration Testing","url":"/docs/contributors/integration-testing/#heading","content":" The heading of file is used to provide metadata about the test. It is a YAML front matter block that contains the following fields:  identity - This instructs the runner to check if the configuration when parsed and then printed back, is the same as the original configuration. This is useful to check whenever a new feature is added in the configuration and the parsers + printer needs to be updated.error - This instructs the runner to expect a validation error while parsing the configuration. This is useful to test validation logic written while converting config to blueprint.skip - This is a special annotation that ensures that the test is skipped.  --- identity: true error: true skip: true ---   The rest of the file is the test's body consisting of code blocks and descriptions.  ","version":"Next","tagName":"h3"},{"title":"Config​","type":1,"pageTitle":"Integration Testing","url":"/docs/contributors/integration-testing/#config","content":" Codeblocks can be enhanced with additional meta information for the test parser to make sense of the code. So for example a GraphQL configuration could be written in a code block with the graphql language and a @config meta information could be attached to it.  ```graphql @config schema { query: Query } type Query { users: [User] posts: [Post] } ```   For each config a few tests are automatically executed:  We check if the config written is valid. If it's not and unless error: true is set in the front matter, the test will fail.We check if the config when parsed and then printed back is the same as the original config. This is useful to check whenever a new feature is added in the configuration and the parsers + printer needs to be updated.We check if the config when merged with an empty configuration is the same as the original config. This is useful to check whenever a new feature is added in the configuration and the merger needs to be updated.We autogenerate the schema of the GraphQL server and snapshot it for later. This is useful to see what would the final GraphQL schema look like.  ","version":"Next","tagName":"h3"},{"title":"Test​","type":1,"pageTitle":"Integration Testing","url":"/docs/contributors/integration-testing/#test","content":" An @test block specifies HTTP requests that the runner should perform in YAML format. It solely contains requests. The response for each request is automatically generated and compared with the snapshot.  note There may be at most one @test block in a test.  Example:  ```yml @test - method: POST url: http://localhost:8080/graphql body: query: query { user { name } } ```   ","version":"Next","tagName":"h3"},{"title":"Mock​","type":1,"pageTitle":"Integration Testing","url":"/docs/contributors/integration-testing/#mock","content":" Mock provides a way to match requests and send back a predefined response. It is used to mock HTTP &amp; gRPC requests in the test.  ```yml @mock - request: # The method to match on (default: Any) method: POST # The URL to match on (default: Any) url: http://jsonplaceholder.typicode.com/users/1 # Predefined response response: status: 200 body: id: 1 name: foo # Number of time we expect this request to be hit (default: 1) expectedHits: 1 # Whether we should assert the number of hits (default: true) assertHits: true ```   ","version":"Next","tagName":"h3"},{"title":"Env​","type":1,"pageTitle":"Integration Testing","url":"/docs/contributors/integration-testing/#env","content":" An @env block specifies environment variables in YAML that the runner should use in the app context. There may be at most one @env block in a test.  Example:  ```yml @env TEST_ID: 1 ```   ","version":"Next","tagName":"h3"},{"title":"File​","type":1,"pageTitle":"Integration Testing","url":"/docs/contributors/integration-testing/#file","content":" A @file block creates a file in the spec's virtual file system. The @config block will have exclusive access to files created in this way: the true filesystem is not available to it.  Every @file block has the filename declared in the header. The language of the code block is optional and does not matter.  Example:  ```js @file:worker.js function onRequest({request}) { request.headers[&quot;x-test&quot;] = &quot;test&quot; return {request} } ``` ```graphql @config schema @link(file: &quot;worker.js&quot;) { query: Query } ```   In the above example we are able to link the worker.js file to the schema and write an integration test where all the requests will be modified by the onRequest function.  ","version":"Next","tagName":"h3"},{"title":"Snapshots​","type":1,"pageTitle":"Integration Testing","url":"/docs/contributors/integration-testing/#snapshots","content":" Tailcall uses the Insta snapshot engine. Snapshots are automatically generated with a .new suffix if there is no pre-existing snapshot, or if the compared data didn't match the existing snapshot.  Instead of writing result cases in tests and updating them when behaviour changes, a snapshot-based testing workflow relies on auto-generation. Whenever a .new snapshot is generated, it means one of the following:  Your code made an unexpected breaking change, and you need to fix it.Your code made an expected breaking change, and you need to accept the new snapshot.  You need to determine which one is the case, and take action accordingly.  Usage of cargo-insta is recommended:  cargo insta test --review   This will regenerate all snapshots without interrupting the test every time there's a diff, and it will also open the snapshot review interface, so that you can accept or reject .new snapshots.  To clean unused snapshots, run:  cargo insta test --delete-unreferenced-snapshots  ","version":"Next","tagName":"h2"},{"title":"Unit Testing","type":0,"sectionRef":"#","url":"/docs/contributors/testing/","content":"","keywords":"","version":"Next"},{"title":"Running Tests​","type":1,"pageTitle":"Unit Testing","url":"/docs/contributors/testing/#running-tests","content":" To execute tests locally on your machine, follow these steps:  Ensure the Rust toolchain is installed on your machine. Execute all tests with the following command in the terminal: cargo test To run a specific test or group of tests, modify the command accordingly: cargo test test_name To view all output from tests (useful if you have added debug logs to your tests), use the command: cargo test -- --show-output For more details and options on how tests function, please refer to the Rust Book Testing Chapter and the rustc tests guide.  ","version":"Next","tagName":"h2"},{"title":"Filtering Running Tests​","type":1,"pageTitle":"Unit Testing","url":"/docs/contributors/testing/#filtering-running-tests","content":" To execute a specific set of tests or exclude some tests, use the following commands:  To run tests that match a certain pattern:  cargo test test_pattern # e.g., to run grpc related tests: cargo test grpc   To run a specific test by passing the full module path:  cargo test -- --exact test_name # e.g., for grpc protobuf conversion: cargo test -- --exact grpc::protobuf::tests::convert_value   To skip certain tests:  cargo test -- --skip test_pattern # e.g., to ignore grpc related tests: cargo test -- --skip grpc   For more available options, please refer to rustc filter's documentation.  ","version":"Next","tagName":"h3"},{"title":"Writing Tests​","type":1,"pageTitle":"Unit Testing","url":"/docs/contributors/testing/#writing-tests","content":" ","version":"Next","tagName":"h2"},{"title":"Unit Tests​","type":1,"pageTitle":"Unit Testing","url":"/docs/contributors/testing/#unit-tests","content":" Unit tests should focus on individual components, ensuring each functions as expected:  Place unit tests in the same file as your code, under a #[cfg(test)] module. Use descriptive function names for your tests, for eg: #[cfg(test)] mod tests { #[test] fn test_addition() { assert_eq!(2 + 2, 4); } } For every new feature or bug fix, structure your tests as follows: Set up the value using helper methods in tests. Compare an actual and an expected value. Assert the two values on separate lines. Ensure there is one assertion per test. For eg: use pretty_assertions::assert_eq; fn test_something_important() { // Setup let value = setup_something_using_a_function(); // Compute Actual let actual = perform_some_operation_on_the_value(value); // Compute Expected let expected = ExpectedValue {foo: 1, bar: 2}; // Compare Actual and Expected assert_eq!(actual, expected); } Before submitting a pull request, verify all tests pass.  ","version":"Next","tagName":"h3"},{"title":"Integration Tests​","type":1,"pageTitle":"Unit Testing","url":"/docs/contributors/testing/#integration-tests","content":" Integration testing is conducted using our markdown-based DSL. Please refer to its own documentation for detailed information.  ","version":"Next","tagName":"h3"},{"title":"Naming Conventions​","type":1,"pageTitle":"Unit Testing","url":"/docs/contributors/testing/#naming-conventions","content":" Test functions should begin with test_ followed by a description of their purpose.Use underscores to separate words in the test function names for readability.  ","version":"Next","tagName":"h2"},{"title":"What to Test​","type":1,"pageTitle":"Unit Testing","url":"/docs/contributors/testing/#what-to-test","content":" In essence, test everything! Write unit tests for modules that can be tested independently and supplement them with integration tests to ensure the overall system stability.  ","version":"Next","tagName":"h2"},{"title":"Troubleshooting Common Issues​","type":1,"pageTitle":"Unit Testing","url":"/docs/contributors/testing/#troubleshooting-common-issues","content":" Ensure your branch is up-to-date with the latest commits from the main branch.Verify that your environment conforms to the required configurations (e.g., versions of Rust and dependencies).Confirm that test failures are not caused by your changes (e.g., run tests with a clean build on the main branch). ","version":"Next","tagName":"h2"},{"title":"Mutability","type":0,"sectionRef":"#","url":"/docs/contributors/mutability/","content":"","keywords":"","version":"Next"},{"title":"Using References​","type":1,"pageTitle":"Mutability","url":"/docs/contributors/mutability/#using-references","content":" When calling functions that do not need to modify the values they receive, pass references to these values. This avoids unnecessary copying and preserves the original data integrity.💰  Consider a function that calculates the total number of items in a list. This function does not alter the list, so pass the list as a reference:  fn count_items(items: &amp;Vec&lt;i32&gt;) -&gt; usize { items.len() } let my_items = vec![1, 2, 3]; let total = count_items(&amp;my_items);   ","version":"Next","tagName":"h2"},{"title":"Using Ownership​","type":1,"pageTitle":"Mutability","url":"/docs/contributors/mutability/#using-ownership","content":" When calling functions that need to modify the values they receive, pass ownership of these values to the function. This makes it clear that the function might change the value. Ensure that the modified values are returned from the function if further use is required.  Consider a function that adds an item to a list. Since this modifies the list, pass the list with ownership and return the modified list:  fn add_item(mut items: Vec&lt;i32&gt;, item: i32) -&gt; Vec&lt;i32&gt; { items.push(item); items } let my_items = vec![1, 2, 3]; let updated_items = add_item(my_items, 4);   ","version":"Next","tagName":"h2"},{"title":"Using Mutable References​","type":1,"pageTitle":"Mutability","url":"/docs/contributors/mutability/#using-mutable-references","content":" Mutable references are particularly useful when you need to modify the data a function receives without taking ownership of it. This approach is ideal for types that behave like classical stateful services, where maintaining state across multiple function calls is necessary.  Consider a caching mechanism where data needs to be frequently updated or retrieved based on function calls. In this case, using a mutable reference allows the cache to be updated without transferring ownership each time:  struct Cache { data: HashMap&lt;String, String&gt;, } impl Cache { fn add_entry(&amp;mut self, key: String, value: String) { self.data.insert(key, value); } fn get_entry(&amp;self, key: &amp;str) -&gt; Option&lt;&amp;String&gt; { self.data.get(key) } } let mut my_cache = Cache { data: HashMap::new() }; my_cache.add_entry(&quot;session1&quot;.to_string(), &quot;User123&quot;.to_string()); if let Some(user) = my_cache.get_entry(&quot;session1&quot;) { println!(&quot;Cached user: {}&quot;, user); }   note Even though in Rust mutability is a lot more tamed than other languages, as a standard we try to stay away from mutable references as much as possible.  ","version":"Next","tagName":"h2"},{"title":"Exceptions​","type":1,"pageTitle":"Mutability","url":"/docs/contributors/mutability/#exceptions","content":" The approach outlined above may not be suitable for performance-sensitive components or frequently executed sections of code (hot code paths). In such scenarios, prioritize efficiency and adopt optimization strategies to enhance performance. Sometimes the API design of a dependent library can also influence the way we write code. These are all the exceptions where it's ok to move away from the above set guidelines. ","version":"Next","tagName":"h2"},{"title":"Deploy Tailcall GraphQL on Fly.io","type":0,"sectionRef":"#","url":"/docs/deploy-tailcall-graphql-fly-actions/","content":"","keywords":"","version":"Next"},{"title":"Generate API Key for Fly.io​","type":1,"pageTitle":"Deploy Tailcall GraphQL on Fly.io","url":"/docs/deploy-tailcall-graphql-fly-actions/#generate-api-key-for-flyio","content":" Follow these steps to generate an API key:  Go to the Fly.io dashboard. Click on Tokens in the left sidebar. Optionally, provide a name and an expiry date for the token. Click on Create Organization Token to generate the token. Copy the generated token and store it securely. You will need this token as input to the tailcallhq/gh-action when deploying to Fly.io.  ","version":"Next","tagName":"h2"},{"title":"Setting Up the Project Repository​","type":1,"pageTitle":"Deploy Tailcall GraphQL on Fly.io","url":"/docs/deploy-tailcall-graphql-fly-actions/#setting-up-the-project-repository","content":" Next, create a new repository on GitHub and use the tailcallhq/gh-action GitHub action to deploy it. The easiest way to get started is by using this template repository: https://github.com/tailcallhq/deploy-tailcall.  Go to the repository and click on Use this template to create a new repository. Name your repository and click on Create repository. After creating the repository, add the Fly.io API token to the repository secrets. To do this, click on Settings. Click on Secrets and variables in the left sidebar to expand the section, then click on Actions. Click on New repository secret to add a new secret. Name the secret FLY_API_TOKEN or any preferred name, and paste the Fly.io API token you generated earlier into the value field. Click on Add secret to save it.  You are now ready to deploy your tailcall server on Fly.io.  ","version":"Next","tagName":"h2"},{"title":"Deploy on Fly.io​","type":1,"pageTitle":"Deploy Tailcall GraphQL on Fly.io","url":"/docs/deploy-tailcall-graphql-fly-actions/#deploy-on-flyio","content":" In this example, we will deploy a simple GraphQL server using tailcall on Fly.io, which converts the JSONPlaceholder REST API to a GraphQL API.  Below is the configuration present in the template repository, which will be used for this deployment.  tip You can learn more about the configuration here  schema @upstream( baseURL: &quot;http://jsonplaceholder.typicode.com&quot; ) { query: Query } type Query { posts: [Post] @http(path: &quot;/posts&quot;) } type User { id: Int! name: String! username: String! email: String! phone: String website: String } type Post { id: Int! userId: Int! title: String! body: String! user: User @http(path: &quot;/users/{{.value.userId}}&quot;) }   To deploy the server, update the provider to fly in the deploy-tailcall job in the .github/workflows/main.yml file, as shown below.  on: [push] jobs: deploy_tailcall: runs-on: ubuntu-latest name: Deploy Tailcall steps: - name: Checkout repository uses: actions/checkout@v2 - name: Deploy Tailcall id: deploy-tailcall uses: tailcallhq/gh-action@&lt;version&gt; # Replace &lt;version&gt; with the desired version with: provider: &quot;fly&quot; # Specifies the cloud provider as 'fly' fly-api-token: ${{ secrets.FLY_API_TOKEN }} fly-app-name: &lt;app-name&gt; # Replace &lt;app-name&gt; with the desired app name fly-region: &quot;lax&quot; tailcall-config: &quot;./config.graphql&quot;   important When specifying the fly-app-name in your GitHub Actions workflow for deploying to Fly.io, ensure the app name you choose is unique across all Fly.io users.  Fly.io requires each app name to be globally unique. If the name you select is already taken by another user, your deployment will fail. To avoid this issue, consider using a name that includes unique identifiers such as your organization name, project name, etc. If you do not specify the app name, &lt;orgname&gt;-&lt;reponame&gt; will be used.  After updating the main.yml file, commit the changes and push them to the repository. This will trigger the deployment of the tailcall server on Fly.io. Once the deployment is successful, you can access the GraphQL playground at https://tailcall.run/playground/?u=https://&lt;fly-app-name&gt;.fly.dev/graphql. ","version":"Next","tagName":"h2"},{"title":"Optimizing Performance of your GraphQL Server","type":0,"sectionRef":"#","url":"/docs/graphql-client-performance-tuning/","content":"","keywords":"","version":"Next"},{"title":"HTTP (Hypertext Transfer Protocol)​","type":1,"pageTitle":"Optimizing Performance of your GraphQL Server","url":"/docs/graphql-client-performance-tuning/#http-hypertext-transfer-protocol","content":" HTTP, the most widely used protocol for communication between clients and servers, carries your request to the server and then brings back the data to your client. TCP forms the foundation of HTTP.  ","version":"Next","tagName":"h3"},{"title":"HTTP Versions: 1.x, 2, and 3​","type":1,"pageTitle":"Optimizing Performance of your GraphQL Server","url":"/docs/graphql-client-performance-tuning/#http-versions-1x-2-and-3","content":" Each version has enhanced HTTP's flexibility and performance.  HTTP/1.x: Creates a separate TCP connection for each HTTP request (or reuses one sequentially).HTTP/2: Introduces multiplexing to allow concurrent sending of requests and responses over a single TCP connection, enhancing performance.HTTP/3: Employs QUIC instead of TCP, further reducing connection setup time and improving packet loss and network change handling.  note The server determines the HTTP version. Thus, if the server supports HTTP/1, the client cannot make an HTTP/2 request, even if compatible. If the client supports HTTP/1, the server should, according to the specification, downgrade to serve the request over HTTP/1.  ","version":"Next","tagName":"h3"},{"title":"TCP (Transmission Control Protocol)​","type":1,"pageTitle":"Optimizing Performance of your GraphQL Server","url":"/docs/graphql-client-performance-tuning/#tcp-transmission-control-protocol","content":" TCP ensures the data sent and received over the internet reaches its destination and in order.  TCP, like dialing a number before talking on the phone, establishes a connection between the client and server before exchanging data using HTTP. This guide will show how to tune Tailcall's HTTP client to enhance this connection's performance. Learn more about TCP in detail here.  ","version":"Next","tagName":"h3"},{"title":"QUIC (Quick UDP Internet Connections)​","type":1,"pageTitle":"Optimizing Performance of your GraphQL Server","url":"/docs/graphql-client-performance-tuning/#quic-quick-udp-internet-connections","content":" Developed by Google, QUIC aims to make web communications faster and more efficient than TCP. It reduces connection establishment time, handles packet loss better, and supports multiplexed streams over a single connection, preventing a slow request from holding up others. HTTP/3 uses QUIC. Learn more about QUIC in detail here.  ","version":"Next","tagName":"h3"},{"title":"Why Managing Connections is Important?​","type":1,"pageTitle":"Optimizing Performance of your GraphQL Server","url":"/docs/graphql-client-performance-tuning/#why-managing-connections-is-important","content":" Performance Overhead: Establishing TCP connections with HTTP/1.x consumes time due to the complete TCP handshake for each new connection. This process adds latency and increases system resources. Limited Ports on Client Side: A unique combination of an IP address and a port number is necessary for each TCP connection from a client. With each new connection, the IP remains the same because the client is the same, but a new port gets used. The number of available ports on a machine is 65535. These ports get shared among all processes, and not all are available for use. Excessive creation of new connections can lead to port exhaustion on the client side, preventing new connections and causing system failures across running processes. tip Use lsof and netstat commands to check the ports to process mapping.  Connection pooling mitigates these issues by reusing existing connections for requests, reducing connection establishment frequency (and thus handshake overhead) and conserving client-side ports. This approach enhances application performance by minimizing the resources and time spent on managing connections.  ","version":"Next","tagName":"h3"},{"title":"Tuning HTTP Client​","type":1,"pageTitle":"Optimizing Performance of your GraphQL Server","url":"/docs/graphql-client-performance-tuning/#tuning-http-client","content":" Tailcall uses connection pooling by default and sets up with default tuning suitable for most use cases. You might need to further tune the HTTP client to improve your application's performance. Tailcall DSL provides a directive named @upstream for this purpose.  note Connection pooling optimizes HTTP/1. Since HTTP/2 and HTTP/3 support multiplexing, pooling enabled does not noticeably affect performance.  When using HTTP/1.x, tune the connection pool with the following parameters:  ","version":"Next","tagName":"h2"},{"title":"poolMaxIdlePerHost​","type":1,"pageTitle":"Optimizing Performance of your GraphQL Server","url":"/docs/graphql-client-performance-tuning/#poolmaxidleperhost","content":" poolMaxIdlePerHost specifies the allowed number of idle connections per host, defaulting to 60. Example:  schema @upstream( poolMaxIdlePerHost: 60 ) { query: Query }   Too idle connections can unnecessarily consume memory and ports, while too few might cause delays as new connections need frequent establishment. poolMaxIdlePerHost ensures judicious use of network and memory resources, avoiding wastage on seldom-used connections.  For applications connecting to hosts, set this value lower to keep connections available for other hosts. Conversely, if you have hosts and all requests must resolve through them, maintain a higher value for this setting.  ","version":"Next","tagName":"h3"},{"title":"tcpKeepAlive​","type":1,"pageTitle":"Optimizing Performance of your GraphQL Server","url":"/docs/graphql-client-performance-tuning/#tcpkeepalive","content":" tcpKeepAlive keeps TCP connections alive for a duration, during inactivity, by periodically sending packets to the server to check if the connection remains open. In connection pooling, tcpKeepAlive maintains reusable connections in a ready-to-use state. This setting is useful for long-lived connections, preventing -lived connections, preventing the client from using a connection the server has closed due to inactivity. Without tcpKeepAlive, connections in the pool might get dropped by the server or intermediate network devices (like firewalls or load balancers). When your client tries to use such a dropped connection, it would fail, causing delays and errors. Keeping connections alive and monitored means you can efficiently reuse them, reducing the overhead of establishing new connections frequently.  Tailcall provides a parameter named tcpKeepAlive for the upstream which defaults to 5 seconds. Example: schema  @upstream ( tcpKeepAlive: 300 ) { query: Query }   ","version":"Next","tagName":"h3"},{"title":"connectTimeout​","type":1,"pageTitle":"Optimizing Performance of your GraphQL Server","url":"/docs/graphql-client-performance-tuning/#connecttimeout","content":" connectTimeout specifically applies to the phase where your client attempts to establish a connection with the server. When making a connection request, the client tries to resolve the DNS, complete the SSL handshake, and establish a TCP connection. In environments where pods are frequently created and destroyed, maintaining a low connectTimeout is crucial to avoid unnecessary delays. In systems using connection pooling, the system aborts the attempt if it cannot establish a connection within the connectTimeout period. This approach prevents indefinite waiting for a connection to establish, which could cause delays and timeouts.  Tailcall offers a connectTimeout parameter to set the connection timeout in seconds for the HTTP client, defaulting to 60 seconds. Example:  schema @upstream( connectTimeout: 10 ) { query: Query }   In summary, maximizing HTTP client performance requires understanding the underlying protocols and configuring client settings through testing. This ensures efficient, robust, and high-performing client-server communication, crucial for the smooth operation of modern web applications. ","version":"Next","tagName":"h3"},{"title":"Github Action for Deploying GraphQL","type":0,"sectionRef":"#","url":"/docs/deploy-graphql-github-actions/","content":"","keywords":"","version":"Next"},{"title":"Deploying to Fly​","type":1,"pageTitle":"Github Action for Deploying GraphQL","url":"/docs/deploy-graphql-github-actions/#deploying-to-fly","content":" Below is an example of how to deploy a tailcall server to Fly using the tailcallhq/gh-action action.  on: [push] jobs: deploy_tailcall: runs-on: ubuntu-latest name: Deploy Tailcall steps: - name: Checkout repository uses: actions/checkout@v2 - name: Deploy Tailcall id: deploy-tailcall uses: tailcallhq/gh-action@&lt;version&gt; # Replace &lt;version&gt; with the desired version with: provider: &quot;fly&quot; # Specifies the cloud provider as 'fly' fly-api-token: ${{ secrets.FLY_API_TOKEN }} fly-app-name: &quot;tailcall&quot; fly-region: &quot;lax&quot; tailcall-config: &quot;config.graphql&quot;   ","version":"Next","tagName":"h2"},{"title":"Inputs for tailcallhq/gh-action​","type":1,"pageTitle":"Github Action for Deploying GraphQL","url":"/docs/deploy-graphql-github-actions/#inputs-for-tailcallhqgh-action","content":" Following are the inputs for the tailcallhq/gh-action action when deploying to Fly:  Input\tDescriptionprovider\tWhen deploying to Fly, this should be set to fly. tailcall-config\tThe path of the tailcall configuration file. tailcall-version\tSpecifies the version of tailcall to use for deployment. If not provided, the Action defaults to the latest available version. fly-api-token\tThe Fly API token required for authentication. Ensure this value is stored securely, such as in GitHub Secrets. fly-app-name\tThe name of the Fly app being deployed. Defaults to &lt;orgname&gt;-&lt;reponame&gt; if not specified. fly-region\tThe region where the Fly app will be deployed. Defaults to ord if not specified.  ","version":"Next","tagName":"h3"},{"title":"Deploying to AWS Lambda​","type":1,"pageTitle":"Github Action for Deploying GraphQL","url":"/docs/deploy-graphql-github-actions/#deploying-to-aws-lambda","content":" on: [push] jobs: deploy_tailcall: runs-on: ubuntu-latest name: Deploy Tailcall steps: - name: Checkout repository uses: actions/checkout@v2 - name: Deploy Tailcall id: deploy-tailcall uses: tailcallhq/gh-action@&lt;version&gt; # Replace &lt;version&gt; with the desired version with: provider: &quot;aws&quot; aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }} aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }} aws-region: &quot;us-east-1&quot; aws-iam-role: &quot;iam_for_tailcall&quot; terraform-api-token: ${{ secrets.TERRAFORM_API_TOKEN }} tailcall-config: &quot;config.graphql&quot;   ","version":"Next","tagName":"h2"},{"title":"Inputs for tailcallhq/gh-action​","type":1,"pageTitle":"Github Action for Deploying GraphQL","url":"/docs/deploy-graphql-github-actions/#inputs-for-tailcallhqgh-action-1","content":" Following are the inputs for the tailcallhq/gh-action action when deploying to AWS Lambda:  Input\tDescriptionprovider\tWhen deploying to AWS Lambda, this should be set to aws. tailcall-config\tThe path to the tailcall configuration file used for deployment. tailcall-version\tSpecifies the version of tailcall to use for deployment. If not provided, the Action defaults to the latest available version. aws-access-key-id\tThe AWS access key ID required for authentication. Ensure this value is stored securely, such as in GitHub Secrets. aws-secret-access-key\tThe AWS secret access key required for authentication. Store this securely, such as in GitHub Secrets. aws-region\tThe AWS region where the Lambda function will be deployed (e.g., us-east-1). aws-iam-role\tThe IAM role name to be created and used for the deployment. If not specified, defaults to iam_for_tailcall. aws-lambda-function-name\tThe name assigned to the created Lambda function. Defaults to tailcall if not specified. terraform-api-token\tThe Terraform Cloud API token required for authentication. Ensure this value is stored securely, such as in GitHub Secrets. ","version":"Next","tagName":"h3"},{"title":"GraphQL Best Practices","type":0,"sectionRef":"#","url":"/docs/graphql-best-practices-tailcall/","content":"","keywords":"","version":"Next"},{"title":"General Naming Principles​","type":1,"pageTitle":"GraphQL Best Practices","url":"/docs/graphql-best-practices-tailcall/#general-naming-principles","content":" Consistency is Key: Ensure that naming conventions are uniform across your entire schema to maintain clarity and consistency.Descriptive Over Generic: Opt for descriptive, specific names rather than broad, generic ones to avoid ambiguity.Avoid Abbreviations: Avoid the use of acronyms, initialism, and abbreviations to keep your schema intuitive and understandable.  ","version":"Next","tagName":"h2"},{"title":"Detailed Naming Cases​","type":1,"pageTitle":"GraphQL Best Practices","url":"/docs/graphql-best-practices-tailcall/#detailed-naming-cases","content":" ","version":"Next","tagName":"h2"},{"title":"Fields, Arguments, and Directives​","type":1,"pageTitle":"GraphQL Best Practices","url":"/docs/graphql-best-practices-tailcall/#fields-arguments-and-directives","content":" Adopt camelCase: Utilize camelCase for field names, argument names, and directive names to achieve a clear, consistent structure.  type Query { postTitle(userId: Int): String } directive @includeIf on FIELD   ","version":"Next","tagName":"h3"},{"title":"Types​","type":1,"pageTitle":"GraphQL Best Practices","url":"/docs/graphql-best-practices-tailcall/#types","content":" Prefer PascalCase: Use PascalCase for defining types, enabling easy identification and differentiation.  type Post { ... } enum StatusEnum { ... } interface UserInterface { ... } union SearchResult = ... scalar Date   Enum Values in SCREAMING_SNAKE_CASE: Distinguish enum values by using SCREAMING_SNAKE_CASE.  enum StatusEnum { PUBLISHED DRAFT }   ","version":"Next","tagName":"h3"},{"title":"Field Naming Best Practices​","type":1,"pageTitle":"GraphQL Best Practices","url":"/docs/graphql-best-practices-tailcall/#field-naming-best-practices","content":" ","version":"Next","tagName":"h2"},{"title":"Queries​","type":1,"pageTitle":"GraphQL Best Practices","url":"/docs/graphql-best-practices-tailcall/#queries","content":" Avoid get or list Prefixes: Refrain from using prefixes like get or list in your query names to ensure predictability and consistency.  type Query { # 👎 Avoid getPosts: [Post] # 👍 Prefer posts: [Post] }   Maintain consistency between root and nested fields:  # 👎 Avoid query PostQuery { getPosts { id getUser { name } } } # 👍 Prefer query PostQuery { posts { id user { name } } }   ","version":"Next","tagName":"h3"},{"title":"Mutations​","type":1,"pageTitle":"GraphQL Best Practices","url":"/docs/graphql-best-practices-tailcall/#mutations","content":" Verb Prefixes for Mutations: Begin mutation field names with a verb to indicate the action being performed, improving schema readability.  type Mutation { # 👎 Avoid postAdd(input: AddPostInput): AddPostPayload! # 👍 Prefer addPost(input: AddPostInput): AddPostPayload! }   ","version":"Next","tagName":"h3"},{"title":"Type Naming Conventions​","type":1,"pageTitle":"GraphQL Best Practices","url":"/docs/graphql-best-practices-tailcall/#type-naming-conventions","content":" ","version":"Next","tagName":"h2"},{"title":"Input Types​","type":1,"pageTitle":"GraphQL Best Practices","url":"/docs/graphql-best-practices-tailcall/#input-types","content":" Input Suffix: Denote input types by appending Input to their names, specifying their use case.  input AddPostInput { title: String! body: String! userId: Int! }   ","version":"Next","tagName":"h3"},{"title":"Output Types​","type":1,"pageTitle":"GraphQL Best Practices","url":"/docs/graphql-best-practices-tailcall/#output-types","content":" Response or Payload Suffix: Use a consistent suffix like Response or Payload for the output types resulting from mutations.  type Mutation { addPost(input: AddPostInput!): AddPostResponse! } type AddPostResponse { success: Boolean! post: Post }   ","version":"Next","tagName":"h3"},{"title":"Advanced Naming Strategies​","type":1,"pageTitle":"GraphQL Best Practices","url":"/docs/graphql-best-practices-tailcall/#advanced-naming-strategies","content":" ","version":"Next","tagName":"h2"},{"title":"Resolving Namespace Conflicts​","type":1,"pageTitle":"GraphQL Best Practices","url":"/docs/graphql-best-practices-tailcall/#resolving-namespace-conflicts","content":" For addressing naming conflicts across different domains within your schema:  Use PascalCase Prefix: Distinguish similar types from distinct domains for clear separation without resorting to underscores. This method ensures a cleaner, more professional look while maintaining the integrity and readability of your schema.  type BlogPost { ... } type ForumPost { ... }   ","version":"Next","tagName":"h3"},{"title":"Conclusion​","type":1,"pageTitle":"GraphQL Best Practices","url":"/docs/graphql-best-practices-tailcall/#conclusion","content":" Implementing a consistent, descriptive, and intuitive naming convention is crucial for developing an understandable and maintainable GraphQL schema. By following the best practices outlined you can improve the clarity and effectiveness of your schema. ","version":"Next","tagName":"h2"},{"title":"Field Level GraphQL Authentication","type":0,"sectionRef":"#","url":"/docs/field-level-access-control-graphql-authentication/","content":"","keywords":"","version":"Next"},{"title":"What is Authentication?​","type":1,"pageTitle":"Field Level GraphQL Authentication","url":"/docs/field-level-access-control-graphql-authentication/#what-is-authentication","content":" Authentication is the process of verifying a user's identity before granting access to data. In most modern applications, some information, such as a list of products in an e-commerce application, is accessible to all users without requiring identification. However, personal data, like a user's order history, is accessible to the user who owns that information. Verifying a user's identity to access such personal data is known as authentication.  The primary reasons for implementing authentication in an application include:  Protecting User-Specific Data Ensuring that data belonging to a user is not accessible by others.Security The ability to block users based on certain criteria necessitates identifying them.Customized User Experiences Delivering personalized experiences based on a user's identity.  Authentication can be implemented using credential validation mechanisms, such as:  Basic AuthJWTOAuthAPI Key  ","version":"Next","tagName":"h2"},{"title":"Entity Level Authentication in GraphqQL​","type":1,"pageTitle":"Field Level GraphQL Authentication","url":"/docs/field-level-access-control-graphql-authentication/#entity-level-authentication-in-graphqql","content":" Entity level authentication in GraphQL refers to applying authentication logic to specific entities or types within your GraphQL schema, rather than at the API entry point or resolver level for individual queries or mutations. This approach allows you to control access to particular data types or fields based on the user's authentication status, enabling a more granular and flexible security model.  Advantages of this approach:  Flexibility: Tailors security measures to precisely fit the needs of your application, enhancing the protection of sensitive data.Scalability: Facilitates extending security policies to new entities and fields as your schema expands.Customization: Enables implementing different authentication mechanisms for distinct entities based on their security requirements.  ","version":"Next","tagName":"h3"},{"title":"GraphQL Authentication​","type":1,"pageTitle":"Field Level GraphQL Authentication","url":"/docs/field-level-access-control-graphql-authentication/#graphql-authentication","content":" Tailcall provides a straightforward way to implement entity level authentication in your GraphQL schema. By leveraging custom directives, you can define which entities or fields require authentication to access their data. Tailcall supports multiple authentication providers, such as Basic Auth and JWT, allowing you to choose the authentication mechanism that best suits your application's requirements. to know more about how to use it, read the following articles:  Basic AuthJWT  ","version":"Next","tagName":"h2"},{"title":"GraphQL Configuration​","type":1,"pageTitle":"Field Level GraphQL Authentication","url":"/docs/field-level-access-control-graphql-authentication/#graphql-configuration","content":" Enabling support for authentication in Tailcall could be done in two steps:  With the help of @link directive connect multiple authentication files as you need for different provides. To connect it use either Htpasswd or Jwks link typeMark that some type of field requires authentication to be fetched with the help of @protected directive  Your config could look like this now:  schema @server(port: 8000) @upstream(baseURL: &quot;http://jsonplaceholder.typicode.com&quot;) @link(id: &quot;auth-basic&quot;, type: Htpasswd, src: &quot;htpasswd&quot;) @link(id: &quot;auth-jwt&quot;, type: Jwks, src: &quot;jwks.json&quot;) { query: Query mutation: Mutation } type Query { posts: [Post] @http(path: &quot;/posts&quot;) user(id: Int!): User @http(path: &quot;/users/{{.args.id}}&quot;) } type Mutation { user(id: Int!): User @http(path: &quot;/users/{{.args.id}}&quot;) } type User @protected { id: Int! name: String! username: String! email: String! phone: String website: String } type Post { id: Int! userId: Int! title: String! body: String! @protected user: User @http(path: &quot;/users/{{.value.userId}}&quot;) }   In that case the whole User type and Post.body are marked as protected and therefore requiring authentication to resolve its content. That means following points:  any query for Post.body will require authenticationany query for any field of User will require authenticationany field that resolves to User type will require authentication  For more info about possible configuration for available providers read articles for Basic Auth and JWT  ","version":"Next","tagName":"h2"},{"title":"Making test requests​","type":1,"pageTitle":"Field Level GraphQL Authentication","url":"/docs/field-level-access-control-graphql-authentication/#making-test-requests","content":" Now let's try to access some data from the example above. Start the graphql server with provided config and use graphQL playground that should be opened automatically in your browser.  If you execute the query that don't have any @protected fields like  { posts { title } }   Then the data for this will be resolved as usual without providing any additional info. showing the list of posts with their titles:But if you change the query to access protected data, then if you don't provide any authentication data, i.e. for query:  { posts { body } }   You will get an authentication failure error stating that authentication parameters were not provided. e.g.:  { &quot;data&quot;: null, &quot;errors&quot;: [ { &quot;message&quot;: &quot;Authentication Failure: Missing Authorization Header.&quot;, &quot;locations&quot;: [ { &quot;line&quot;: 3, &quot;column&quot;: 5 } ] } ] }     Now update the request by providing additional Authorization header. You can do in the Playground by navigating to the tab HTTP HEADERS at the bottom by adding following header for Basic Auth:  { &quot;Authorization&quot;: &quot;Basic dGVzdHVzZXIxOnBhc3N3b3JkMTIzs&quot; }   Now after executing the request again you'll get the response for all the requested fields without any error.  ","version":"Next","tagName":"h2"},{"title":"How it works​","type":1,"pageTitle":"Field Level GraphQL Authentication","url":"/docs/field-level-access-control-graphql-authentication/#how-it-works","content":" ","version":"Next","tagName":"h2"},{"title":"@protected Type​","type":1,"pageTitle":"Field Level GraphQL Authentication","url":"/docs/field-level-access-control-graphql-authentication/#protected-type","content":" If type is marked with @protected then:  attempt to request any field of that type will require authenticationattempt to request any field from other type that resolves to protected type will require authentication and the underlying IO operation won't be executed without it  ","version":"Next","tagName":"h3"},{"title":"Mutation​","type":1,"pageTitle":"Field Level GraphQL Authentication","url":"/docs/field-level-access-control-graphql-authentication/#mutation","content":" For mutation entity level authentication works similar to queries. But since mutation involves requests that changes external state you should be careful where do you specify @protected directive because marking some nested field as protected doesn't prevent from executing the request to resolve the parent fields. I.e. following example is problematic:  schema { query: Query mutation: Mutation } type Query { user(id: Int!): User @http(path: &quot;/users/{{.args.id}}&quot;) } type Mutation { user(id: Int!): User @http(path: &quot;/users/{{.args.id}}&quot;, method: POST) } type User { id: Int! name: String! website: String @protected }   Here you can still execute the mutation without any authentication and fail on attempting to resolve website field.  To resolve this issue, consider marking root fields as protected in case they require authentication, i.e.:  schema { query: Query mutation: Mutation } type Query { user(id: Int!): User @http(path: &quot;/users/{{.args.id}}&quot;) } type Mutation { user(id: Int!): User @http(path: &quot;/users/{{.args.id}}&quot;, method: POST) @protected } type User { id: Int! name: String! website: String @protected }   ","version":"Next","tagName":"h3"},{"title":"Multiple auth providers​","type":1,"pageTitle":"Field Level GraphQL Authentication","url":"/docs/field-level-access-control-graphql-authentication/#multiple-auth-providers","content":" In case you linked multiple authentication files all of them will be used to execute validation process. In that case, by default, Tailcall will validate all of them in parallel and succeed if at least one of them succeed.  ","version":"Next","tagName":"h3"},{"title":"Authentication headers​","type":1,"pageTitle":"Field Level GraphQL Authentication","url":"/docs/field-level-access-control-graphql-authentication/#authentication-headers","content":" To validate authentication for user request the specific headers are used (like Authorization header). In case auth is enabled for tailcall those headers will be also added to the allowedHeaders list and therefore they will be forwarded to the upstream requests implicitly.  ","version":"Next","tagName":"h3"},{"title":"Basic Authentication​","type":1,"pageTitle":"Field Level GraphQL Authentication","url":"/docs/field-level-access-control-graphql-authentication/#basic-authentication","content":" Basic Authentication is a straightforward authentication scheme that sends base64-encoded usernames and passwords in the HTTP Authorization header with each request. It's simple to implement but requires HTTPS to ensure security due to its lack of encryption.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Field Level GraphQL Authentication","url":"/docs/field-level-access-control-graphql-authentication/#prerequisites","content":" To be able to use Basic Authentication support you should have configured htpasswd file that contains users credentials data.  To generate this data you can use Apache tooling itself or available web-tool  important Since this file stores secure information make sure to hash the password you use with secure algorithms  ","version":"Next","tagName":"h3"},{"title":"Basic Auth GraphQL Configuration​","type":1,"pageTitle":"Field Level GraphQL Authentication","url":"/docs/field-level-access-control-graphql-authentication/#basic-auth-graphql-configuration","content":" To use Basic Auth you should first include htpasswd file generated from Prerequisites with the help of @link directive.  We can use that file as an example for it that has data for testuser:mypassword credentials in encrypted format:  htpasswd testuser:$2y$10$wJ/mZDURcAOBIrswCAKFsO0Nk7BpHmWl/XuhF7lNm3gBAFH3ofsuu   After adding @link you can use the @protected directive to mark the fields that requiring success authentication to be requested.  The whole example could look like this:  schema @server(port: 8000) @upstream(baseURL: &quot;http://jsonplaceholder.typicode.com&quot;) @link(id: &quot;auth-basic&quot;, type: Htpasswd, src: &quot;htpasswd&quot;) { query: Query } type Query { user(id: Int!): User @http(path: &quot;/users/{{.args.id}}&quot;) } type User @protected { id: Int! name: String! username: String! email: String! phone: String website: String }   ","version":"Next","tagName":"h3"},{"title":"Making test request​","type":1,"pageTitle":"Field Level GraphQL Authentication","url":"/docs/field-level-access-control-graphql-authentication/#making-test-request","content":" Now you can run the example file with Tailcall and try to make a query for data with specifying credentials.  To make the request first create base64 encoded string from the testuser:mypassword string and then append the result to the Authorization: Basic header.  A request example with curl:  curl --request POST \\ --url http://localhost:8000/graphql \\ --header 'Authorization: Basic dGVzdHVzZXI6bXlwYXNzd29yZA==' \\ --data '{&quot;query&quot;:&quot;query {\\n\\tuser(id: 1) { name }\\n}&quot;}'   or you can use the GraphQL Playground and add the header in the HTTP Headers section:  { &quot;Authorization&quot;: &quot;Basic dGVzdHVzZXIyOm15cGFzc3dvcmQ=&quot; }   with query:  query { user(id: 1) { name } }   Executing such request should be resolved with the user and its name.  ","version":"Next","tagName":"h3"},{"title":"JWT Authentication​","type":1,"pageTitle":"Field Level GraphQL Authentication","url":"/docs/field-level-access-control-graphql-authentication/#jwt-authentication","content":" JWT Authentication uses digitally signed tokens to authenticate and transmit user information in a compact JSON format, allowing stateless and secure communication between clients and servers. It offers greater flexibility and security, supporting expiration times and custom data embedding within the token itself.  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Field Level GraphQL Authentication","url":"/docs/field-level-access-control-graphql-authentication/#prerequisites-1","content":" To be able to use JWT authentication you should have configured JSON Web Key Sets (JWKS for short) file.  To create this file you can use available web-tools like JWK creator in case you already have RSA key-pair or mkjwk if you don't.  ","version":"Next","tagName":"h3"},{"title":"JWT Auth GraphQL Configuration​","type":1,"pageTitle":"Field Level GraphQL Authentication","url":"/docs/field-level-access-control-graphql-authentication/#jwt-auth-graphql-configuration","content":" To use JWT you should first include JWKS file generated from Prerequisites with the help of @link directive.  We can use that file as an example for it:  jwks.json { &quot;keys&quot;: [ { &quot;kty&quot;: &quot;RSA&quot;, &quot;use&quot;: &quot;sig&quot;, &quot;alg&quot;: &quot;RS256&quot;, &quot;kid&quot;: &quot;I48qMJp566SSKQogYXYtHBo9q6ZcEKHixNPeNoxV1c8&quot;, &quot;n&quot;: &quot;ksMb5oMlhJ_HzAebCuBG6-v5Qc4J111ur7Aux6-8SbxzqFONsf2Bw6ATG8pAfNeZ-USA3_T1mGkYTDvfoggXnxsduWV_lePZKKOq_Qp_EDdzic1bVTJQDad3CXldR3wV6UFDtMx6cCLXxPZM5n76e7ybPt0iNgwoGpJE28emMZJXrnEUFzxwFMq61UlzWEumYqW3uOUVp7r5XAF5jQ_1nQAnpHBnRFzdNPVb3E6odMGu3jgp8mkPbPMP16Fund4LVplLz8yrsE9TdVrSdYJThylRWn_BwvJ0DjUcp8ibJya86iClUlixAmBwR9NdStHwQqHwmMXMKkTXo-ytRmSUobzxX9T8ESkij6iBhQpmDMD3FbkK30Y7pUVEBBOyDfNcWOhholjOj9CRrxu9to5rc2wvufe24VlbKb9wngS_uGfK4AYvVyrcjdYMFkdqw-Mft14HwzdO2BTS0TeMDZuLmYhj_bu5_g2Zu6PH5OpIXF6Fi8_679pCG8wWAcFQrFrM0eA70wD_SqD_BXn6pWRpFXlcRy_7PWTZ3QmC7ycQFR6Wc6Px44y1xDUoq3rH0RlZkeicfvP6FRlpjFU7xF6LjAfd9ciYBZfJll6PE7zf-i_ZXEslv-tJ5-30-I4Slwj0tDrZ2Z54OgAg07AIwAiI5o4y-0vmuhUscNpfZsGAGhE&quot;, &quot;e&quot;: &quot;AQAB&quot; } ] }   After adding @link you can use the @protected directive to mark the fields that requiring success authentication to be requested.  The whole example could look like this:  schema @server(port: 8000) @upstream(baseURL: &quot;http://jsonplaceholder.typicode.com&quot;) @link(id: &quot;auth-jwks&quot;, type: Jwks, src: &quot;jwks.json&quot;) { query: Query } type Query { user(id: Int!): User @http(path: &quot;/users/{{.args.id}}&quot;) } type User @protected { id: Int! name: String! username: String! email: String! phone: String website: String }   ","version":"Next","tagName":"h3"},{"title":"Making test request​","type":1,"pageTitle":"Field Level GraphQL Authentication","url":"/docs/field-level-access-control-graphql-authentication/#making-test-request-1","content":" Now you can run the example file with Tailcall and try to make a query for data with specifying credentials.  To make the request first obtain JWT token compatible with JWKS file you've linked before (if you've used the example jwks.json file from above then you can use the token from the example below).  An request example with curl:  curl --request POST \\ --url http://localhost:8000/graphql \\ --header 'Authorization: Bearer eyJhbGciOiJSUzI1NiIsImtpZCI6Ikk0OHFNSnA1NjZTU0tRb2dZWFl0SEJvOXE2WmNFS0hpeE5QZU5veFYxYzgifQ.eyJleHAiOjIwMTkwNTY0NDEuMCwiaXNzIjoibWUiLCJzdWIiOiJ5b3UiLCJhdWQiOlsidGhlbSJdfQ.cU-hJgVGWxK3-IBggYBChhf3FzibBKjuDLtq2urJ99FVXIGZls0VMXjyNW7yHhLLuif_9t2N5UIUIq-hwXVv7rrGRPCGrlqKU0jsUH251Spy7_ppG5_B2LsG3cBJcwkD4AVz8qjT3AaE_vYZ4WnH-CQ-F5Vm7wiYZgbdyU8xgKoH85KAxaCdJJlYOi8mApE9_zcdmTNJrTNd9sp7PX3lXSUu9AWlrZkyO-HhVbXFunVtfduDuTeVXxP8iw1wt6171CFbPmQJU_b3xCornzyFKmhSc36yvlDfoPPclWmWeyOfFEp9lVhQm0WhfDK7GiuRtaOxD-tOvpTjpcoZBeJb7bSg2OsneyeM_33a0WoPmjHw8WIxbroJz_PrfE72_TzbcTSDttKAv_e75PE48Vvx0661miFv4Gq8RBzMl2G3pQMEVCOm83v7BpodfN_YVJcqZJjVHMA70TZQ4K3L4_i9sIK9jJFfwEDVM7nsDnUu96n4vKs1fVvAuieCIPAJrfNOUMy7TwLvhnhUARsKnzmtNNrJuDhhBx-X93AHcG3micXgnqkFdKn6-ZUZ63I2KEdmjwKmLTRrv4n4eZKrRN-OrHPI4gLxJUhmyPAHzZrikMVBcDYfALqyki5SeKkwd4v0JAm87QzR4YwMdKErr0Xa5JrZqHGe2TZgVO4hIc-KrPw' \\ --data '{&quot;query&quot;:&quot;query {\\n\\tuser(id: 1) { name }\\n}&quot;}'   Executing such request should be resolved with the user and its name. ","version":"Next","tagName":"h3"},{"title":"Sequencing & Parallelism","type":0,"sectionRef":"#","url":"/docs/graphql-data-access-parallel-vs-sequence/","content":"","keywords":"","version":"Next"},{"title":"Examples​","type":1,"pageTitle":"Sequencing & Parallelism","url":"/docs/graphql-data-access-parallel-vs-sequence/#examples","content":" ","version":"Next","tagName":"h2"},{"title":"Example 1: Fetching a Specific User and Their Posts​","type":1,"pageTitle":"Sequencing & Parallelism","url":"/docs/graphql-data-access-parallel-vs-sequence/#example-1-fetching-a-specific-user-and-their-posts","content":" Imagine you're building a blog and want to display a specific user's profile page containing their information and all their posts.  Schema:  type Query { # Retrieve a specific user by ID user(id: Int!): User @http(path: &quot;/users/{{.value.id}}&quot;) } type User { id: Int! name: String! username: String! email: String! # Access user's posts using their ID in the path posts: [Post] @http(path: &quot;/users/{{.value.id}}/posts&quot;) } type Post { id: Int! title: String! body: String! }   GraphQL Query:  query getUserAndPosts($userId: Int!) { # Fetch the user by ID user(id: $userId) { id name username email # Sequentially retrieve all posts for the fetched user posts { id title body } } }   Tailcall understands that retrieving the user's posts depends on knowing the user's ID, which is obtained in the first step. Therefore, it automatically fetches the user first and then uses their ID to retrieve all their posts in a sequential manner.  ","version":"Next","tagName":"h3"},{"title":"Example 2: Searching Multiple Posts and Users by ID​","type":1,"pageTitle":"Sequencing & Parallelism","url":"/docs/graphql-data-access-parallel-vs-sequence/#example-2-searching-multiple-posts-and-users-by-id","content":" Suppose you're building a social media platform and want to display profiles of specific users and their recent posts.  Schema:  type Query { # Retrieve users from the &quot;/users&quot; endpoint users: [User] @http(path: &quot;/users&quot;) } type User { id: Int! name: String! username: String! email: String! # Access user's posts using their ID in the path posts: [Post] @http(path: &quot;/users/{{.value.id}}/posts&quot;) } type Post { id: Int! title: String! body: String! }   GraphQL Query:  query getUsersWithLatestPosts { # Retrieve all users users { id name username email # Access user's posts through the nested field posts { id title body } } }   This query retrieves details of multiple users and their most recent posts based on the provided user IDs. Tailcall recognizes that fetching user details and their individual posts are independent tasks. As a result, it can execute these requests concurrently for each user.  ","version":"Next","tagName":"h3"},{"title":"Example 3: Fetching Posts with Users​","type":1,"pageTitle":"Sequencing & Parallelism","url":"/docs/graphql-data-access-parallel-vs-sequence/#example-3-fetching-posts-with-users","content":" Imagine you're building a social media platform and want to display a list of posts with each post's author. Traditionally, you might write a query that retrieves all posts and then, for each post, make a separate request to fetch its corresponding user. This approach leads to the N+1 problem, where N represents the number of posts, and 1 represents the additional request per post to retrieve its user.  Schema:  type Query { posts: [Post] @http(path: &quot;/posts&quot;) } type Post { id: Int! userId: Int! title: String! body: String! user: User @http(path: &quot;/users/{{.value.userId}}&quot;) } type User { id: Int! name: String! }   GraphQL Query:  query getPostsWithUsers { posts { id userId title body user { id name } } }   Tailcall analyzes the schema and recognizes that fetching user details for each post is independent. It can potentially execute these requests to /users/{{.value.userId}} concurrently, fetching user data for multiple posts simultaneously.  In summary, Tailcall automates the management of sequence and parallelism in API calls. It analyzes the defined schema to optimize execution, freeing developers from manual intervention. ","version":"Next","tagName":"h3"},{"title":"Data Dog Telemetry Integration","type":0,"sectionRef":"#","url":"/docs/graphql-data-dog-telemetry-tailcall/","content":"Data Dog Telemetry Integration This guide is based on the official doc. Go to datadoghq.comLogin to your account (make sure you choose right region for your account on login)Go to Organization Settings -&gt; API Keys and copy the value of existing key or create a new oneIntegration with datadog requires OpenTelemetry Collector to be able to send data to. As an example we can use following config for the collector: receivers: otlp: protocols: grpc: endpoint: 0.0.0.0:4317 exporters: logging: verbosity: detailed datadog: traces: span_name_as_resource_name: true hostname: &quot;otelcol&quot; api: key: ${DATADOG_API_KEY} # make sure to specify right datadog site based on # https://docs.datadoghq.com/getting_started/site/ site: us5.datadoghq.com processors: batch: datadog/processor: probabilistic_sampler: sampling_percentage: 30 service: pipelines: traces: receivers: [otlp] processors: [batch, datadog/processor] exporters: [datadog] metrics: receivers: [otlp] processors: [batch] exporters: [datadog] logs: receivers: [otlp] processors: [batch] exporters: [datadog] Go to your GraphQL configuration and update it to: schema @telemetry( export: {otlp: {url: &quot;http://localhost:4317&quot;}} ) { query: Query } Set the api key you've copied before to the environment variable named DATADOG_API_KEY and start Otel collector and tailcall with updated config Now make some requests to running service and wait a little bit until Datadog proceeds the data. After that you can go to APM -&gt; Traces, locate the span with name request and click on it. You should see something like on screenshot below: To see metrics now go to Metrics -&gt; Explorer and search for metric you want to see. After updating the query you should see something like on example below:","keywords":"","version":"Next"},{"title":"Reading Environment Variables","type":0,"sectionRef":"#","url":"/docs/graphql-environment-variables/","content":"","keywords":"","version":"Next"},{"title":"Need for Environment Variables​","type":1,"pageTitle":"Reading Environment Variables","url":"/docs/graphql-environment-variables/#need-for-environment-variables","content":" Applications rely on external tools, authentication methods, and configurations. For proper functioning, our code needs to access these values.  Consider a scenario of JWT authentication. When signing tokens for our users, we need:  Expiry time: The duration after which the token expires.Secret key: The key for encrypting the token.Issuer: The token issuer, often the organization's name.  There are two ways to manage this:  Hardcode the values in our code: This approach, while simple, poses a massive security risk by exposing sensitive information and requires code changes and application redeployment for updates. Store the values in environment variables: Storing sensitive values in the OS of the server running your application allows runtime access without code modifications, keeping sensitive information secure and simplifying value changes.  ","version":"Next","tagName":"h2"},{"title":"Environment Variables​","type":1,"pageTitle":"Reading Environment Variables","url":"/docs/graphql-environment-variables/#environment-variables","content":" With Tailcall, you can seamlessly integrate environment variables into your GraphQL schema. Tailcall supports this through a env Context variable. All directives share this Context, allowing you to resolve values in your schema.  Example schema:  type Query { users: [User]! @http( baseUrl: &quot;https://jsonplaceholder.typicode.com&quot; path: &quot;/users&quot; ) }   Here, we fetch a list of users from the JSONPlaceholder API. The users field will contain the fetched value at runtime. This works fine, but what if we want to change the API endpoint? We would need to update the code and redeploy the application, which is cumbersome.  We can address this issue using environment variables. Replace the API endpoint with an environment variable, allowing us to change the variable's value without altering our codebase.  type Query { users: [User]! @http(baseUrl: &quot;{{env.API_ENDPOINT}}&quot;, path: &quot;/users&quot;) }   Here, you must set API_ENDPOINT as an environment variable on the device running your server. Upon startup, the server retrieves this value and makes it accessible through the env Context variable.  This approach allows us to change the API endpoint without modifying our codebase. For instance, we might use different API endpoints for development (stage-api.example.com) and production (api.example.com) environments.  Remember, environment variables are not limited to the baseUrl or @http directive. You can use them throughout your schema, as a Mustache template handles their evaluation.  Here's another example, using an environment variable in the headers of @grpc:  type Query { users: [User] @grpc( service: &quot;UserService&quot; method: &quot;ListUsers&quot; protoPath: &quot;./proto/user_service.proto&quot; baseURL: &quot;https://grpc-server.example.com&quot; headers: [ {key: &quot;X-API-KEY&quot;, value: &quot;{{.env.API_KEY}}&quot;} ] ) }   ","version":"Next","tagName":"h2"},{"title":"Security Aspects and Best Practices​","type":1,"pageTitle":"Reading Environment Variables","url":"/docs/graphql-environment-variables/#security-aspects-and-best-practices","content":" Environment variables help reduce security risks, but it's crucial to understand that they do not remove these risks entirely because the values are in plain text. Even if configuration values are not always highly sensitive, there is still a potential for compromising secrets. To ensure your secrets remain secure, consider the following tips:  Use a .env file: It's a common practice to create a .env file in your project's root directory for storing all environment variables. Avoid committing this file to your version control system; instead, add it to .gitignore to prevent public exposure of your secrets. For clarity and collaboration, maintain a .env.example file that enumerates all the necessary environment variables for your application, thereby guiding other developers on what variables they need to set. Within Tailcall (or in other environments), you can make use of this .env file by exporting its key-value pairs to your operating system. For example, if your .env file looks like this: API_ENDPOINT=https://jsonplaceholder.typicode.com Export it to your OS with: export $(cat .env | xargs) On Windows: Get-Content .env | Foreach-Object { [System.Environment]::SetEnvironmentVariable($_.Split(&quot;=&quot;)[0], $_.Split(&quot;=&quot;)[1], &quot;User&quot;) } After this, you can access API_ENDPOINT in your codebase. Use Kubernetes Secrets: When deploying your application with Kubernetes, use its Secrets feature to manage environment variables. This approach ensures your secrets remain private and are not embedded in your codebase, while also making it easier to update values as necessary. Store Secrets Through Cloud Provider GUIs: For deployments using a cloud provider, use their GUI for environment variable management. These interfaces are intuitive and practical for containerized applications that automatically scale.  Following these practices ensures effective and secure management of your environment variables. ","version":"Next","tagName":"h2"},{"title":"Honeycomb Telemetry Integration","type":0,"sectionRef":"#","url":"/docs/graphql-honeycomb-telemetry-tailcall/","content":"Honeycomb Telemetry Integration Go to honeycomb.ioLogin to your accountGo to Account -&gt; Team Settings -&gt; Environments and API Keys -&gt; Configuration and create new or copy existing api keyGo to your GraphQL configuration and update settings: schema @telemetry( export: { otlp: { url: &quot;https://api.honeycomb.io:443&quot; headers: [ { key: &quot;x-honeycomb-team&quot; value: &quot;{{.env.HONEYCOMB_API_KEY}}&quot; } { key: &quot;x-honeycomb-dataset&quot; value: &quot;&lt;your-dataset&gt;&quot; } ] } } ) { query: Query } Set the api key you've copied before to the environment variable named HONEYCOMB_API_KEY and start tailcall with updated config Now make some requests to running service and wait a little bit until honeycomb proceeds the data. After that you can go to Home -&gt; Total traces and click on the trace with name request. Now choose Traces in the bottom and click on the first trace from the list. You should see the picture similar to this: Here you can see data about the request that was made to the GraphQL server and what actions were made to handle this request. To see metrics now go Query and run a query to fetch the data about metrics. You can use following screenshot as an example:","keywords":"","version":"Next"},{"title":"Step-by-Step Tutorial: Building GraphQL over gRPC","type":0,"sectionRef":"#","url":"/docs/graphql-grpc-tailcall/","content":"","keywords":"","version":"Next"},{"title":"What is gRPC?​","type":1,"pageTitle":"Step-by-Step Tutorial: Building GraphQL over gRPC","url":"/docs/graphql-grpc-tailcall/#what-is-grpc","content":" This guide assumes a basic familiarity with gRPC. It is a high-performance framework created by Google for remote procedure calls (RPCs). Its key features include:  HTTP/2 Transport: Ensures efficient and fast data transfer.Protocol Buffers (Protobuf): Serves as a powerful interface description language.Efficiency: Offers binary serialization, reduces latency, and supports data streaming.  This combination of features makes gRPC ideal for microservices and distributed systems. If you need a more detailed understanding or are new to gRPC, we recommend visiting the official gRPC website for comprehensive documentation and resources.  Now, let's explore how gRPC can be integrated into our proxy gateway to enhance communication and data exchange in distributed systems.  ","version":"Next","tagName":"h2"},{"title":"gRPC upstream​","type":1,"pageTitle":"Step-by-Step Tutorial: Building GraphQL over gRPC","url":"/docs/graphql-grpc-tailcall/#grpc-upstream","content":" We need some gRPC service available to be able to execute requests from the Tailcall gateway. For pure example purposes, we will build some simple gRPC services.  ","version":"Next","tagName":"h2"},{"title":"Protobuf definition​","type":1,"pageTitle":"Step-by-Step Tutorial: Building GraphQL over gRPC","url":"/docs/graphql-grpc-tailcall/#protobuf-definition","content":" First, we need to create an example protobuf file that will define the structure of the data we want to transmit using gRPC. Here is the definition of NewsService that implements CRUD operations on news data that we'll put into the news.proto file.  syntax = &quot;proto3&quot;; import &quot;google/protobuf/empty.proto&quot;; package news; // Define message type for News with all its fields message News { int32 id = 1; string title = 2; string body = 3; string postImage = 4; } // Message with the id of a single news message NewsId { int32 id = 1; } // List of IDs of news to get multiple responses message MultipleNewsId { repeated NewsId ids = 1; } // List of all news message NewsList { repeated News news = 1; } // NewsService defines read and write operations for news items service NewsService { // GetAllNews retrieves all news items without any arguments rpc GetAllNews (google.protobuf.Empty) returns (NewsList) {} // GetNews fetches a single news item by its ID rpc GetNews (NewsId) returns (News) {} // GetMultipleNews retrieves multiple news items based on their IDs rpc GetMultipleNews (MultipleNewsId) returns (NewsList) {} }   ","version":"Next","tagName":"h3"},{"title":"Implement gRPC service​","type":1,"pageTitle":"Step-by-Step Tutorial: Building GraphQL over gRPC","url":"/docs/graphql-grpc-tailcall/#implement-grpc-service","content":" Now having the protobuf file you can write a server that implements NewsService at any language you want that supports gRPC. Tailcall organization has a sample node.js service inside this repo that you can pull to your local machine. To spin up the sample service run inside the repo and wait for logs about the service running.  npm i npm start   ","version":"Next","tagName":"h3"},{"title":"GraphQL Configuration for GRPC​","type":1,"pageTitle":"Step-by-Step Tutorial: Building GraphQL over gRPC","url":"/docs/graphql-grpc-tailcall/#graphql-configuration-for-grpc","content":" Now when we have a running gRPC service we're going to write Tailcall's config to make the integration. To do this we need to specify GraphQL types corresponding to gRPC types we have defined in the protobuf file. Let's create a new file grpc.graphql file with the following content:  # The GraphQL representation for News message type type News { id: Int title: String body: String postImage: String } # Input type that is used to fetch news data by its id input NewsInput { id: Int } # Resolves multiple news entries type NewsData { news: [News]! }   Now when we have corresponding types in schema we want to define GraphQL Query that specifies the operation we can execute onto news. We can extend our config with the next Query:  type Query { # Get all news i.e. NewsService.GetAllNews news: NewsData! # Get single news by id i.e. NewsService.GetNews newsById(news: NewsInput!): News! }   Also, let's specify options for Tailcall's ingress and egress at the beginning of the config using @server and @upstream directives.  schema @server(port: 8000) @upstream( baseURL: &quot;http://localhost:50051&quot; httpCache: 42 ) { query: Query }   To specify the protobuf file to read types from, use the @link directive with the type Protobuf on the schema. id is an important part of the definition that will be used by the @grpc directive later  schema @link(id: &quot;news&quot;, src: &quot;./news.proto&quot;, type: Protobuf)   Now you can connect GraphQL types to gRPC types. To do this you may want to explore more about @grpc directive. Its usage is pretty straightforward and requires you to specify the path to a method that should be used to make a call. The method name will start with the package name, followed by the service name and the method name, all separated by the . symbol.  If you need to provide any input to the gRPC method call you can specify it with the body option that allows you to specify a Mustache template and therefore it could use any input data like args and value to construct the body request. The body value is specified in the JSON format if you need to create the input manually and cannot use args input.  type Query { news: NewsData! @grpc(method: &quot;news.news.NewsService.GetAllNews&quot;) newsById(news: NewsInput!): News! @grpc( service: &quot;news.news.NewsService.GetNews&quot; body: &quot;{..args.news}}&quot; ) }   Wrapping up the whole result config that may look like this:  # file: app.graphql schema @server(port: 8000) @upstream( baseURL: &quot;http://localhost:50051&quot; httpCache: 42 ) @link(id: &quot;news&quot;, src: &quot;./news.proto&quot;, type: Protobuf) { query: Query } type Query { news: NewsData! @grpc(method: &quot;news.news.NewsService.GetAllNews&quot;) newsById(news: NewsInput!): News! @grpc( method: &quot;news.news.NewsService.GetNews&quot; body: &quot;{{.args.news}}&quot; ) } type News { id: Int title: String body: String postImage: String } input NewsInput { id: Int } type NewsData { news: [News]! }   Start the server by pointing it to the config.  tailcall start ./app.graphql   And now you can go to the page http://127.0.0.1:8000/graphql and execute some GraphQL queries e.g.:  { news { news { id title body } } }   Or  { newsById(news: {id: 2}) { id title body } }   ","version":"Next","tagName":"h2"},{"title":"Batching​","type":1,"pageTitle":"Step-by-Step Tutorial: Building GraphQL over gRPC","url":"/docs/graphql-grpc-tailcall/#batching","content":" Another important feature of the @grpc directive is that it allows you to implement request batching for remote data almost effortlessly as soon as you have gRPC methods that resolve multiple responses for multiple inputs in a single request.  In our protobuf example file, we have a method called GetMultipleNews that we can use. To enable batching we need to enable @upstream.batch option first and specify batchKey option for the @grpc directive.  schema @server(port: 8000) @upstream( baseURL: &quot;http://localhost:50051&quot; httpCache: 42 batch: {delay: 10} ) @link(id: &quot;news&quot;, src: &quot;./news.proto&quot;, type: Protobuf) { query: Query } type Query { newsById(news: NewsInput!): News! @grpc( method: &quot;news.NewsService.GetNews&quot; body: &quot;{{.args.news}}&quot; batchKey: [&quot;news&quot;, &quot;id&quot;] ) }   Restart the GraphQL server and make the query with multiple news separately, e.g.:  { n1: newsById(news: {id: 1}) { id title body } n2: newsById(news: {id: 2}) { id title body } }   Those 2 requests will be executed inside a single request to the gRPC method GetMultipleNews  ","version":"Next","tagName":"h2"},{"title":"Reflection​","type":1,"pageTitle":"Step-by-Step Tutorial: Building GraphQL over gRPC","url":"/docs/graphql-grpc-tailcall/#reflection","content":" gRPC reflection is a potent feature enabling clients to dynamically discover services and their methods at runtime. Tailcall enhances this capability by obviating the need for developers to link each proto file individually. This feature proves particularly valuable in environments where proto files are continuously evolving or when services dynamically expose varying methods. Here are the steps to follow:  Add the gRPC endpoint as a [link] with type set to Grpc. This enables the GraphQL server to understand that the specified source is a gRPC endpoint that supports reflection. schema @link( src: &quot;https://my-grpc-service.com:50051&quot; type: Grpc ) { query: Query } Next, as before we will just add the methods with a fully qualified name: type Query { news: [News] @grpc(method: &quot;news.NewsService.GetAllNews&quot;) } type News { id: Int title: String body: String postImage: String }   ","version":"Next","tagName":"h2"},{"title":"Conclusion​","type":1,"pageTitle":"Step-by-Step Tutorial: Building GraphQL over gRPC","url":"/docs/graphql-grpc-tailcall/#conclusion","content":" Well done on integrating a gRPC service with the Tailcall gateway! This tutorial has demonstrated the straightforward and efficient process, showcasing Tailcall's compatibility with advanced communication protocols like gRPC.  You can find this working example and test it by yourself by the next links:  node-grpc - example implementation for gRPC service in node.jsgRPC example config - Tailcall's config to integrate with gRPC service above  ","version":"Next","tagName":"h2"},{"title":"Key Takeaways​","type":1,"pageTitle":"Step-by-Step Tutorial: Building GraphQL over gRPC","url":"/docs/graphql-grpc-tailcall/#key-takeaways","content":" Simplicity of Integration: The integration of gRPC with Tailcall seamlessly enhances the overall capability of your system to handle high-performance and efficient data composition.Scalability and Performance: By leveraging the power of gRPC along with Tailcall, we've laid a foundation for building scalable and high-performing distributed systems.  ","version":"Next","tagName":"h3"},{"title":"Next Steps​","type":1,"pageTitle":"Step-by-Step Tutorial: Building GraphQL over gRPC","url":"/docs/graphql-grpc-tailcall/#next-steps","content":" With the basics in place, we encourage you to explore further:  Dive Deeper: Tailcall gateway offers a lot of other features and configurations that you can utilize. Dive deeper into our documentation to explore more advanced settings and customization options.Explore Other Guides: Our documentation includes a variety of guides and tutorials that can help you leverage the full potential of Tailcall in different scenarios. Whether it's adding security layers, load balancing, or detailed logging, there's a lot more to explore. ","version":"Next","tagName":"h3"},{"title":"Using HTTP Cache","type":0,"sectionRef":"#","url":"/docs/graphql-http-cache-guide-tailcall/","content":"","keywords":"","version":"Next"},{"title":"Understanding HTTP Caching​","type":1,"pageTitle":"Using HTTP Cache","url":"/docs/graphql-http-cache-guide-tailcall/#understanding-http-caching","content":" HTTP Caching involves saving copies of HTTP responses to serve identical future requests directly from the cache, bypassing the need for new API calls. This reduces latency, conserves bandwidth, and alleviates the load on upstream services by utilizing a cache keyed by request URLs and headers.  By default, HTTP caching is turned off in Tailcall. Enabling it requires setting the httpCache parameter to integer value which is greater than 0 in the @upstream configuration. Tailcall employs a in-memory Least_Recently_Used (LRU) cache mechanism to manage stored responses, adhering to upstream-provided caching directives like Cache-Control to optimize the caching process and minimize redundant upstream API requests.  ","version":"Next","tagName":"h3"},{"title":"Enabling HTTP Caching​","type":1,"pageTitle":"Using HTTP Cache","url":"/docs/graphql-http-cache-guide-tailcall/#enabling-http-caching","content":" To activate HTTP caching, adjust the upstream configuration in Tailcall by setting httpCache to appropriate cache size, as shown in the following example:  schema @server(port: 4000) @upstream( baseURL: &quot;https://api.example.com&quot; httpCache: 42 ) { query: Query }   This configuration instructs Tailcall to cache responses from the designated upstream API.  ","version":"Next","tagName":"h3"},{"title":"Cache-Control headers in responses​","type":1,"pageTitle":"Using HTTP Cache","url":"/docs/graphql-http-cache-guide-tailcall/#cache-control-headers-in-responses","content":" Enabling the cacheControl setting in Tailcall ensures that Cache-Control headers are included in the responses returned to clients. When activated, Tailcall dynamically sets the max-age directive in the Cache-Control header to the minimum max-age value encountered in any of the responses from upstream services. This approach guarantees that the caching duration for the composite response is conservative, aligning with the shortest cache validity period provided by the upstream services. By default, this feature is disabled (false), meaning Tailcall will not modify or add Cache-Control headers unless explicitly instructed to do so. This setting is distinct from the general HTTP cache setting, which controls whether responses are cached internally by Tailcall; cacheControl specifically controls the caching instructions sent to clients.  Here is how you can enable the cacheControl setting within your Tailcall schema to apply these caching instructions:  schema @server(headers: {cacheControl: true}) { query: Query mutation: Mutation }   ","version":"Next","tagName":"h3"},{"title":"Best Practices for Enhancing REST API Performance on GraphQL​","type":1,"pageTitle":"Using HTTP Cache","url":"/docs/graphql-http-cache-guide-tailcall/#best-practices-for-enhancing-rest-api-performance-on-graphql","content":" The combination of httpCache and cacheControl provides a comprehensive caching solution. While httpCache focuses on internal caching to reduce the impact of high latency and frequent requests, cacheControl manages client-side caching policies, ensuring an optimal balance between performance, data freshness, and efficient resource use.  These caching primitives are beneficial for REST APIs that are latency-sensitive, have a high rate of request repetition, or come with explicit caching headers indicating cacheable responses. Together, they tackle the common challenges of optimizing REST API performance by minimizing unnecessary network traffic and server load while ensuring response accuracy.  To further enhance the performance of any API with Tailcall, integrating the @cache directive offers protocol agnostic control over caching at the field level within a GraphQL schema. ","version":"Next","tagName":"h3"},{"title":"GraphQL over HTTP/2","type":0,"sectionRef":"#","url":"/docs/graphql-http2-guide-tailcall/","content":"","keywords":"","version":"Next"},{"title":"SSL​","type":1,"pageTitle":"GraphQL over HTTP/2","url":"/docs/graphql-http2-guide-tailcall/#ssl","content":" For Tailcall to serve GraphQL over HTTP/2 we need to first enable SSL for which we need to generate a certificate and a key. To generate the required certificates (cert.pem and key.pem) OpenSSL is a widely used option. Here are the steps to get started with SSL:  Install OpenSSL: Download and install OpenSSL from its official website if it's not already installed on your system. Generate Private Key openssl genrsa -out key.pem 2048 This creates a 2048-bit RSA private key, storing it in a file named key.pem. Generate Certificate Signing Request (CSR) openssl req -new -key key.pem -out csr.pem You will be prompted to provide information for the certificate, such as the Common Name (CN), organization details, and locality. This information is embedded into the CSR, saved in a file named csr.pem. This file can be used to request a certificate from a Certificate Authority (CA) or generate a self-signed certificate. Generate Self-Signed Certificate openssl x509 -req -days 365 -in csr.pem -signkey key.pem -out cert.pem This generates a self-signed certificate valid for 365 days using the CSR from step 3 and the private key from step 2. The validity period can be adjusted by changing the number of days (-days). A &quot;Signature ok&quot; prompt confirms the successful creation. Cleanup Intermediate Files rm csr.pem After using the CSR to generate the self-signed certificate (cert.pem), the CSR file (csr.pem) becomes redundant. This step removes intermediate files created during the certificate generation process.  tip Use self-signed certificates for HTTP/2 configurations in development environments. While they enable convenient HTTPS testing locally, in production, always opt for certificates issued by trusted Certificate Authorities.  ","version":"Next","tagName":"h2"},{"title":"Configuration​","type":1,"pageTitle":"GraphQL over HTTP/2","url":"/docs/graphql-http2-guide-tailcall/#configuration","content":" Once the certificate and key are generated we can link them with our main configuration using the @link directive, to enable HTTPS.  schema @link(type: &quot;Cert&quot;, src: &quot;./cert.pem&quot;) @link(type: &quot;Key&quot;, src: &quot;./key.pem&quot;) { query: Query mutation: Mutation } type Query { posts: [Post] @http(path: &quot;/posts&quot;) } type User { id: Int! name: String! }   Once HTTPS is enabled we set the version to HTTP2 for the server:  schema @link(type: &quot;Cert&quot;, src: &quot;./cert.pem&quot;) @link(type: &quot;Key&quot;, src: &quot;./key.pem&quot;) @server(version: HTTP2) { query: Query mutation: Mutation } type Query { posts: [Post] @http(path: &quot;/posts&quot;) } type User { id: Int! name: String! }   That's pretty much all that's required. Now you can go ahead and launch your server as usual.  INFO File read: ./jsonplaceholder.graphql ... ok INFO N + 1 detected: 0 INFO 🚀 Tailcall launched at [0.0.0.0:8000] over HTTP/2.0 INFO 🌍 Playground: https://tailcall.run/playground/?u=http://127.0.0.1:8000/graphql  ","version":"Next","tagName":"h2"},{"title":"Logging Levels Configuration","type":0,"sectionRef":"#","url":"/docs/graphql-logging-levels-tailcall/","content":"","keywords":"","version":"Next"},{"title":"error​","type":1,"pageTitle":"Logging Levels Configuration","url":"/docs/graphql-logging-levels-tailcall/#error","content":" This is the highest severity level. It indicates a critical issue that may lead to the failure of the program or a part of it.  TAILCALL_LOG_LEVEL=error tailcall &lt;COMMAND&gt; # or TC_LOG_LEVEL=error tailcall &lt;COMMAND&gt;   ","version":"Next","tagName":"h3"},{"title":"warn​","type":1,"pageTitle":"Logging Levels Configuration","url":"/docs/graphql-logging-levels-tailcall/#warn","content":" This log level signifies potential issues or warnings that do not necessarily result in immediate failure but may require attention.  TAILCALL_LOG_LEVEL=warn tailcall &lt;COMMAND&gt; # or TC_LOG_LEVEL=warn tailcall &lt;COMMAND&gt;   ","version":"Next","tagName":"h3"},{"title":"info​","type":1,"pageTitle":"Logging Levels Configuration","url":"/docs/graphql-logging-levels-tailcall/#info","content":" This level offers general information about the program's execution, providing insights into its state and activities.  TAILCALL_LOG_LEVEL=info tailcall &lt;COMMAND&gt; # or TC_LOG_LEVEL=info tailcall &lt;COMMAND&gt;   ","version":"Next","tagName":"h3"},{"title":"debug​","type":1,"pageTitle":"Logging Levels Configuration","url":"/docs/graphql-logging-levels-tailcall/#debug","content":" The debug log level is useful for developers during the debugging process, providing detailed information about the program's internal workings.  TAILCALL_LOG_LEVEL=debug tailcall &lt;COMMAND&gt; # or TC_LOG_LEVEL=debug tailcall &lt;COMMAND&gt;   ","version":"Next","tagName":"h3"},{"title":"trace​","type":1,"pageTitle":"Logging Levels Configuration","url":"/docs/graphql-logging-levels-tailcall/#trace","content":" The trace log level is the most detailed logging level, used for fine-grained debugging. This level provides exhaustive details about the program's execution flow.  TAILCALL_LOG_LEVEL=trace tailcall &lt;COMMAND&gt; # or TC_LOG_LEVEL=trace tailcall &lt;COMMAND&gt;   ","version":"Next","tagName":"h3"},{"title":"off​","type":1,"pageTitle":"Logging Levels Configuration","url":"/docs/graphql-logging-levels-tailcall/#off","content":" This level serves as a special indicator for generating no logs, allowing the option to disable logging entirely.  TAILCALL_LOG_LEVEL=off tailcall &lt;COMMAND&gt; # or TC_LOG_LEVEL=off tailcall &lt;COMMAND&gt;   info The default log level is info.  Log levels are hierarchical, meaning if you set the log level to a specific level, it includes all the levels above it. For example, setting the log level to info will include logs at the info, warn, and error levels, but exclude debug and trace logs.    info You can specify log levels in either uppercase or lowercase; both yield the same result. For example, TAILCALL_LOG_LEVEL=DEBUG and TAILCALL_LOG_LEVEL=debug are same. ","version":"Next","tagName":"h3"},{"title":"Customizing using Javascript","type":0,"sectionRef":"#","url":"/docs/graphql-javascript-customization/","content":"","keywords":"","version":"Next"},{"title":"Getting Started​","type":1,"pageTitle":"Customizing using Javascript","url":"/docs/graphql-javascript-customization/#getting-started","content":" To leverage this customization, JavaScript functions must be created in a JavaScript file and linked with the main configuration file using the @link directive. There are two primary ways to achieve this:  Define an onRequest property with the JS function name in the http directive.Define it in the upstream directive, which acts as a global middleware for all requests.  tip If you specify a onRequest handler for both http and upstream the http one will always take precedence over the global onRequest handler.  The function serves as middleware, allowing for the interception and modification of the request, as well as the production of artificial responses. Here is a simple example of a worker.js file with a function named foo, which takes a request object as an argument, logs the request, and returns the original request without any modifications.  function foo({request}) { console.log(`${request.method} ${request.uri.path}`) return {request} }   Once you have a worker file ready, link that file to the GraphQL configuration using the @link directive and define the onRequest property.  schema @link(type: Script, src: &quot;./worker.js&quot;) @upstream(onRequest: &quot;foo&quot;) { query: Query }   Now, you can start the server using the usual start command. Requests made to the GraphQL server will now be intercepted by the worker and logged to the console.  ","version":"Next","tagName":"h2"},{"title":"Modify Request​","type":1,"pageTitle":"Customizing using Javascript","url":"/docs/graphql-javascript-customization/#modify-request","content":" You can modify the request by returning a request object from the onRequest function. Below is an example where we are modifying the request to add a custom header.  function onRequest({request}) { request.headers[&quot;x-custom-header&quot;] = &quot;Hello, Tailcall!&quot; return {request} }   ","version":"Next","tagName":"h2"},{"title":"Create Response​","type":1,"pageTitle":"Customizing using Javascript","url":"/docs/graphql-javascript-customization/#create-response","content":" You can respond with custom responses by returning a response object from the onRequest function. Below is an example where we are responding with a custom response for all requests that start with https://api.example.com.  function onRequest({request}) { if (request.uri.path.startsWith(&quot;https://api.example.com&quot;)) { return { response: { status: 200, headers: { &quot;content-type&quot;: &quot;application/json&quot; }, body: JSON.stringify({message: &quot;Hello, Tailcall!&quot;}) } } } else { return {request} }   ","version":"Next","tagName":"h2"},{"title":"Response Redirect​","type":1,"pageTitle":"Customizing using Javascript","url":"/docs/graphql-javascript-customization/#response-redirect","content":" Sometimes you might want to redirect the request to a different URL. You can do this by returning a response object with a status of 301 or 302 and a Location header. The following example redirects all requests to https://example.com to https://tailcall.com.  function onRequest({request}) { if (request.uri.path.startsWith(&quot;https://example.com&quot;)) { return { response: { status: 301, headers: { Location: &quot;https://tailcall.com&quot;, }, }, } } else { return {request} } }   important The new request that's created as a result of the redirect will not be intercepted by the worker.  ","version":"Next","tagName":"h2"},{"title":"Schema​","type":1,"pageTitle":"Customizing using Javascript","url":"/docs/graphql-javascript-customization/#schema","content":" The onRequest function takes a single argument that contains the request object. The return value of the onRequest function can be a request object, or a response object. It can not be null or undefined.  ","version":"Next","tagName":"h2"},{"title":"Request​","type":1,"pageTitle":"Customizing using Javascript","url":"/docs/graphql-javascript-customization/#request","content":" The request object has the following shape:  type Request = { method: string uri: { path: string query?: {[key: string]: string} scheme: &quot;Http&quot; | &quot;Https&quot; host?: string port?: number } headers: {[key: string]: string} body?: string }   By default the headers field will be empty in most cases, unless headers are whitelisted via the allowedHeaders setting in @upstream.  The http filter doesn't have access to the request's body, hence you can't directly modify the body of an outgoing request. This is more of a design choice than a limitation we have made to ensure that developers don't misuse this API to write all kind of business logic in Tailcall.  tip As an escape hatch you can pass the request body as a query param instead of an actual request body and read in the JS.  The modified request that's returned from the above onRequest function can optionally provide the body. This body is used by Tailcall as the request body while making the upstream request.  ","version":"Next","tagName":"h3"},{"title":"Response​","type":1,"pageTitle":"Customizing using Javascript","url":"/docs/graphql-javascript-customization/#response","content":" The response object has the following shape:  type Response = { status: number headers: {[key: string]: string} body?: string }  ","version":"Next","tagName":"h3"},{"title":"New Relic Telemetry Integration","type":0,"sectionRef":"#","url":"/docs/graphql-newrelic-guide-telemetry/","content":"New Relic Telemetry Integration The guide is based on official doc Go to newrelic.comLogin to your accountGo to &lt;your user name&gt; -&gt; Api Keys and copy license value for key with access to write dataGo to GraphQL configuration and update it with: schema @telemetry( export: { otlp: { url: &quot;https://otlp.nr-data.net:4317&quot; headers: [ { key: &quot;api-key&quot; value: &quot;{{.env.NEWRELIC_API_KEY}}&quot; } ] } } ) { query: Query } Set the api key you've copied before to the environment variable named NEWRELIC_API_KEY and start tailcall with updated config Now make some requests to running service and wait a little bit until New Relic proceeds the data. After that you can go to Traces locate request trace, click on it, then pick one of the available traces and click on it. You should see something like the screenshot below: To see metrics now go to APM &amp; Services -&gt; Metrics Explorer and choose the metrics you want to see like on example below.","keywords":"","version":"Next"},{"title":"GraphQL Configuration Generation with Tailcall","type":0,"sectionRef":"#","url":"/docs/graphql-configuration-generation-with-tailcall/","content":"","keywords":"","version":"Next"},{"title":"What is Configuration Generation?​","type":1,"pageTitle":"GraphQL Configuration Generation with Tailcall","url":"/docs/graphql-configuration-generation-with-tailcall/#what-is-configuration-generation","content":" Configuration generation is the process of automatically generating graphQL configurations from the various sources such as REST, gRPC and already existing GraphQL configuration files.  ","version":"Next","tagName":"h2"},{"title":"Why is it Hard to Write GraphQL Schemas by Hand?​","type":1,"pageTitle":"GraphQL Configuration Generation with Tailcall","url":"/docs/graphql-configuration-generation-with-tailcall/#why-is-it-hard-to-write-graphql-schemas-by-hand","content":" Writing GraphQL schemas manually presents several challenges that can complicate and slow down the development process:  Complex API Responses: Large and Detailed Responses: APIs often return extensive and intricate data, making it laborious to map these responses accurately to GraphQL types.Nested Structures: Dealing with deeply nested JSON objects requires careful handling to ensure all relationships and data hierarchies are correctly represented in the schema. Data Consistency: Missing Properties: APIs can have inconsistent data where some items might lack certain properties, necessitating meticulous examination to define accurate types and optional fields.Dynamic Data: Handling APIs with dynamic data fields adds another layer of complexity, requiring flexible and robust schema definitions to accommodate various data shapes. Migration Efforts: Manual Workload: Converting existing REST APIs or gRPC to GraphQL involves substantial manual effort, such as Type and Schema Writing: Each endpoint must be meticulously mapped to corresponding GraphQL types and queries.Type Merging: Identifying types that are similar in configuration and merging them into single type is tedious and time taking task and prone to errors.Duplicate Type: Identifying and eliminating duplicate in the entire configuration is challenging especially for large scheams, to ensure a clean schema.Type Naming: Inferring and naming types manually, which requires a deep understanding of the underlying data structures and their relationships. Error-Prone Process: The manual creation of schemas increases the likelihood of errors, leading to potential issues in data fetching and integration.  These challenges highlight the need for automated tools, which streamline the process of generating GraphQL schemas, ensuring accuracy and efficiency while reducing the manual workload and error potential.  For more insights on why manual GraphQL schema writing is becoming obsolete, you can read this blog post by Tailcall.  ","version":"Next","tagName":"h3"},{"title":"Features​","type":1,"pageTitle":"GraphQL Configuration Generation with Tailcall","url":"/docs/graphql-configuration-generation-with-tailcall/#features","content":"   ","version":"Next","tagName":"h2"},{"title":"Effortless REST Integration​","type":1,"pageTitle":"GraphQL Configuration Generation with Tailcall","url":"/docs/graphql-configuration-generation-with-tailcall/#effortless-rest-integration","content":" Tailcall simplifies GraphQL schema generation from REST APIs, supporting various request types and scenarios. Let's understand this through various examples.  Simple GET Request: In the following example, we demonstrate how to generate a GraphQL schema from https://jsonplaceholder.typicode.com/posts endpoint. This configuration allows Tailcall to fetch data from the specified endpoint, generate a GraphQL schema and save it to output path provided in configuration. JSON Config FormatYML Config Format { &quot;inputs&quot;: [ { &quot;curl&quot;: { &quot;src&quot;: &quot;https://jsonplaceholder.typicode.com/posts&quot;, &quot;fieldName&quot;: &quot;posts&quot; } } ], &quot;preset&quot;: { &quot;mergeType&quot;: 1.0 }, &quot;output&quot;: { &quot;path&quot;: &quot;./jsonplaceholder.graphql&quot;, &quot;format&quot;: &quot;graphQL&quot; }, &quot;schema&quot;: { &quot;query&quot;: &quot;Query&quot; } } Let's understand the above configuration file. Input: Defines the API endpoints that the configuration interacts with. Each input specifies: src: Specifies the endpoint URL (https://jsonplaceholder.typicode.com/posts) in this example. fieldName: A unique name that should be used as the field name, which is then used in the operation type. In the example above, it's set to posts. important Ensure that each field name is unique across the entire configuration to prevent overwriting previous definitions. Preset: We've applied only one tuning parameter for the configuration. let's understand it in short. We've set mergeType to 1.0, which basically tells config generator to merge any two GraphQL types that are exactly similar. if you're interested in understanding preset's in detail head over to preset section. Output: Specifies where and in what format the output data should be saved. path: Defines the output file path (in above example, it's ./jsonplaceholder.graphql).format: Specifies the output format as GraphQL (in above example, it's graphQL). Schema: Specifies the name of the Query operation type, which is Query in this example. Generated GraphQL Configuration schema @server @upstream( baseURL: &quot;https://jsonplaceholder.typicode.com&quot; ) { query: Query } type Post { body: String id: Int title: String userId: Int } type Query { posts: [Post] @http(path: &quot;/posts&quot;) } GET Request with Headers In the following example, we demonstrate how to generate a GraphQL schema from https://jsonplaceholder.typicode.com/posts/1 endpoint which requires some headers in order to produce the response. This configuration allows Tailcall to fetch data from the specified endpoint with provided headers, generate a GraphQL schema and save it to output path provided in configuration. JSON Config FormatYML Config Format { &quot;inputs&quot;:[ { &quot;curl&quot;:{ &quot;src&quot;:&quot;https://jsonplaceholder.typicode.com/posts/1&quot;, &quot;fieldName&quot;:&quot;post&quot;, &quot;headers&quot;:{ &quot;Accept&quot;:&quot;application/json&quot;, &quot;secretToken&quot;:&quot;{{.env.TOKEN}}&quot; } } } ], &quot;preset&quot;:{ &quot;mergeType&quot;:1.0 }, &quot;output&quot;:{ &quot;path&quot;:&quot;./jsonplaceholder.graphql&quot;, &quot;format&quot;:&quot;graphQL&quot; }, &quot;schema&quot;:{ &quot;query&quot;:&quot;Query&quot; } } Let's understand the above configuration file. Input: Defines the API endpoints that the configuration interacts with. Each input specifies: src: Specifies the endpoint URL (https://jsonplaceholder.typicode.com/posts/1 in this example).fieldName: Assigns a descriptive name (post in this case) to uniquely identify the retrieved data.headers: Optional section for specifying HTTP headers required for the API request. tip Never store sensitive information like access tokens directly in configuration files. Leverage templates to securely reference secrets from environment variables. Preset: We've applied only one tuning parameter for the configuration. let's understand it in short. We've set mergeType to 1.0, which basically tells config generator to merge any two GraphQL types that are exactly similar. if you're interested in understanding preset's in detail head over to preset section. Output: Specifies where and in what format the output data should be saved. path: Defines the output file path (in above example, it's ./jsonplaceholder.graphql).format: Specifies the output format as GraphQL (in above example, it's graphQL). Schema: Specifies the name of the Query operation type, which is Query in this example.  Generated GraphQL Configuration schema @server @upstream( baseURL: &quot;https://jsonplaceholder.typicode.com&quot; ) { query: Query } type Post { body: String id: Int title: String userId: Int } type Query { post(p1: Int!): Post @http(path: &quot;/posts/{{.args.p1}}&quot;) }   ","version":"Next","tagName":"h3"},{"title":"Effortless gRPC Integration​","type":1,"pageTitle":"GraphQL Configuration Generation with Tailcall","url":"/docs/graphql-configuration-generation-with-tailcall/#effortless-grpc-integration","content":" Tailcall simplifies the process of generating GraphQL schemas from gRPC. By specifying the proto file path, Tailcall parses it and generates the corresponding GraphQL types and queries within minutes.  gRPC Integration: In the following example, we demonstrate how to generate a GraphQL schema from a news.proto file. This configuration allows Tailcall to parse the proto file, generate a GraphQL schema and save it to the output path provided in the configuration. JSON Config FormatYML Config Format { &quot;inputs&quot;:[ { &quot;proto&quot;:{ &quot;src&quot;:&quot;./news.proto&quot; } } ], &quot;preset&quot;:{ &quot;mergeType&quot;:1.0 }, &quot;output&quot;:{ &quot;path&quot;:&quot;./jsonplaceholder.graphql&quot;, &quot;format&quot;:&quot;graphQL&quot; }, &quot;schema&quot;:{ &quot;query&quot;:&quot;Query&quot; } } Let's understand the above configuration file. Proto: Defines the path to the proto file that the configuration interacts with. src: Specifies the path to the proto file (./news.proto in this example). Preset: We've applied only one tuning parameter for the configuration. let's understand it in short. We've set mergeType to 1.0, which basically tells config generator to merge any two GraphQL types that are exactly similar. if you're interested in understanding preset's in detail head over to preset section. Output: Specifies where and in what format the output data should be saved. path: Defines the output file path (in above example, it's ./jsonplaceholder.graphql).format: Specifies the output format as GraphQL (in above example, it's graphQL). Schema: Specifies the name of the Query operation type, which is Query in this example. Generated GraphQL Configuration schema @link(src: &quot;./news.proto&quot;, type: Protobuf) @server { query: Query } type News @tag(id: &quot;news.News&quot;) { id: Int title: String content: String author: String } type Query { news: [News] @grpc(method: &quot;news.NewsService.GetNews&quot;) }   for more insights on how gPRC works with GraphQL, you can read this GraphQL over gRPC article.  ","version":"Next","tagName":"h3"},{"title":"Hybrid Integration (REST + gRPC)​","type":1,"pageTitle":"GraphQL Configuration Generation with Tailcall","url":"/docs/graphql-configuration-generation-with-tailcall/#hybrid-integration-rest--grpc","content":" The Configuration Generator with Tailcall supports a hybrid integration of REST and gRPC. This feature allows you to leverage the strengths of both REST APIs and gRPC to create a unified GraphQL schema. By integrating both sources, you can ensure that your GraphQL schema is comprehensive and up-to-date with your existing APIs and data definitions.  Example Configuration​  Here is an example configuration that demonstrates how to set up a hybrid integration using a REST and gRPC:  JSON Config FormatYML Config Format { &quot;inputs&quot;: [ { &quot;curl&quot;: { &quot;src&quot;: &quot;https://jsonplaceholder.typicode.com/posts&quot;, &quot;fieldName&quot;: &quot;posts&quot; } }, { &quot;proto&quot;: { &quot;src&quot;: &quot;./news.proto&quot; } } ], &quot;preset&quot;: { &quot;mergeType&quot;: 1.0 }, &quot;output&quot;: { &quot;path&quot;: &quot;./output.graphql&quot;, &quot;format&quot;: &quot;graphQL&quot; }, &quot;schema&quot;: { &quot;query&quot;: &quot;Query&quot; } }   Let's understand the above configuration file.  Inputs​  curl - section where we can specify the REST endpoint. src: The URL of the REST API endpoint.fieldName: The field name to use in the GraphQL schema for the REST data. proto - section where we can specify the Proto File. src: The path to the Proto file.  Preset: We've applied only one tuning parameter for the configuration. let's understand it in short.  We've set mergeType to 1.0, which basically tells config generator to merge any two GraphQL types that are exactly similar. if you're interested in understanding preset's in detail head over to preset section.  Output: Specifies where and in what format the output data should be saved.  path: Defines the output file path (in above example, it's ./jsonplaceholder.graphql).format: Specifies the output format as GraphQL (in above example, it's graphQL).  Schema: Specifies the name of the Query operation type, which is Query in this example.  schema @link(src: &quot;./news.proto&quot;, type: Protobuf) @upstream(baseURL: &quot;https://jsonplaceholder.typicode.com&quot;) @server { query: Query } type News @tag(id: &quot;news.News&quot;) { id: Int title: String content: String author: String } type Post { body: String id: Int title: String userId: Int } type Query { posts: [Post] @http(path: &quot;/posts&quot;) news: [News] @grpc(method: &quot;news.NewsService.GetNews&quot;) }   ","version":"Next","tagName":"h3"},{"title":"Advanced Features​","type":1,"pageTitle":"GraphQL Configuration Generation with Tailcall","url":"/docs/graphql-configuration-generation-with-tailcall/#advanced-features","content":"   ","version":"Next","tagName":"h2"},{"title":"Understanding Presets​","type":1,"pageTitle":"GraphQL Configuration Generation with Tailcall","url":"/docs/graphql-configuration-generation-with-tailcall/#understanding-presets","content":" This entire section is optional and we use best defaults to generate the configuration but you can override these parameter through preset section present in configuration like shown in following. if you feel generated GraphQL configuration is good enough then feel free to skip this section.  The config generator provides a set of tuning parameters that can make the generated configurations more readable by reducing duplication and making configuration more readable. This can be configured using the preset section present in configuration.  JSONYML { &quot;preset&quot;: { &quot;mergeType&quot;: 0.8, &quot;consolidateURL&quot;: 0.8 } }   Let's understand how each of the parameter works.  mergeType:​ This setting merges types in the configuration that satisfy the threshold criteria. It takes a threshold value between 0.0 and 1.0 to determine if two types should be merged or not. The default is 1.0. MergeType also supports union types as well as interface types but merging of these types will happen only when they match exactly. Example 1: following types T1 and T2 are exactly similar, and with a threshold value of 1.0, they can be merged into a single type called M1: Merging type T1 and T2 into M1 # BEFORE type T1 { id: ID firstName: String lastName: String } type T2 { id: ID firstName: String lastName: String } # AFTER: T1 and T2 are merged into M1. type M1 { id: ID firstName: String lastName: String } Example 2: following types T1 and T2 are similar with a threshold value of 0.5, they can be merged into a single type called M1: Merging type T1 and T2 into M1 # BEFORE type T1 { id: ID firstName: String age: Int } type T2 { id: ID firstName: String lastName: String } # AFTER: T1 and T2 are merged into M1. type M1 { id: ID firstName: String lastName: String age: Int } Example 3: following types T1 and T2 are similar with a threshold value of 0.5 but we can't merge them as they have same field name but different types: Can't Merge type T1 and T2 as they've same field name but different type # BEFORE type T1 { id: ID firstName: String age: Int } type T2 { id: ID firstName: String age: Float } Example 4: following types Foo and Bar will be merged into type M1 as they match exactly and same change will reflected in union type FooBar. Merging type Foo and Bar into M1 # BEFORE type Foo { id: ID firstName: String age: Int } type Bar { id: ID firstName: String age: Int } union FooBar = Foo | Bar # After merging type M1 { id: ID firstName: String age: Int } union FooBar = M1 Example 5: following types Foo and Bar won't be merged into type M1 as they don't match exactly. Can't Merge type T1 and T2 as they've same field name but different type # BEFORE type Foo { id: ID firstName: String age: Float } type Bar { id: ID firstName: String age: Int } union FooBar = Foo | Bar consolidateURL:​ The setting identifies the most common base URL among multiple REST endpoints and uses this URL in the upstream directive. It takes a threshold value between 0.0 and 1.0 to determine the most common endpoint. The default is 0.5. For example, if the Query type has three base URLs, using the consolidateURL setting with a 0.5 threshold will pick the base URL that is used in more than 50% of the http directives, http://jsonplaceholder.typicode.com, and add it to the upstream, cleaning the base URLs from the Query type. schema @server(hostname: &quot;0.0.0.0&quot;, port: 8000) @upstream(httpCache: 42) { query: Query } type Query { post(id: Int!): Post @http( baseURL: &quot;http://jsonplaceholder.typicode.com&quot; path: &quot;/posts/{{.args.id}}&quot; ) posts: [Post] @http( baseURL: &quot;http://jsonplaceholder.typicode.com&quot; path: &quot;/posts&quot; ) user(id: Int!): User @http( baseURL: &quot;http://jsonplaceholder.typicode.com&quot; path: &quot;/users/{{.args.id}}&quot; ) users: [User] @http( baseURL: &quot;http://jsonplaceholder-1.typicode.com&quot; path: &quot;/users&quot; ) } After enabling the consolidateURL setting: schema @server(hostname: &quot;0.0.0.0&quot;, port: 8000) @upstream( baseURL: &quot;http://jsonplaceholder.typicode.com&quot; httpCache: 42 ) { query: Query } type Query { post(id: Int!): Post @http(path: &quot;/posts/{{.args.id}}&quot;) posts: [Post] @http(path: &quot;/posts&quot;) user(id: Int!): User @http(path: &quot;/users/{{.args.id}}&quot;) users: [User] @http( baseURL: &quot;http://jsonplaceholder-1.typicode.com&quot; path: &quot;/users&quot; ) }   ","version":"Next","tagName":"h3"},{"title":"Recommended Configuration Parameters​","type":1,"pageTitle":"GraphQL Configuration Generation with Tailcall","url":"/docs/graphql-configuration-generation-with-tailcall/#recommended-configuration-parameters","content":" When setting up your configuration file for GraphQL generation with Tailcall, consider these key parameters to optimize and customize your setup:  Merge Type: Controls the merging of similar GraphQL types to reduce duplication. Adjust the threshold (0.0 to 1.0) based on how strictly you want types to match for merging. the closer the number to 1.0, you get the best type inference in graphQL playground. Recommended threshold is anything above 0.9. JSONYML { &quot;preset&quot;: { &quot;mergeType&quot;: 0.9 } } Consolidate URL: Identifies the most common base URL among multiple REST endpoints and uses it in the @upstream directive. Set a threshold (0.0 to 1.0) to determine when to consolidate URLs. Recommended threshold is anything above 0.5. JSONYML { &quot;preset&quot;: { &quot;consolidateURL&quot;: 0.5 } } Headers: Never store sensitive information like access tokens directly in configuration files. Leverage templates to securely reference secrets from environment variables. JSONYML { &quot;headers&quot;: { &quot;secretToken&quot;: &quot;{{.env.TOKEN}}&quot; } }   ","version":"Next","tagName":"h2"},{"title":"FAQ's​","type":1,"pageTitle":"GraphQL Configuration Generation with Tailcall","url":"/docs/graphql-configuration-generation-with-tailcall/#faqs","content":" Q. Can I use environment variables in my configuration?  Answer: Yes, you can use environment variables to securely reference sensitive information like access tokens. Here is an example:  JSONYML { &quot;curl&quot;: { &quot;src&quot;: &quot;https://jsonplaceholder.typicode.com/posts/1&quot;, &quot;fieldName&quot;: &quot;post&quot;, &quot;headers&quot;: { &quot;secretToken&quot;: &quot;{{.env.TOKEN}}&quot; } } }   Q. How do I merge similar types in the configuration?  Answer: Adjust the mergeType parameter in the preset section to control the merging of similar types. A threshold value between 0.0 and 1.0 determines if two types should be merged or not. if you to understand this in detail then please head over to preset section. Here is an example:  JSONYML { &quot;preset&quot;: { &quot;mergeType&quot;: 0.9 } }   Q. What if I have multiple REST endpoints with different base URLs?  Answer: Use the consolidateURL parameter to identify the most common base URL among multiple REST endpoints and it will automatically select the most common base url and add it to the @upstream directive. Here is an example:  JSONYML { &quot;preset&quot;: { &quot;consolidateURL&quot;: 0.5 } }   Q. Can I specify multiple input sources in a single configuration?  Answer: Yes, you can specify multiple input sources, such as different REST endpoints or Proto files, in a single configuration. Here is an example:  JSONYML { &quot;inputs&quot;: [ { &quot;curl&quot;: { &quot;src&quot;: &quot;https://jsonplaceholder.typicode.com/posts&quot;, &quot;fieldName&quot;: &quot;posts&quot; } }, { &quot;proto&quot;: { &quot;src&quot;: &quot;./news.proto&quot; } } ], &quot;schema&quot;: { &quot;query&quot;: &quot;Query&quot; } }  ","version":"Next","tagName":"h2"},{"title":"GraphQL Playground","type":0,"sectionRef":"#","url":"/docs/graphql-playground-guide/","content":"","keywords":"","version":"Next"},{"title":"Performance and Security​","type":1,"pageTitle":"GraphQL Playground","url":"/docs/graphql-playground-guide/#performance-and-security","content":" Performance Impact: The showcase feature prioritizes flexibility and ease of testing over speed, leading to slower response times due to the overhead of dynamically applied configurations.Security Risk: There's a potential security risk as it may allow unauthorized access to files and environment variables.  important Due to these concerns, this mode is not recommended for production environments. ","version":"Next","tagName":"h2"},{"title":"Solving GraphQL N+1 Problem with Tailcall","type":0,"sectionRef":"#","url":"/docs/graphql-n-plus-one-problem-solved-tailcall/","content":"","keywords":"","version":"Next"},{"title":"What is the N+1 Problem?​","type":1,"pageTitle":"Solving GraphQL N+1 Problem with Tailcall","url":"/docs/graphql-n-plus-one-problem-solved-tailcall/#what-is-the-n1-problem","content":" In simple terms, the N+1 problem occurs when your server fetches data in an inefficient manner that is - instead of making a single request to retrieve necessary data set, it makes multiple, separate requests.  For instance, if you're fetching a list of posts and their authors, an inefficient server might first fetch the posts (1 request). Then, for each post, it makes request to fetch their author (N requests). This results in N+1 total requests.  This approach can quickly become problematic as your nested data grows. It leads to a large number of unnecessary requests, slowing down your server and wasting resources.  ","version":"Next","tagName":"h2"},{"title":"Why is N+1 a Problem for GraphQL?​","type":1,"pageTitle":"Solving GraphQL N+1 Problem with Tailcall","url":"/docs/graphql-n-plus-one-problem-solved-tailcall/#why-is-n1-a-problem-for-graphql","content":" The N+1 problem in GraphQL queries can lead to inefficiencies in data fetching due to poorly optimized resolvers. Let's delve into this issue with a simplified explanation:  Resolver Efficiency: In GraphQL, each nested field within a single query might require its own request. For instance, if you're dealing with a list of N items, this results in N additional requests, culminating in a total of N+1 requests. Complexity in Detection: Identifying and resolving the N+1 problem can be challenging for developers, especially by merely examining GraphQL queries, schema or the server side resolver logic. Optimization Necessity: Addressing this issue often requires employing advanced techniques such as batching request or utilizing open source tools like DataLoader for batch loading to minimize the number of requests triggered by resolvers on your GraphQL server. While effective, these solutions can introduce additional complexity into the development process.    ","version":"Next","tagName":"h2"},{"title":"N+1 in REST APIs​","type":1,"pageTitle":"Solving GraphQL N+1 Problem with Tailcall","url":"/docs/graphql-n-plus-one-problem-solved-tailcall/#n1-in-rest-apis","content":" Imagine we need to fetch data from the jsonplaceholder.typicode.com, requiring both posts and their authors' details.  First, we request all posts:  ❯ curl https://jsonplaceholder.typicode.com/posts [ { &quot;userId&quot;: 1, &quot;id&quot;: 1, &quot;title&quot;: &quot;Creating Solutions for Challenges&quot;, &quot;body&quot;: &quot;We anticipate and understand challenges, creating solutions while considering exceptions and criticisms.&quot; }, { &quot;userId&quot;: 1, &quot;id&quot;: 2, &quot;title&quot;: &quot;Understanding Identity&quot;, &quot;body&quot;: &quot;Life's essence, measured through time, presents pains and joys. We find solace in the mundane, seeking meaning beyond the visible.&quot; } ]   This command retrieves posts from the API, with each post containing a userId field indicating its author.  Next, we fetch details for each post's author, such as:  ❯ curl https://jsonplaceholder.typicode.com/users/1 { &quot;id&quot;: 1, &quot;name&quot;: &quot;Leanne Graham&quot;, &quot;username&quot;: &quot;Bret&quot;, &quot;email&quot;: &quot;Sincere@april.biz&quot;, &quot;address&quot;: { &quot;street&quot;: &quot;Kulas Light&quot;, &quot;suite&quot;: &quot;Apt. 556&quot;, &quot;city&quot;: &quot;Gwenborough&quot;, &quot;zipcode&quot;: &quot;92998-3874&quot;, &quot;geo&quot;: { &quot;lat&quot;: &quot;-37.3159&quot;, &quot;lng&quot;: &quot;81.1496&quot; } }, &quot;phone&quot;: &quot;1-770-736-8031 x56442&quot;, &quot;website&quot;: &quot;hildegard.org&quot;, &quot;company&quot;: { &quot;name&quot;: &quot;Romaguera-Crona&quot;, &quot;catchPhrase&quot;: &quot;Multi-layered client-server neural-net&quot;, &quot;bs&quot;: &quot;harness real-time e-markets&quot; } }   For 100 posts, this results in 100 additional requests for author details, totaling 101 requests. This is the infamous N+1 problem:  1 request for /posts100 or N requests for /users/:id for each user  info This issue can escalate in real-world scenarios, leading to straining resources, increasing server costs, slowing response times, and potentially causing server downtime even at a moderate scale.  Hope this gives you a high-level overview of what the N+1 problem is in the API context. It's a common problem not specific to just APIs or GraphQL, you will see this problem very commonly in database queries also. However addressing the N+1 problem during application design and development is crucial and we will see how this is tackled in Tailcall.  ","version":"Next","tagName":"h2"},{"title":"N+1 in GraphQL using Tailcall​","type":1,"pageTitle":"Solving GraphQL N+1 Problem with Tailcall","url":"/docs/graphql-n-plus-one-problem-solved-tailcall/#n1-in-graphql-using-tailcall","content":" Before diving into solutions, let's observe the N+1 problem in the following GraphQL configuration:  tip If you are new here you might want to check out our Getting Started guide.  schema @upstream( baseURL: &quot;http://jsonplaceholder.typicode.com&quot; ) { query: Query } type Query { posts: [Post] @http(path: &quot;/posts&quot;) } type Post { id: Int! userId: Int! title: String! body: String! user: User @http(path: &quot;/users/{{.value.userId}}&quot;) } type User { id: Int! name: String! username: String! email: String! }   This configuration sets up a GraphQL schema for a server utilizing jsonplaceholder.typicode.com as its data source. It allows direct querying of posts and, for each post, retrieves the associated author information. Similar to our curl requests above, when we query for posts and their authors using graphql query on client side given below we end up issuing multiple user calls upstream:  query { posts { id title user { id name email } } }   Let's examine the CLI output for this configuration with Tailcall's start command:  ❯ tailcall start ./examples/jsonplaceholder.graphql INFO File read: ./examples/jsonplaceholder.graphql ... ok INFO N+1 detected: 1 INFO 🚀 Tailcall launched at [0.0.0.0:8000] over HTTP/1.1 INFO 🌍 Playground: https://tailcall.run/playground/?u=http://127.0.0.1:8000/graphql INFO GET http://jsonplaceholder.typicode.com/posts HTTP/1.1 INFO GET http://jsonplaceholder.typicode.com/users/8 HTTP/1.1 ... INFO GET http://jsonplaceholder.typicode.com/users/8 HTTP/1.1 INFO GET http://jsonplaceholder.typicode.com/users/10 HTTP/1.1   Tailcall logs a sequence of requests made to fetch posts and then their individual authors, highlighting the N+1 problem in real-time. Since there are 100 posts, so 100 requests are made to fetch the authors.  ","version":"Next","tagName":"h2"},{"title":"Deduplication using DataLoaders​","type":1,"pageTitle":"Solving GraphQL N+1 Problem with Tailcall","url":"/docs/graphql-n-plus-one-problem-solved-tailcall/#deduplication-using-dataloaders","content":" If you run the query, at first you will observe a lot of duplicate requests are being made for getting the same author details.    This happens because of the 100 posts, a lot them are authored by the same user and by default Tailcall will make a request for every user when requested. You can fix this by setting dedupe to true in server.  schema @server( dedupe: true port: 8000) @upstream(baseURL: &quot;http://jsonplaceholder.typicode.com&quot;) { query: Query } type Query { # ... } type Post { # ... } type User { # ... }   When you enable dedupe, for each downstream request, Tailcall will automatically using a dataloader deduplicate all upstream requests and instead of making 100 it will only make 10 requests for unique users:  ❯ tailcall start ./examples/jsonplaceholder.graphql INFO File read: ./examples/jsonplaceholder.graphql ... ok INFO N+1 detected: 1 INFO 🚀 Tailcall launched at [0.0.0.0:8000] over HTTP/1.1 INFO 🌍 Playground: https://tailcall.run/playground/?u=http://127.0.0.1:8000/graphql INFO GET http://jsonplaceholder.typicode.com/posts HTTP/1.1 INFO GET http://jsonplaceholder.typicode.com/users/1 HTTP/1.1 INFO GET http://jsonplaceholder.typicode.com/users/2 HTTP/1.1 INFO GET http://jsonplaceholder.typicode.com/users/3 HTTP/1.1 INFO GET http://jsonplaceholder.typicode.com/users/4 HTTP/1.1 INFO GET http://jsonplaceholder.typicode.com/users/5 HTTP/1.1 INFO GET http://jsonplaceholder.typicode.com/users/6 HTTP/1.1 INFO GET http://jsonplaceholder.typicode.com/users/7 HTTP/1.1 INFO GET http://jsonplaceholder.typicode.com/users/8 HTTP/1.1 INFO GET http://jsonplaceholder.typicode.com/users/9 HTTP/1.1 INFO GET http://jsonplaceholder.typicode.com/users/10 HTTP/1.1   This is a massive 10x improvement over the previous implementation. However, it might not always be the case. For eg: If all the posts are created by different users you might still end up making 100 requests upstream.    tip Dedupe has a slight performance overhead so if your use case doesn't have any N+1 issues, it might be worth keeping this setting disabled.  ","version":"Next","tagName":"h2"},{"title":"Detect N+1 using Tailcall​","type":1,"pageTitle":"Solving GraphQL N+1 Problem with Tailcall","url":"/docs/graphql-n-plus-one-problem-solved-tailcall/#detect-n1-using-tailcall","content":" Before we get into the actual solution, if you observe closely the above logs Tailcall identified that there was one N+1 issue, even before the requests were made:  ❯ tailcall start ./examples/jsonplaceholder.graphql INFO File read: ./examples/jsonplaceholder.graphql ... ok INFO N+1 detected: 1 INFO 🚀 Tailcall launched at [0.0.0.0:8000] over HTTP/1.1 INFO 🌍 Playground: https://tailcall.run/playground/?u=http://127.0.0.1:8000/graphql INFO GET http://jsonplaceholder.typicode.com/posts HTTP/1.1 INFO GET http://jsonplaceholder.typicode.com/users/8 HTTP/1.1 ... INFO GET http://jsonplaceholder.typicode.com/users/10 HTTP/1.1   To get a deeper understanding of what this N+1 issue is, we can use the --n-plus-one-queries parameter with the check command:  ❯ tailcall check ./jsonplaceholder.graphql --n-plus-one-queries INFO File read: ./examples/jsonplaceholder.graphql ... ok INFO Config ./examples/jsonplaceholder.graphql ... ok INFO N+1 detected: 1 query { posts { user } }   Incredible, isn't it? Tailcall has discovered that querying for posts followed by their users would result in N+1 upstream calls. This represents a significant productivity gain, as you can now identify all such N+1 issues upfront without resorting to complex profiling, tracing, or other runtime techniques. The check command also identifies the minimal query that could lead to these N+1 problems by performing semantic analysis of your configuration. With these powerful tools handy you can go about making extremely efficient GraphQL backends as we will see next:  ","version":"Next","tagName":"h2"},{"title":"Using Batch APIs​","type":1,"pageTitle":"Solving GraphQL N+1 Problem with Tailcall","url":"/docs/graphql-n-plus-one-problem-solved-tailcall/#using-batch-apis","content":"   An effective technique to mitigate the N+1 problem is deduplicating similar requests, significantly reducing the number of server calls. We achieved it previously using the dedupe setting. With Tailcall we can go one step further by giving hints about &quot;batch APIs&quot;.  Batch APIs: Are special APIs that allow us to query multiple things at once. In our case we can pass multiple user Ids as query params, to the /users API to resolve many users at once:  tip Try to hit /users?id=1&amp;id=2  TailCall provides the capability to leverage Batch APIs. To utilize this feature, edit the @http directive on Post.user field in your GraphQL schema as follows:  type Post { id: Int! userId: Int! title: String! body: String! user: User @http( path: &quot;/users&quot; query: [{key: &quot;id&quot;, value: &quot;{{.value.userId}}&quot;}] batchKey: [&quot;id&quot;] ) }   The described changes introduce two significant tweaks to the @http directive:  Addition of a query parameter: type Post { # ... user: User @http( path: &quot;/users&quot; query: [{key: &quot;id&quot;, value: &quot;{{.value.userId}}&quot;}] batchKey: [&quot;id&quot;] ) } This configuration generates a URL with the userId from the Post in the query params. For a batch of users, the CLI compiles a single URL, such as /users?id=1&amp;id=2&amp;id=3...id=10, consolidating the 10 requests into one. Addition of a batchKey: type Post { # ... user: User @http( path: &quot;/users&quot; query: [{key: &quot;id&quot;, value: &quot;{{.value.userId}}&quot;}] batchKey: [&quot;id&quot;] ) } This parameter instructs the system to use the user's id, in the User type, as the unique identifier. This helps in differentiating between users received from the batch API.  Let's see what the server logs when you now start Tailcall with the updated configuration:  schema @server(port: 8000) @upstream( baseURL: &quot;http://jsonplaceholder.typicode.com&quot; ) { query: Query } type Query { posts: [Post] @http(path: &quot;/posts&quot;) } type Post { id: Int! userId: Int! title: String! body: String! user: User @http( path: &quot;/users&quot; query: [{key: &quot;id&quot;, value: &quot;{{.value.userId}}&quot;}] batchKey: [&quot;id&quot;] ) } type User { id: Int! name: String! username: String! email: String! }   Let's start the server as usual and focus on the detected N+1 issues:  ❯ tailcall start ./examples/jsonplaceholder.graphql INFO File read: ./examples/jsonplaceholder.graphql ... ok INFO N+1 detected: 0 INFO 🚀 Tailcall launched at [0.0.0.0:8000] over HTTP/1.1 INFO 🌍 Playground: https://tailcall.run/playground/?u=http://127.0.0.1:8000/graphql INFO GET http://jsonplaceholder.typicode.com/posts HTTP/1.1 INFO GET http://jsonplaceholder.typicode.com/users?id=1&amp;id=10&amp;id=2&amp;id=3&amp;id=4&amp;id=5&amp;id=6&amp;id=7&amp;id=8&amp;id=9 HTTP/1.1   As you can see there are ZERO N+1 detected this time! It basically means that irrespective of how large the list of posts is there is a finite number of requests that will be issued in this case that's always going to be TWO. And this is how Tailcall users tackle the N+1 problem in GraphQL.  ","version":"Next","tagName":"h2"},{"title":"Conclusion​","type":1,"pageTitle":"Solving GraphQL N+1 Problem with Tailcall","url":"/docs/graphql-n-plus-one-problem-solved-tailcall/#conclusion","content":" To summarize, we learnt that N+1 is a general problem and not specific to GraphQL. It's a hard problem to identify, and developers often resort to runtime analysis to find such scenarios. N+1 can really strain your infrastructure, leading to serious downtime in certain cases.    We also learnt that in Tailcall, the CLI can introspect your configuration and identify all the potential N+1 issues upfront. Using dedupe, you can improve the N+1 problem significantly, however, it's not a complete solution. To completely eliminate the N+1 problem, you can configure Tailcall to leverage Batch APIs. Hopefully, this guide underscores the effectiveness of Tailcall in addressing the N+1 problem.   ","version":"Next","tagName":"h2"},{"title":"Exposing GraphQL as REST APIs","type":0,"sectionRef":"#","url":"/docs/graphql-rest-integration/","content":"","keywords":"","version":"Next"},{"title":"How it works​","type":1,"pageTitle":"Exposing GraphQL as REST APIs","url":"/docs/graphql-rest-integration/#how-it-works","content":" This guide show you how to expose REST endpoints for your GraphQL operations by using the @rest directive like follows:  There are three main steps to this process:  Define your Tailcall GraphQL configuration file.Define an operation using @rest directive in a separate file.Link the operation to the main config file.  ","version":"Next","tagName":"h2"},{"title":"Example​","type":1,"pageTitle":"Exposing GraphQL as REST APIs","url":"/docs/graphql-rest-integration/#example","content":" ","version":"Next","tagName":"h2"},{"title":"Step 1: Define your GraphQL configuration​","type":1,"pageTitle":"Exposing GraphQL as REST APIs","url":"/docs/graphql-rest-integration/#step-1-define-your-graphql-configuration","content":" schema @upstream( baseURL: &quot;https://jsonplaceholder.typicode.com&quot; ) { query: Query } type Query { post(id: Int!): Post @http(path: &quot;/posts/{{.args.id}}&quot;) } type Post { userId: Int! id: Int title: String body: String user: User @http(path: &quot;/users/{{.value.userId}}&quot;) } type User { id: Int! name: String! username: String! email: String! phone: String website: String }   for more information on how to define your Tailcall GraphQL configuration file, please refer to the Tailcall GraphQL Configuration.  ","version":"Next","tagName":"h3"},{"title":"Step 2: Define an operation using @rest directive​","type":1,"pageTitle":"Exposing GraphQL as REST APIs","url":"/docs/graphql-rest-integration/#step-2-define-an-operation-using-rest-directive","content":" We will define an operation and use the @rest directive to define a REST endpoint for the operation. We will create a new file and add the following content to it. Save the file with the filename: user-operation.graphql. You can name the file anything you want, but make sure to link it to the main config file in the next step.  query ($id: Int!) @rest(method: GET, path: &quot;/post/$id&quot;) { post(id: $id) { id title body user { id name } } }   to know more about the @rest directive, please refer to the Tailcall GraphQL Directives.  ","version":"Next","tagName":"h3"},{"title":"Step 3: Link the operation to the main config file​","type":1,"pageTitle":"Exposing GraphQL as REST APIs","url":"/docs/graphql-rest-integration/#step-3-link-the-operation-to-the-main-config-file","content":" checkout the @link directive in the config snippet below to link the operation file. This step is crucial to make the REST endpoint available.  schema @upstream(baseURL: &quot;https://jsonplaceholder.typicode.com&quot;) @link(type: Operation, src: &quot;user-operation.graphql&quot;) { query: Query }   To know more about the @link directive, please refer to the Tailcall GraphQL Directives.  Response​    In summary, by utilizing the @rest directive, we've seamlessly exposed RESTful services over Tailcall's GraphQL, enhancing the traditional posts API to offer richer functionality without additional code. This approach combines the simplicity and ubiquity of REST with the modularity and flexibility of GraphQL, allowing for easy consumption from any HTTP client while leveraging GraphQL's powerful data querying capabilities. ","version":"Next","tagName":"h3"},{"title":"GraphQL Resolver Context","type":0,"sectionRef":"#","url":"/docs/graphql-resolver-context-tailcall/","content":"","keywords":"","version":"Next"},{"title":"Schema Definition​","type":1,"pageTitle":"GraphQL Resolver Context","url":"/docs/graphql-resolver-context-tailcall/#schema-definition","content":" type Context = { args: Map&lt;string, JSON&gt; value: JSON env: Map&lt;string, string&gt; vars: Map&lt;string, string&gt; headers: Map&lt;string, string&gt; }   Context operates by storing values as key-value pairs, which can be accessed through mustache template syntax.  ","version":"Next","tagName":"h2"},{"title":"args​","type":1,"pageTitle":"GraphQL Resolver Context","url":"/docs/graphql-resolver-context-tailcall/#args","content":" This property facilitates access to query arguments. Consider the example:  type Query { user(id: ID!): User @http(path: &quot;/users/{{.args.id}}&quot;) }   Here, args.id is utilized to retrieve the id argument provided to the user query.  ","version":"Next","tagName":"h3"},{"title":"value​","type":1,"pageTitle":"GraphQL Resolver Context","url":"/docs/graphql-resolver-context-tailcall/#value","content":" This enables access to the fields of the specified type.  type Post { id: ID! title: String! body: String! comments: [Comment] @http(path: &quot;/posts/{{.value.id}}/comments&quot;) }   In this case, value.id accesses the id field of the Post type.  ","version":"Next","tagName":"h3"},{"title":"env​","type":1,"pageTitle":"GraphQL Resolver Context","url":"/docs/graphql-resolver-context-tailcall/#env","content":" Environment variables, set at server startup, allow directives to dynamically adapt behavior based on external configurations without altering the server configuration itself.  Example:  type Query { users: [User]! @http(baseUrl: &quot;{{.env.API_ENDPOINT}}&quot;, path: &quot;/users&quot;) }   env.API_ENDPOINT references an environment variable named API_ENDPOINT, which specifies the base URL for HTTP requests.  ","version":"Next","tagName":"h3"},{"title":"vars​","type":1,"pageTitle":"GraphQL Resolver Context","url":"/docs/graphql-resolver-context-tailcall/#vars","content":" vars offers a mechanism for defining reusable variables within the configuration. Unlike env, these are embedded and can be universally applied across configurations.  schema @server( vars: {key: &quot;apiKey&quot;, value: &quot;{{.env.AUTH_TOKEN}}&quot;} ) { query: Query } type Query { user(id: ID!): [User] @http( url: &quot;/users&quot; headers: [ { key: &quot;Authorization&quot; value: &quot;Bearer {{.vars.apiKey}}&quot; } ] ) }   Here, the variable apiKey is set using an environment variable and subsequently utilized in the Authorization header for HTTP requests.  ","version":"Next","tagName":"h3"},{"title":"headers​","type":1,"pageTitle":"GraphQL Resolver Context","url":"/docs/graphql-resolver-context-tailcall/#headers","content":" Headers originate from the request made to the GraphQL server.  type Query { commentsForUser: [Comment] @http(path: &quot;/users/{{.headers.x-user-id}}/comments&quot;) }   In this example, headers.x-user-id extracts the value of the x-user-id header present in the request, dynamically constructing the request path. ","version":"Next","tagName":"h3"},{"title":"Simplifying GraphQL Scalars","type":0,"sectionRef":"#","url":"/docs/graphql-scalars-guide/","content":"","keywords":"","version":"Next"},{"title":"Default Scalars​","type":1,"pageTitle":"Simplifying GraphQL Scalars","url":"/docs/graphql-scalars-guide/#default-scalars","content":" Here is a list of default scalars that are built into the GraphQL Spec:  Scalar\tDescription\tSpecificationInt\tA type representing non-fractional signed whole numbers. Values can range up to (2^31 - 1).\tGraphQL Specification for Int Float\tA type for signed double-precision floating-point numbers.\tGraphQL Specification for Float String\tA sequence of UTF-8 characters, representing textual data.\tGraphQL Specification for String Boolean\tA boolean type that represents true or false.\tGraphQL Specification for Boolean ID\tA unique identifier, typically used to refetch an object or as a cache key.\tGraphQL Specification for ID  ","version":"Next","tagName":"h2"},{"title":"GraphQL Scalars​","type":1,"pageTitle":"Simplifying GraphQL Scalars","url":"/docs/graphql-scalars-guide/#graphql-scalars","content":" These are the current set of custom scalars supported by Tailcall:  Scalar\tDescription\tSpecificationEmail\tA string that conforms to the email format as defined in the HTML specification, utilizing the Unicode character set.\tHTML Specification for Valid Email Addresses PhoneNumber\tA string format adhering to the E.164 international standard, which outlines the numbering plan for the worldwide public switched telephone network (PSTN) and certain data networks.\tE.164 International Numbering Plan Date\tA string that represents dates and times in the Internet protocols, following the ISO 8601 standard via the Gregorian calendar.\tRFC 3339 Date and Time Internet Formats Url\tA standardized format for Uniform Resource Identifiers (URI) that includes both the generic URI syntax and guidelines for resolving URI references, which may be in relative form.\tRFC 3986 Uniform Resource Identifier JSON\tA lightweight data interchange format based on the ECMAScript Programming Language Standard, designed for human-readable data representation.\tRFC 7159 The JavaScript Object Notation (JSON) Data Interchange Format Empty\tA type that represents no value or is used as a placeholder in contexts where no other data is expected or returned. It's equivalent to unit or void in other programming languages.\t  If none of the scalars make sense for your use case, consider opening an issue on the Tailcall github repository.  ","version":"Next","tagName":"h2"},{"title":"Custom Scalars​","type":1,"pageTitle":"Simplifying GraphQL Scalars","url":"/docs/graphql-scalars-guide/#custom-scalars","content":" Apart from the pre-defined list of scalars, you can define your own custom scalars in your GraphQL schema like in the example below.  scalar AnyScalar schema @server(port: 8000, hostname: &quot;localhost&quot;) { query: Query } type Query { any(value: AnyScalar!): AnyScalar! @expr(body: &quot;{{.args.value}}&quot;) }   important Be aware that custom scalars don't have any validation and can be mapped to any data structure when using it.  ","version":"Next","tagName":"h2"},{"title":"Example Usage​","type":1,"pageTitle":"Simplifying GraphQL Scalars","url":"/docs/graphql-scalars-guide/#example-usage","content":" Let's try using these custom scalars in our GraphQL schema.  schema @server(port: 8000, hostname: &quot;localhost&quot;) { query: Query } type Query { email(value: Email!): Email! @expr(body: &quot;{{.args.value}}&quot;) }   ","version":"Next","tagName":"h2"},{"title":"Valid Query Example​","type":1,"pageTitle":"Simplifying GraphQL Scalars","url":"/docs/graphql-scalars-guide/#valid-query-example","content":" Here is an example of a valid query that passes the custom scalar validations:  ","version":"Next","tagName":"h3"},{"title":"Invalid Query Example​","type":1,"pageTitle":"Simplifying GraphQL Scalars","url":"/docs/graphql-scalars-guide/#invalid-query-example","content":" And here is an example of an invalid query that fails the custom scalar validations as expected: ","version":"Next","tagName":"h3"},{"title":"GraphQL Server Watch Mode","type":0,"sectionRef":"#","url":"/docs/graphql-watch-mode-tailcall/","content":"","keywords":"","version":"Next"},{"title":"Use case​","type":1,"pageTitle":"GraphQL Server Watch Mode","url":"/docs/graphql-watch-mode-tailcall/#use-case","content":" Running a server in watch mode offers a lot of key benefits:  Real-time Feedback: Watch mode ensures that your server remains up-to-date with your code changes, instantly reflecting those changes and providing you with real-time feedback during development.Efficiency: Manually restarting the server each time you change code can be tedious and time-consuming. Watch mode automates this process, enhancing development efficiency.Debugging: It enables you to identify and resolve issues as they occur, reducing debugging time. With your server automatically restarting upon code changes, you detect errors earlier.  ","version":"Next","tagName":"h2"},{"title":"Using entr​","type":1,"pageTitle":"GraphQL Server Watch Mode","url":"/docs/graphql-watch-mode-tailcall/#using-entr","content":" It's a powerful file-watching utility that makes running a server in watch mode a breeze. Let's go through the steps for the installation process for different operating system :  ","version":"Next","tagName":"h2"},{"title":"Installation​","type":1,"pageTitle":"GraphQL Server Watch Mode","url":"/docs/graphql-watch-mode-tailcall/#installation","content":" Homebrew​  Open the Terminal, which you can find in the &quot;Utilities&quot; folder within the &quot;Applications&quot; folder. Install Homebrew if you haven't already. Run the following command in your Terminal: /bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)&quot; After installing Homebrew, proceed to install entr by executing the following command: brew install entr To verify the installation, run: entr --version   Upon successful installation, it will display the latest version of entr.  Windows Subsystem​  Install Windows Subsystem for Linux (WSL) on your Windows machine by following Microsoft's official documentation. After setting up WSL, open the Linux terminal by running: wsl -d &lt;DistributionName&gt; Replace &lt;DistributionName&gt; with the name of the Linux distribution that you have installed. Install entr within the Linux terminal using the package manager of your chosen Linux distribution. For example, on Ubuntu, you can use: sudo apt update sudo apt install entr Verify the installation by running: entr --version   A successful installation will display the latest version of entr.  apt-get​  On Linux, you can install entr using your distribution's package manager. For example, on Ubuntu, use: sudo apt update sudo apt install entr To verify the installation, run: entr --version   If you install it, it will show the latest version of the entr  ","version":"Next","tagName":"h3"},{"title":"Watch Mode​","type":1,"pageTitle":"GraphQL Server Watch Mode","url":"/docs/graphql-watch-mode-tailcall/#watch-mode","content":" To run your server in watch mode with entr, use the ls command to list the files you want to track. The general syntax is as follows:  ls *.graphql | entr -r tailcall start ./jsonplaceholder.graphql   This command uses entr to continuously track the jsonplaceholder.graphql file and when it changes, It runs the tailcall start command with the file as an argument  Detailing the above command as follows:  ls *.graphql : This part of the code lists the file or files you want to track for changes. In this case, it lists the file named &quot;jsonplaceholder.graphql&quot; within the &quot;examples&quot; directory. | : The pipe symbol ('|') takes the output of the preceding command (the file listing) and feeds it as input to the following command (entr). entr -r tc start ./jsonplaceholder.graphql : Whenever the file &quot;jsonplaceholder.graphql&quot; changes, this command executes.  entr is a command-line tool for running arbitrary commands whenever files change. It tracks the files specified in the previous command (ls ./jsonplaceholder.graphql) r : This flag instructs entr to persist in running the command through errors, ensuring continuous operation. tc start ./jsonplaceholder.graphql : This command runs upon detecting changes, executing tc start with the file path./jsonplaceholder.graphql as an argument  ","version":"Next","tagName":"h3"},{"title":"Some Best Practices​","type":1,"pageTitle":"GraphQL Server Watch Mode","url":"/docs/graphql-watch-mode-tailcall/#some-best-practices","content":" To make the most of running a server in watch mode with entr, consider the following best practices:  Selective File Watching: Be selective about which files you track with entr. Watching unnecessary files can lead to increased CPU and memory usage. Focus on the essential files related to your project. Organize Your Project: Maintain a well-organized project structure to make it easier to identify which files need tracking. Clear Output: Clear the terminal output before running entr to have a clean workspace. Version Control: Ensure that your project is under version control (e.g., Git) to track changes and revert if necessary. Update entr: Ensure entr is always updated to the latest version for bug fixes and enhancements.  By following these best practices and using entr effectively, you can greatly improve your development workflow. Experiment with entr, adapt it to your project's specific requirements, and enjoy a smoother and more efficient development process. Happy coding! ","version":"Next","tagName":"h2"},{"title":"GraphQL Telemetry","type":0,"sectionRef":"#","url":"/docs/graphql-telemetry-guide/","content":"","keywords":"","version":"Next"},{"title":"What is Observability​","type":1,"pageTitle":"GraphQL Telemetry","url":"/docs/graphql-telemetry-guide/#what-is-observability","content":" Observability is essential for maintaining the health and performance of your applications. It provides insights into your software's operation in real-time by analyzing telemetry data — logs, metrics, and traces. This data helps in troubleshooting, optimizing, and ensuring your application works as expected.  Logs offer a record of events that have happened within your application, useful for understanding actions taken or errors that have occurred.Metrics are numerical data that measure different aspects of your system's performance, such as request rates or memory usage.Traces show the journey of requests through your system, highlighting how different parts of your application interact and perform.  Tailcall provides observability support by integrating OpenTelemetry specification into it with help of provided SDKs and data formats.  OpenTelemetry is a toolkit for collecting telemetry data in a consistent manner across different languages and platforms. It frees you from being locked into a single observability platform, allowing you to send your data to different tools for analysis, such as New Relic or Honeycomb.  ","version":"Next","tagName":"h2"},{"title":"Comparison with Apollo Studio​","type":1,"pageTitle":"GraphQL Telemetry","url":"/docs/graphql-telemetry-guide/#comparison-with-apollo-studio","content":" While Apollo studio telemetry also provides analytics tools for your schema but when choosing between it and OpenTelemetry integration consider next points:  OpenTelemetry is more generalized observability framework that could be used for cross-service analytics while Apollo Studio can provide insights related purely to graphQLOpenTelemetry is vendor-agnostic and therefore you could actually use different observability platforms depending on your needs and don't rely on single tool like Apollo StudioOpenTelemetry integration in Tailcall can provide more analytical data that is out of scope of graphQL analytics provided by Apollo Studio  ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"GraphQL Telemetry","url":"/docs/graphql-telemetry-guide/#prerequisites","content":" Consider we have the following GraphQL configuration that connects with jsonplaceholder.com to fetch the data about user and posts  schema @server(port: 8000, hostname: &quot;0.0.0.0&quot;) @upstream( baseURL: &quot;http://jsonplaceholder.typicode.com&quot; ) { query: Query } type Query { posts: [Post] @http(path: &quot;/posts&quot;) @cache(maxAge: 3000) user(id: Int!): User @http(path: &quot;/users/{{.args.id}}&quot;) } type User { id: Int! name: String! username: String! email: String! phone: String website: String } type Post { id: Int! userId: Int! title: String! body: String! user: User @http(path: &quot;/users/{{.value.userId}}&quot;) }   We will update that config with telemetry integration in following sections.  ","version":"Next","tagName":"h2"},{"title":"GraphQL Configuration for Telemetry​","type":1,"pageTitle":"GraphQL Telemetry","url":"/docs/graphql-telemetry-guide/#graphql-configuration-for-telemetry","content":" By default, telemetry data is not generated by Tailcall since it requires some setup to know where to send this data and also that affects performance of server that could be undesirable in some cases.  Telemetry configuration is provided by @telemetry directive to setup how and where the telemetry data is send.  To enable it we can update our config with something like config below:  schema @telemetry( export: { otlp: {url: &quot;http://your-otlp-compatible-backend.com&quot;} } ) { query: Query }   Here, export specifies the format of generated data and endpoint to which to send that data. Continue reading to know more about different options for it.  ","version":"Next","tagName":"h2"},{"title":"Export to OTLP​","type":1,"pageTitle":"GraphQL Telemetry","url":"/docs/graphql-telemetry-guide/#export-to-otlp","content":" OTLP is a vendor agnostic protocol that is supported by growing number of observability backends.  OpenTelemetry Collector​  OpenTelemetry Collector is a vendor-agnostic way to receive, process and export telemetry data in OTLP format.  Although, tailcall can send the data directly to the backends that supports OTLP format using Otel Collector could be valuable choice since it's more robust solution well-suited for a high-scale, more flexible settings and ability to export in different formats other than OTLP.  In summary, if you're gonna to use OTLP compatible platform or prometheus and your load is not that massive you could send the data directly to platforms. From the other side, if you need to export to different formats (like Jaeger or Datadog) or your application involves high load consider using Otel Collector as an export target.  ","version":"Next","tagName":"h3"},{"title":"Export to prometheus​","type":1,"pageTitle":"GraphQL Telemetry","url":"/docs/graphql-telemetry-guide/#export-to-prometheus","content":" Prometheus is a metric monitoring solution. Please note that prometheus works purely with metrics and other telemetry data like traces and logs won't be sent to it.  Prometheus integration works by adding a special route for the GraphQL server's router that outputs generated metrics in prometheus format consumable by prometheus scraper.  ","version":"Next","tagName":"h3"},{"title":"Data generated​","type":1,"pageTitle":"GraphQL Telemetry","url":"/docs/graphql-telemetry-guide/#data-generated","content":" You can find a reference of type of info generated by Tailcall in the @telemetry reference or consult examples in the next section, in order to gain some understanding.  ","version":"Next","tagName":"h2"},{"title":"Relation with other services​","type":1,"pageTitle":"GraphQL Telemetry","url":"/docs/graphql-telemetry-guide/#relation-with-other-services","content":" Tailcall fully supports Context Propagation functionality and therefore you can analyze distributed traces across all of your services that are provides telemetry data.  That may look like this:    Where Tailcall is a part of whole distributed trace  ","version":"Next","tagName":"h3"},{"title":"Customize generated data​","type":1,"pageTitle":"GraphQL Telemetry","url":"/docs/graphql-telemetry-guide/#customize-generated-data","content":" In some cases you may want to customize the data that were added to telemetry payload to have more control over analyzing process. Tailcall supports that customization for specific use cases described below. For eg. the metric http.server.request.count can be customized with the requestHeaders property to allow splitting the overall count by specific headers.  important The value of specified headers will be sent to telemetry backend as is, so use it with care to prevent of leaking any sensitive data to third-party services you don't have control over. ","version":"Next","tagName":"h3"},{"title":"Integrating Tailcall with Apollo Studio","type":0,"sectionRef":"#","url":"/docs/integrate-apollo-studio-graphql-tailcall/","content":"","keywords":"","version":"Next"},{"title":"Creating a monolith graph​","type":1,"pageTitle":"Integrating Tailcall with Apollo Studio","url":"/docs/integrate-apollo-studio-graphql-tailcall/#creating-a-monolith-graph","content":" Before you configure tailcall, you will need to create a Monolith graph on Apollo Studio. Go to your organization's home page and click on Create your first graph, if this is your first graph or Create New Graph if you have existing graphs. Change the Graph title, Graph ID and other fields as desired and make sure to change Graph Architecture to Monolith, assuming tailcall is booted in monolith mode. Once you are done, click on Next. You'll see the following screen. Copy the fields APOLLO_KEY and APOLLO_GRAPH_REF as they are required by tailcall to be able to send the usage metrics. Next we need to connect Apollo with our running instance of Tailcall. There are two ways to let Apollo know about your GraphQL schema: Navigate to Local Introspection. If you have a deployed instance of your GraphQL server you can put the URL pointing to that in Endpoint URL and click on Introspect and Upload. If not, start a local instance of tailcall and put the local url here, similar to how is shown in the image below. You can start a local instance of Tailcall by running tailcall start (click here to know more). Or, Navigate to Local Schema and insert your schema generated by tailcall and click Upload. You can get the schema by running tailcall check (click here to know more).  You have now created a Monolith graph in Apollo Studio. The next step is to configure tailcall to use the APOLLO_API_KEY and APOLLO_GRAPH_REF. Follow detailed instructions here.  ","version":"Next","tagName":"h2"},{"title":"Checking the metrics in Apollo Studio​","type":1,"pageTitle":"Integrating Tailcall with Apollo Studio","url":"/docs/integrate-apollo-studio-graphql-tailcall/#checking-the-metrics-in-apollo-studio","content":" To see the metrics for you queries follow these instructions:  Start tailcall with the appropriate configuration for Apollo (click here to know more). Below is an example of what a config may look like: schema @server(port: 8000) @upstream( baseURL: &quot;http://jsonplaceholder.typicode.com&quot; ) @telemetry( export: { apollo: { apiKey: &quot;&lt;APOLLO_API_KEY from Apollo Website&gt;&quot; graphRef: &quot;&lt;APOLLO_GRAPH_REF from Apollo Website&gt;&quot; } } ) { query: Query } type Query { posts: [Post] @http(path: &quot;/posts&quot;) } type Post { id: Int! userId: Int! title: String! body: String! } Visit http://localhost:8000/graphql and create a query with an appropriate name (below is an example query named MyQuery) and run it multiple times to send the metrics to Apollo Studio. tip Naming the query is not required to be able to send the metrics, but it helps to organize the metrics with appropriate names when viewed in Apollo Studio. query MyQuery { posts { id title } } To see the metrics click on the VARIANT NAME of your graph. In the example below, the variant name is current. You will see the following page. From here click on insights icon as highlighted on the left side of the image. You will now be able to see all the information related to your queries here  important If you don't see the name of your query here, try running the query multiple times and waiting for some time. Since the metric isn't sent to Apollo Studio for each query, instead they are batched together and sent at once for efficiency reasons. ","version":"Next","tagName":"h2"},{"title":"Deploy Tailcall on AWS Lambda","type":0,"sectionRef":"#","url":"/docs/tailcall-on-aws/","content":"","keywords":"","version":"Next"},{"title":"Generate Access Keys for AWS​","type":1,"pageTitle":"Deploy Tailcall on AWS Lambda","url":"/docs/tailcall-on-aws/#generate-access-keys-for-aws","content":" Follow the steps below to generate the Access Keys:  Go to AWS Management Console and click the drop down menu in the top right corner and Click on Security credentials. Scroll down to the Access Keys section and click on Create access key. You will get the following warning since we are trying to create access keys for the root user. For this guide, we will continue with creating the access keys. If you do not want to continue with the root user, you can learn more about the AWS security credentials here and managing access keys here. Once you click on Create access key, you will get the Access key ID and Secret access key. Make sure to download the CSV file and store it securely.  ","version":"Next","tagName":"h2"},{"title":"Terraform setup​","type":1,"pageTitle":"Deploy Tailcall on AWS Lambda","url":"/docs/tailcall-on-aws/#terraform-setup","content":" Now that you have the AWS Access Key ID and Secret Access Key, you will need to generate API token for terraform and setup a terraform organization and workspace. If you don't have a Terraform Cloud account, you can create one here.  ","version":"Next","tagName":"h2"},{"title":"Terraform API Token​","type":1,"pageTitle":"Deploy Tailcall on AWS Lambda","url":"/docs/tailcall-on-aws/#terraform-api-token","content":" Follow these steps to generate the Terraform API token:  Go to the Tokens section in Settings and click on Create an API token. Give a description for the token and change the expiration if required. Click on Generate token. Copy the generated token and store it securely.  ","version":"Next","tagName":"h3"},{"title":"Terraform Organization and Workspace​","type":1,"pageTitle":"Deploy Tailcall on AWS Lambda","url":"/docs/tailcall-on-aws/#terraform-organization-and-workspace","content":" To create an organization, go to the Organizations section in Settings and click on Create organization. Fill in the organization name and email and click on Create organization. Now that you have created an organization, you will be presented with the following page for creating a workspace. Click on CLI-Driven Workflow, since the github action which we will be using for deployment, tailcallhq/gh-action, uses the terraform CLI. Fill in the workspace name. By default the project will be set to Default Project, if you have any project in terraform cloud, you can select that project, otherwise continue with the Default Project and click on Create.  You now have everything required for a successful deployment of your tailcall server on AWS Lambda.  ","version":"Next","tagName":"h3"},{"title":"Setting up the project repo​","type":1,"pageTitle":"Deploy Tailcall on AWS Lambda","url":"/docs/tailcall-on-aws/#setting-up-the-project-repo","content":" Now you need to create a new repository on Github and use the Github action tailcallhq/gh-action to deploy it. The easiest way to get started is to create a new repository using this template repo https://github.com/tailcallhq/deploy-tailcall.  Go to the repo and click on Use this template and create a new repository. Give your repository a name and click on Create repository. Now that you have created a repository, you will need to add the AWS access keys and Terraform API token to the repository secrets. To do that, click on Settings. Click on Secrets and variables in the left side bar to expand the section and click on Actions. Click on New repository secret to add a new secret. Add the secret name as AWS_ACCESS_KEY_ID or any name you prefer and paste the AWS access key ID that you generated earlier in the value field. Click on Add secret to save the secret. Similarly add the AWS secret access key and the Terraform API token as secrets to the repository.  You are now ready to deploy your tailcall server on AWS Lambda using terraform.  ","version":"Next","tagName":"h2"},{"title":"Deploy on AWS Lambda using terraform​","type":1,"pageTitle":"Deploy Tailcall on AWS Lambda","url":"/docs/tailcall-on-aws/#deploy-on-aws-lambda-using-terraform","content":" In this example, we will deploy a simple graphQL server using tailcall, on AWS Lambda using terraform, which will convert the JSONPlaceholder REST API to a GraphQL API.  Below is the config present in the template repo, that will be used for this deployment. You can learn more about this here.  schema @upstream( baseURL: &quot;http://jsonplaceholder.typicode.com&quot; ) { query: Query } type Query { posts: [Post] @http(path: &quot;/posts&quot;) } type User { id: Int! name: String! username: String! email: String! phone: String website: String } type Post { id: Int! userId: Int! title: String! body: String! user: User @http(path: &quot;/users/{{.value.userId}}&quot;) }   To deploy the server, just update the provider to aws in the deploy-tailcall job in the .github/workflows/main.yml file, similar to the example below. Also, update the terraform-workspace and terraform-org as well as the other inputs based on your requirements.  on: [push] jobs: deploy_tailcall: runs-on: ubuntu-latest name: Deploy Tailcall steps: - name: Checkout repository uses: actions/checkout@v2 - name: Deploy Tailcall id: deploy-tailcall uses: tailcallhq/gh-action@v0.2 with: provider: &quot;aws&quot; aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }} aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }} aws-region: &quot;us-east-1&quot; aws-iam-role: &quot;iam_for_tailcall&quot; terraform-api-token: ${{ secrets.TERRAFORM_API_TOKEN }} terraform-org: &quot;tailcall-demo&quot; terraform-workspace: &quot;tailcall&quot; tailcall-config: &quot;config.graphql&quot;   After updating the main.yml file, commit the changes and push them to the repository. This will trigger the deployment of the tailcall server on AWS Lambda. ","version":"Next","tagName":"h2"},{"title":"Command Line Reference","type":0,"sectionRef":"#","url":"/docs/tailcall-graphql-cli/","content":"","keywords":"","version":"Next"},{"title":"check​","type":1,"pageTitle":"Command Line Reference","url":"/docs/tailcall-graphql-cli/#check","content":" The check command validates a composition spec. Notably, this command can detect potential N+1 issues. To use the check command, follow this format:  tailcall check [OPTIONS] &lt;FILE_PATHS&gt;...   The check command offers options that control settings such as the display of the generated schema, n + 1 issues etc.  ","version":"Next","tagName":"h2"},{"title":"--n-plus-one-queries​","type":1,"pageTitle":"Command Line Reference","url":"/docs/tailcall-graphql-cli/#--n-plus-one-queries","content":" This flag triggers the detection of N+1 issues.  Type: BooleanDefault: false  tailcall check --n-plus-one-queries &lt;FILE_PATHS&gt; ...   ","version":"Next","tagName":"h3"},{"title":"--schema​","type":1,"pageTitle":"Command Line Reference","url":"/docs/tailcall-graphql-cli/#--schema","content":" This option enables the display of the schema of the composition spec.  Type: BooleanDefault: false  tailcall check --schema &lt;file1&gt; &lt;file2&gt; ... &lt;fileN&gt;   The check command allows for files. Specify each file path, separated by a space, after the options.  Example:  tailcall check --schema ./path/to/file1.graphql ./path/to/file2.graphql   ","version":"Next","tagName":"h3"},{"title":"--format​","type":1,"pageTitle":"Command Line Reference","url":"/docs/tailcall-graphql-cli/#--format","content":" This is an optional command which allows changing the format of the input file. It accepts gql or graphql,yml or yaml, json .  tailcall check ./path/to/file1.graphql ./path/to/file2.graphql --format json   ","version":"Next","tagName":"h3"},{"title":"start​","type":1,"pageTitle":"Command Line Reference","url":"/docs/tailcall-graphql-cli/#start","content":" The start command launches the GraphQL Server for the specific configuration.  To start the server, use the following command:  tailcall start &lt;file1&gt; &lt;file2&gt; ... &lt;fileN&gt; &lt;http_path1&gt; &lt;http_path2&gt; .. &lt;http_pathN&gt;   The start command allows for files and supports loading configurations over HTTP. You can mix file system paths with HTTP paths. Specify each path, separated by a space, after the options.  Example:  tailcall start ./path/to/file1.graphql ./path/to/file2.graphql http://example.com/file2.graphql   ","version":"Next","tagName":"h2"},{"title":"init​","type":1,"pageTitle":"Command Line Reference","url":"/docs/tailcall-graphql-cli/#init","content":" The init command bootstraps a new TailCall project. It creates the necessary GraphQL schema files in the provided file path.  tailcall init &lt;file_path&gt;   This command prompts for file creation and configuration, creating the following files:  File Name\tDescription.tailcallrc.schema.json\tProvides autocomplete in your editor when the configuration is written in json or yml format. .graphqlrc.yml\tAn IDE configuration that references your GraphQL configuration (if it's in .graphql format) and the following .tailcallrc.graphql. .tailcallrc.graphql\tContains Tailcall specific auto-completions for .graphql format.  ","version":"Next","tagName":"h2"},{"title":"gen​","type":1,"pageTitle":"Command Line Reference","url":"/docs/tailcall-graphql-cli/#gen","content":" The gen command in the TailCall CLI is designed to generate GraphQL configurations from various sources, such as protobuf files and REST endpoints.  usage:  tailcall gen path_to_configuration_file.json   To generate a TailCall GraphQL configuration, provide a configuration file to the gen command like done above. This configuration file should be in JSON or YAML format, as illustrated in the example below:  JSONYML { &quot;inputs&quot;: [ { &quot;curl&quot;: { &quot;src&quot;: &quot;https://jsonplaceholder.typicode.com/posts/1&quot;, &quot;fieldName&quot;: &quot;post&quot;, &quot;headers&quot;: { &quot;Content-Type&quot;: &quot;application/json&quot;, &quot;Accept&quot;: &quot;application/json&quot;, &quot;Authorization&quot;: &quot;Bearer {{.env.AUTH_TOKEN}}&quot; } } }, { &quot;proto&quot;: { &quot;src&quot;: &quot;./news.proto&quot; } } ], &quot;output&quot;: { &quot;path&quot;: &quot;./output.graphql&quot;, &quot;format&quot;: &quot;graphQL&quot; }, &quot;schema&quot;: { &quot;query&quot;: &quot;Query&quot; }, &quot;preset&quot;: { &quot;mergeType&quot;: 1, &quot;consolidateURL&quot;: 0.5 } }   ","version":"Next","tagName":"h2"},{"title":"Inputs​","type":1,"pageTitle":"Command Line Reference","url":"/docs/tailcall-graphql-cli/#inputs","content":" The inputs section specifies the sources from which the GraphQL configuration should be generated. Each source can be either a REST endpoint or a protobuf file.  REST: When defining REST endpoints, the configuration should include the following properties. src (Required): The URL of the REST endpoint. In this example, it points to a specific post on jsonplaceholder.typicode.com. fieldName (Required): A unique name that should be used as the field name, which is then used in the operation type. In the example below, it's set to post. headers (Optional): Users can specify the required headers to make the HTTP request in the headers section. info Ensure that secrets are not stored directly in the configuration file. Instead, use templates to securely reference secrets from environment variables. For example, see the following configuration where AUTH_TOKEN is referenced from the environment like {{.env.AUTH_TOKEN}}. JSONYML { &quot;curl&quot;: { &quot;src&quot;: &quot;https://jsonplaceholder.typicode.com/posts/1&quot;, &quot;fieldName&quot;: &quot;post&quot;, &quot;headers&quot;: { &quot;Authorization&quot;: &quot;Bearer {{.env.AUTH_TOKEN}}&quot; } } } For the above input configuration, the following field will be generated in the operation type: type Query { # field name is taken from the above JSON config post(p1: Int!): Post @http(path: &quot;/posts/{{arg.p1}}&quot;) } important Ensure that each field name is unique across the entire configuration to prevent overwriting previous definitions. Proto: For protobuf files, specify the path to the proto file (src). JSONYML { &quot;proto&quot;: { &quot;src&quot;: &quot;./path/to/file.proto&quot; } }   ","version":"Next","tagName":"h3"},{"title":"Output​","type":1,"pageTitle":"Command Line Reference","url":"/docs/tailcall-graphql-cli/#output","content":" The output section specifies the path and format for the generated GraphQL configuration.  path: The file path where the output will be saved.format: The format of the output file. Supported formats are json, yml, and graphQL.  tip You can also change the format of the configuration later using the check command.  ","version":"Next","tagName":"h3"},{"title":"Preset​","type":1,"pageTitle":"Command Line Reference","url":"/docs/tailcall-graphql-cli/#preset","content":" The config generator provides a set of tuning parameters that can make the generated configurations more readable by reducing duplication. This can be configured using the preset section.  JSONYML Presets with default values { &quot;preset&quot;: { &quot;mergeType&quot;: 1, &quot;consolidateURL&quot;: 0.5, }, }   mergeType: This setting merges types in the configuration that satisfy the threshold criteria. It takes a threshold value between 0.0 and 1.0 to determine if two types should be merged or not. The default is 1.0. For example, the following types T1 and T2 are exactly similar, and with a threshold value of 1.0, they can be merged into a single type called M1: Merging type T1 and T2 into M1 # BEFORE type T1 { id: ID firstName: String lastName: String } type T2 { id: ID firstName: String lastName: String } # AFTER: T1 and T2 are merged into M1. type M1 { id: ID firstName: String lastName: String } consolidateURL: The setting identifies the most common base URL among multiple REST endpoints and uses this URL in the upstream directive. It takes a threshold value between 0.0 and 1.0 to determine the most common endpoint. The default is 0.5. For example, if the Query type has three base URLs, using the consolidateURL setting with a 0.5 threshold will pick the base URL that is used in more than 50% of the http directives, http://jsonplaceholder.typicode.com, and add it to the upstream, cleaning the base URLs from the Query type. schema @server(hostname: &quot;0.0.0.0&quot;, port: 8000) @upstream(httpCache: 42) { query: Query } type Query { post(id: Int!): Post @http( baseURL: &quot;http://jsonplaceholder.typicode.com&quot; path: &quot;/posts/{{.args.id}}&quot; ) posts: [Post] @http( baseURL: &quot;http://jsonplaceholder.typicode.com&quot; path: &quot;/posts&quot; ) user(id: Int!): User @http( baseURL: &quot;http://jsonplaceholder.typicode.com&quot; path: &quot;/users/{{.args.id}}&quot; ) users: [User] @http( baseURL: &quot;http://jsonplaceholder-1.typicode.com&quot; path: &quot;/users&quot; ) } After enabling the consolidateURL setting: schema @server(hostname: &quot;0.0.0.0&quot;, port: 8000) @upstream( baseURL: &quot;http://jsonplaceholder.typicode.com&quot; httpCache: 42 ) { query: Query } type Query { post(id: Int!): Post @http(path: &quot;/posts/{{.args.id}}&quot;) posts: [Post] @http(path: &quot;/posts&quot;) user(id: Int!): User @http(path: &quot;/users/{{.args.id}}&quot;) users: [User] @http( baseURL: &quot;http://jsonplaceholder-1.typicode.com&quot; path: &quot;/users&quot; ) }  ","version":"Next","tagName":"h3"},{"title":"CTOs Guide to GraphQL","type":0,"sectionRef":"#","url":"/graphql/cto-guide/","content":"CTOs Guide to GraphQL Good APIs craft a broad spectrum of functionalities. Yet, the broader their scope, the more they diverge from being the perfect fit for any specific use case. This fundamental discrepancy — the impedance mismatch between the general capabilities of an API and the precise needs of a particular scenario — amplifies the necessity for an orchestration layer. Such a layer adeptly bridges this gap, tailor-fitting generic APIs to meet exact requirements with finesse. Tailcall stands at the forefront of this innovation, seamlessly transforming the way APIs are integrated and interacted with. Tailcall introduces a set of primitives, enabling developers to express and fine-tune how APIs are orchestrated without writing any code. This approach facilitates specifying different caching and batching strategies to enhance the overall system's efficiency. It also enables precise governance and access control mechanisms on actual domain entities and their relationships. Tailcall serves as a central hub for team collaboration, offering a unified point for managing all APIs, documentation, and more. Once configured, it positions itself between the clients and microservices, adeptly managing all requests and orchestrating them as needed. Manually crafting BFF (Backend for Frontend) layers has become outdated. With Tailcall, API orchestration evolves into a streamlined and highly optimized process. It functions as an essential intermediary, intelligently directing requests and assembling responses from each microservice. This approach diminishes the development burden associated with traditional BFF layers but also bolsters performance, reliability, and scalability throughout the application infrastructure.","keywords":"","version":"Next"},{"title":"GraphQL Configuration Formats","type":0,"sectionRef":"#","url":"/docs/tailcall-graphql-configuration-format-conversion/","content":"","keywords":"","version":"Next"},{"title":"Converting Formats​","type":1,"pageTitle":"GraphQL Configuration Formats","url":"/docs/tailcall-graphql-configuration-format-conversion/#converting-formats","content":" To convert files between different formats, use the following command:  tailcall check format_type &lt; input_files &gt; --format   Let's try to convert a Tailcall graphql file to json and then back to graphql  To convert graphql to json  tailcall check examples/jsonplaceholder.graphql --format json &gt; &quot;examples/jsonplaceholder.json&quot;   Now to convert back to graphql  tailcall check examples/jsonplaceholder.json --format graphql &gt; &quot;examples/jsonplaceholder2.graphql&quot;   To learn more about writing configuration to leverage the full power of Tailcall, explore the Directives documentation.  ","version":"Next","tagName":"h2"},{"title":"Format Conversions​","type":1,"pageTitle":"GraphQL Configuration Formats","url":"/docs/tailcall-graphql-configuration-format-conversion/#format-conversions","content":" graphqlymljson schema @server(port: 8000, hostname: &quot;0.0.0.0&quot;) @upstream(baseURL: &quot;http://jsonplaceholder.typicode.com&quot;, httpCache: 42, batch: {delay: 100}) { query: Query } type Query { posts: [Post] @http(path: &quot;/posts&quot;) users: [User] @http(path: &quot;/users&quot;) user(id: Int!): User @http(path: &quot;/users/{{.args.id}}&quot;) } type User { id: Int! name: String! username: String! email: String! phone: String website: String } type Post { id: Int! userId: Int! title: String! body: String! user: User @call(query: &quot;user&quot;, args: {id: &quot;{{.value.userId}}&quot;}) }   ","version":"Next","tagName":"h2"},{"title":"Editor Support​","type":1,"pageTitle":"GraphQL Configuration Formats","url":"/docs/tailcall-graphql-configuration-format-conversion/#editor-support","content":" To leverage autocomplete and validation for GraphQL configurations, the init command can be used to automatically create .tailcallrc.graphql for GraphQL configurations and .tailcallrc.schema.json for JSON and YAML configurations. These files enhance editor support by providing schema definitions, facilitating faster and error-free configuration.  ","version":"Next","tagName":"h2"},{"title":"GraphQL​","type":1,"pageTitle":"GraphQL Configuration Formats","url":"/docs/tailcall-graphql-configuration-format-conversion/#graphql","content":" When you run tailcall init, it creates a .tailcallrc.graphql file containing your GraphQL schema definitions and a .graphqlrc.yml file configured to use this schema. The .graphqlrc.yml file is set up as follows:  schema: - &quot;./.tailcallrc.graphql&quot; - &quot;./app.graphql&quot;   This file contains the path to the .tailcallrc.graphql file and the path to the main GraphQL configuration file which is app.graphql. This setup allows GraphQL IDE plugins and Language Server Protocols (LSP) to automatically pick up the schema for autocomplete and validation, enhancing your development experience with real-time feedback and suggestions.  ","version":"Next","tagName":"h3"},{"title":"JSON & YAML​","type":1,"pageTitle":"GraphQL Configuration Formats","url":"/docs/tailcall-graphql-configuration-format-conversion/#json--yaml","content":" For JSON or YAML configurations, tailcall init also creates a .tailcallrc.schema.json file. To enable validation and autocomplete in your JSON files, reference the .tailcallrc.schema.json in the $schema attribute at the beginning of your JSON file:  { &quot;$schema&quot;: &quot;./.tailcallrc.schema.json&quot; }   This reference enables your IDE to validate and autocomplete using the JSON schema, offering a streamlined configuration process with instant error and typo detection. ","version":"Next","tagName":"h3"},{"title":"The Comprehensive Guide to GraphQL","type":0,"sectionRef":"#","url":"/graphql/","content":"","keywords":"","version":"Next"},{"title":"Introduction to GraphQL​","type":1,"pageTitle":"The Comprehensive Guide to GraphQL","url":"/graphql/#introduction-to-graphql","content":" GraphQL, developed by Facebook in 2012 and open-sourced in 2015, is a query language for your API, and a server-side runtime for executing queries. It was originally developed to simplify endpoint management for REST-based APIs. Instead of maintaining multiple endpoints with small amounts of disjointed data, GraphQL provides a single endpoint that inputs complex queries and outputs only as much information as is needed for the query. This flexibility empowers developers to design more efficient and adaptable APIs, making GraphQL increasingly popular in modern web development.  At its core, GraphQL enables declarative data fetching, where clients specify the exact structure of the data they require, and the server responds with precisely that data. This contrasts with REST APIs, where endpoints are predefined, and clients receive fixed responses regardless of their specific data needs.  To know more in detail about GraphQL, you can refer What is GraphQL.  ","version":"Next","tagName":"h2"},{"title":"Fundamental Concepts of GraphQL Server​","type":1,"pageTitle":"The Comprehensive Guide to GraphQL","url":"/graphql/#fundamental-concepts-of-graphql-server","content":" GraphQL is built upon several core concepts that form the foundation of its query language and API design. Understanding these concepts is essential for effectively utilizing GraphQL in your projects.  ","version":"Next","tagName":"h2"},{"title":"1. Schema​","type":1,"pageTitle":"The Comprehensive Guide to GraphQL","url":"/graphql/#1-schema","content":" At the heart of every GraphQL API is the schema which is a set of types which completely describe the set of possible data you can query on that server. Then, when queries come in, they are validated and executed against that schema.  GraphQL schemas are typically defined using the GraphQL Schema Definition Language (SDL), a simple syntax for describing types. Each type in the schema represents a distinct object in the API. Types can have fields that correspond to the properties of the object, as well as relationships to other types.  Here's a code snippet illustrating how a GraphQL schema might be defined using the GraphQL Schema Definition Language (SDL):  schema { query: Query } type Post { id: ID! title: String! content: String! userId: ID! } type Query { post(id: ID!): Post }   In this schema:  We define an object type: Post.Each Post object has fields for id, title, content, and a userId.We also define a root Query type, which contains entry points for querying individual users and posts by their IDs. We will dig deep into this in Next Section  This schema serves as a contract between the client and the server, defining the structure of the data available through the API and specifying how clients can interact with it. It forms the foundation for building and querying data in a GraphQL API. You can learn more about GraphQL Schema in detail.  ","version":"Next","tagName":"h3"},{"title":"2. Query​","type":1,"pageTitle":"The Comprehensive Guide to GraphQL","url":"/graphql/#2-query","content":" Most types in your schema will just be normal object types, but there are two types that are special within a schema Query and Mutation.  Every GraphQL server has a query type and may or may not have a mutation type. These types are the same as a regular object type, but they are special because they define the entry point of every GraphQL query. So if you see a query that looks like:  query { user(id: &quot;123&quot;) { name } }   That means that the GraphQL service needs to have a Query type with user field:  type Query { user(id: ID!): User }   ","version":"Next","tagName":"h3"},{"title":"3. Mutation​","type":1,"pageTitle":"The Comprehensive Guide to GraphQL","url":"/graphql/#3-mutation","content":" Mutation work in a similar way as Query - you define fields on the Mutation type, and those are available as the root mutation fields you can call in your query.  It’s important to remember that other than the special status of being the “entry point” into the schema, the Query and Mutation types are the same as any other GraphQL object type, and their fields work exactly the same way. While queries are used for reading data from a GraphQL API, mutations are used for modifying or updating data. Mutations allow clients to perform actions such as creating, updating, or deleting objects in the API's data store.  Here's a code snippet illustrating how mutations might be defined in GraphQL:  type Mutation { createUser(input: CreateUserInput!): User! }   To query above mutation, you would use a query like this:  mutation { createUser( input: {name: &quot;Alice&quot;, email: &quot;dummy@gmail.com&quot;} ) { id name email } }   ","version":"Next","tagName":"h3"},{"title":"4. Subscriptions​","type":1,"pageTitle":"The Comprehensive Guide to GraphQL","url":"/graphql/#4-subscriptions","content":" Subscriptions enable real-time communication between clients and servers in GraphQL APIs. Unlike queries and mutations, which are request-response interactions, subscriptions establish a persistent connection between the client and the server, allowing the server to push updates to the client as they occur.  Subscriptions are defined in the GraphQL schema alongside queries and mutations, specifying the events or data changes that clients can subscribe to. Clients initiate a subscription by sending a subscription query to the server, which then notifies the client whenever the subscribed event occurs.  Here's a code snippet illustrating how subscriptions might be defined in GraphQL:  type Subscription { newMessage: Message! } type Message { id: ID! content: String! createdAt: String! } input SendMessageInput { content: String! } type Mutation { sendMessage(input: SendMessageInput!): Message! }   In this schema:  We define a Subscription type, which contains an entry point newMessage representing the event that clients can subscribe to receive updates about new messages.The newMessage subscription returns a Message object whenever a new message is created.We define a Message type with fields for id, content, and createdAt, representing a message object.We also define an input object type SendMessageInput with a field for content, which represents the input data for creating a new message.Finally, we have a Mutation type with an operation sendMessage that takes an input object of type SendMessageInput and returns the created Message object.  Clients can subscribe to the newMessage event to receive real-time updates whenever a new message is created. When a new message is created, the server pushes the updated message data to all subscribed clients in real-time.  In summary, the fundamental concepts of GraphQL—schema, queries, mutations, and subscriptions—provide a powerful framework for building flexible and efficient APIs that meet the diverse needs of modern applications.  This GraphQL configuration is enough for starting a GraphQL server using Tailcall. You can start the server using the start command.  ","version":"Next","tagName":"h3"},{"title":"Advantages of Using GraphQL​","type":1,"pageTitle":"The Comprehensive Guide to GraphQL","url":"/graphql/#advantages-of-using-graphql","content":" While REST (Representational State Transfer) has long been a dominant paradigm for building web APIs, GraphQL offers several key advantages that revolutionize API design and development:  ","version":"Next","tagName":"h2"},{"title":"1. Efficient Data Fetching​","type":1,"pageTitle":"The Comprehensive Guide to GraphQL","url":"/graphql/#1-efficient-data-fetching","content":" One of the primary advantages of GraphQL is its ability to efficiently fetch data from the server. With GraphQL, clients can request only the specific fields they need, avoiding the problem of over-fetching data that often occurs with REST APIs. This minimizes the amount of data transferred over the network, leading to faster response times and improved performance for client applications.  Additionally, GraphQL enables clients to retrieve related pieces of data in a single request by specifying nested query structures. This eliminates the need for multiple round-trip requests to fetch data from different endpoints, further optimizing data fetching efficiency.  ","version":"Next","tagName":"h3"},{"title":"2. Strongly-Typed Schema​","type":1,"pageTitle":"The Comprehensive Guide to GraphQL","url":"/graphql/#2-strongly-typed-schema","content":" GraphQL employs a strongly-typed schema to define the structure of the API and the types of data it supports. This schema serves as a contract between the client and the server, providing clear documentation of the available data and operations.  By defining a schema upfront, GraphQL enables type checking and validation of queries at compile-time, catching errors early in the development process. This helps prevent runtime errors and improves the reliability of client-server interactions.  Additionally, the schema serves as a central source of truth for the API, making it easier for frontend and backend developers to collaborate. Changes to the schema can be communicated effectively, and tools can be built around the schema to provide features like autocomplete and code generation.  ","version":"Next","tagName":"h3"},{"title":"3. Reduced Network Requests​","type":1,"pageTitle":"The Comprehensive Guide to GraphQL","url":"/graphql/#3-reduced-network-requests","content":" GraphQL reduces the number of network requests required to fetch data by allowing clients to specify complex queries in a single request. Unlike REST APIs, which often require multiple requests to retrieve related data from different endpoints, GraphQL enables clients to fetch all the necessary data in a single round trip.  This reduction in network requests can have significant performance benefits, particularly for mobile and web applications operating over limited bandwidth or high-latency networks. By minimizing the overhead associated with network communication, GraphQL helps improve the responsiveness and efficiency of client applications.  In summary, GraphQL represents a paradigm shift in API design, offering developers greater flexibility, efficiency, and control over data fetching compared to traditional RESTful architectures. By leveraging the advantages of GraphQL—efficient data fetching, strongly-typed schema, and reduced network requests—developers can create more scalable, flexible, and performant systems that meet the needs of modern applications and users.  ","version":"Next","tagName":"h3"},{"title":"Getting Started with GraphQL​","type":1,"pageTitle":"The Comprehensive Guide to GraphQL","url":"/graphql/#getting-started-with-graphql","content":" Before diving into GraphQL development, it's essential to understand the basic steps involved in setting up a GraphQL server, defining a schema, and creating resolvers.  ","version":"Next","tagName":"h2"},{"title":"1. Setting up a GraphQL Server​","type":1,"pageTitle":"The Comprehensive Guide to GraphQL","url":"/graphql/#1-setting-up-a-graphql-server","content":" Setting up a GraphQL server involves configuring a server environment capable of processing GraphQL requests and responses. You can set up a GraphQL Server using Tailcall by following the steps below:  Install Tailcall's GraphQL CLI by running the following command:  npm install -g @tailcallhq/tailcall   Set up a new GraphQL project by running the following command on a new project directory:  tailcall init   Using Tailcall's GraphQL CLI, you can quickly set up a GraphQL server with minimal configuration. The CLI provides a streamlined development environment for building GraphQL APIs.  ","version":"Next","tagName":"h3"},{"title":"2. Defining a Schema​","type":1,"pageTitle":"The Comprehensive Guide to GraphQL","url":"/graphql/#2-defining-a-schema","content":" The schema is a critical component of any GraphQL API, as it defines the types of data that clients can query and manipulate. GraphQL schemas are typically written using the GraphQL Schema Definition Language (SDL), a simple syntax for describing types, fields, and relationships.  To define a schema, you'll need to specify the types of data available in your API, including objects, along with their associated fields and relationships. You'll also define query, mutation, and subscription types to specify the operations that clients can perform. for example:  schema { query: Query } type Post { id: ID! title: String! content: String! } type Query { post(id: ID!): Post }   ","version":"Next","tagName":"h3"},{"title":"3. Attaching Resolvers​","type":1,"pageTitle":"The Comprehensive Guide to GraphQL","url":"/graphql/#3-attaching-resolvers","content":" Resolvers are responsible for fetching the data requested by clients and returning it in the appropriate format. Each field in your schema corresponds to a resolver function, which retrieves the data from your data sources, such as databases, APIs, or in-memory caches.  When a client sends a GraphQL query, the server resolves each field in the query by executing the corresponding resolver function. Resolvers may perform database queries, call external APIs, or perform other tasks to retrieve the requested data. Once the data is fetched, the resolvers return it to the client in the format specified by the GraphQL schema.  By setting up a GraphQL server, defining a schema, and attaching resolvers, you can begin building powerful and flexible APIs that provide clients with the precise data they need. This foundational knowledge forms the basis for more advanced GraphQL development, including integrating GraphQL with existing APIs, handling complex data relationships, and optimizing API performance.  Lets attach resolvers to the schema we defined in the previous section using Tailcall's GraphQL Configuration: We will add the resolvers with @http directive:  schema @upstream( baseURL: &quot;http://jsonplaceholder.typicode.com&quot; ) { query: Query } type Post { id: ID! title: String! content: String! } type Query { post(id: ID!): Post @http(path: &quot;/posts/{{.args.id}}&quot;) }   In this example: We have used the @http directive to attach resolvers to the post field in the Query. and @upstream directive to define the base URL for the upstream server.  This configuration is enough for starting a GraphQL server using Tailcall. You can start the server using the start command.  For starting the server, you can use the following command:  tailcall start path/to/your-graphql-configuration   ","version":"Next","tagName":"h3"},{"title":"4. Handling Data Relationships​","type":1,"pageTitle":"The Comprehensive Guide to GraphQL","url":"/graphql/#4-handling-data-relationships","content":" One of the strengths of GraphQL is its ability to handle data relationships with ease. When designing your GraphQL schema, consider how different types of data relate to each other and how clients may want to query or manipulate these relationships.  Use GraphQL's powerful type system to define clear relationships between your data types, including one-to-one, one-to-many, and many-to-many relationships. This allows clients to fetch related data in a single query, reducing the need for multiple round-trip requests.  Here's a code snippet illustrating how to handle data relationships in GraphQL:  schema @upstream( baseURL: &quot;http://jsonplaceholder.typicode.com&quot; ) { query: Query } type User { id: ID! name: String! email: String! } type Post { id: ID! title: String! content: String! userId: ID! author: User! @http(path: &quot;/users/{{.value.userId}}&quot;) } type Query { post(id: ID!): Post! @http(path: &quot;/posts/{{.args.id}}&quot;) }   In this example:  We define a User type representing a user object with fields for id, name, and email.We define a Post type representing a post object with fields for id, title, content, and userId.We establish a relationship between Post and User by adding an author field to the Post type. The author field fetches the user data corresponding to the userId of the post.We attach resolvers to the author field using the @http directive to fetch user data from the upstream server.  ","version":"Next","tagName":"h3"},{"title":"Tools and Libraries for GraphQL​","type":1,"pageTitle":"The Comprehensive Guide to GraphQL","url":"/graphql/#tools-and-libraries-for-graphql","content":" Utilizing the right tools and libraries can significantly streamline the development, testing, and maintenance of your GraphQL API. Here are some essential tools and resources:  ","version":"Next","tagName":"h2"},{"title":"1. Popular GraphQL Clients​","type":1,"pageTitle":"The Comprehensive Guide to GraphQL","url":"/graphql/#1-popular-graphql-clients","content":" Apollo Client: A comprehensive GraphQL client for JavaScript applications, offering features such as query caching, local state management, and error handling. Apollo Client supports various frontend frameworks, including React, Angular, and Vue.js.  Relay: Developed by Facebook, Relay is a powerful GraphQL client optimized for React applications. It provides features like declarative data fetching, pagination, and efficient updates through GraphQL mutations.  URQL (formerly known as &quot;Urql&quot;): A lightweight and flexible GraphQL client for React and other JavaScript frameworks. URQL focuses on simplicity, performance, and customization, offering hooks-based APIs for querying and caching GraphQL data.  ","version":"Next","tagName":"h3"},{"title":"2. Testing and Debugging Tools​","type":1,"pageTitle":"The Comprehensive Guide to GraphQL","url":"/graphql/#2-testing-and-debugging-tools","content":" GraphQL Playground: An interactive GraphQL IDE that allows you to explore, test, and debug GraphQL APIs using a web-based interface. GraphQL Playground supports features like syntax highlighting, query autocompletion, and response visualization, making it invaluable for API development and testing.  GraphiQL: Similar to GraphQL Playground, GraphiQL is a web-based IDE for testing and debugging GraphQL APIs. It provides a user-friendly interface for composing and executing GraphQL queries, with built-in documentation and query history functionality.  Apollo Studio Explorer: Part of the Apollo Studio platform, Apollo Studio Explorer offers a visual GraphQL editor and testing environment for exploring GraphQL schemas, executing queries, and analyzing query performance. It integrates seamlessly with Apollo Client and provides insights into schema usage and query execution metrics.  Tailcall Playground: Tailcall Playground is a web-based IDE for testing and debugging GraphQL servers,and analyze the behavior of your GraphQL server.  ","version":"Next","tagName":"h3"},{"title":"3. Community Resources and Support​","type":1,"pageTitle":"The Comprehensive Guide to GraphQL","url":"/graphql/#3-community-resources-and-support","content":" GraphQL Documentation: Comprehensive documentation resources are available for GraphQL, covering topics such as schema definition, query syntax, and best practices for API design. The official GraphQL website (graphql.org) provides detailed guides, tutorials, and reference documentation for learning and mastering GraphQL.  GitHub Repositories: Explore open-source GraphQL projects and libraries on GitHub to discover reusable components, utilities, and tools for building and extending GraphQL APIs. Many popular GraphQL clients, server frameworks, and development tools are hosted on GitHub, offering opportunities for collaboration and contribution.  Online Communities: Engage with the GraphQL community through online forums, discussion groups, and social media channels. Platforms like Reddit (r/graphql), Stack Overflow, and Discord host active communities of GraphQL enthusiasts and practitioners, where you can ask questions, share knowledge, and seek advice on GraphQL-related topics.  By leveraging these tools and resources, you can enhance your GraphQL development workflow, streamline API testing and debugging, and tap into the collective expertise of the GraphQL community for support and guidance.  ","version":"Next","tagName":"h3"},{"title":"Real-world Examples of GraphQL Implementation​","type":1,"pageTitle":"The Comprehensive Guide to GraphQL","url":"/graphql/#real-world-examples-of-graphql-implementation","content":" Real-world examples of GraphQL implementation offer valuable insights into the practical applications, benefits, and challenges of adopting GraphQL in different industries and use cases. Here are some case studies and lessons learned from successful GraphQL adoption:  ","version":"Next","tagName":"h2"},{"title":"1. Case Studies of Successful GraphQL Adoption​","type":1,"pageTitle":"The Comprehensive Guide to GraphQL","url":"/graphql/#1-case-studies-of-successful-graphql-adoption","content":" GitHub: GitHub adopted GraphQL to address the inefficiencies and complexities of its RESTful API, which led to over-fetching, under-fetching, and versioning challenges. By transitioning to GraphQL, GitHub improved data fetching efficiency, reduced network requests, and provided a more flexible and intuitive API for developers. GraphQL enabled GitHub to deliver personalized data, optimize performance, and streamline client-server communication.  Shopify: Shopify leveraged GraphQL to power its next-generation commerce platform, enabling merchants to build customized storefronts and applications. GraphQL empowered Shopify developers to fetch precisely the data they needed, avoiding over-fetching and reducing latency. Shopify's adoption of GraphQL resulted in improved developer productivity, faster feature development, and enhanced API performance for merchants and partners.  ","version":"Next","tagName":"h3"},{"title":"2. Lessons Learned from Industry Use Cases​","type":1,"pageTitle":"The Comprehensive Guide to GraphQL","url":"/graphql/#2-lessons-learned-from-industry-use-cases","content":" Performance Optimization: Many companies, including Facebook, Twitter, and Airbnb, have shared insights into optimizing GraphQL performance at scale. Key strategies include batching requests, implementing caching mechanisms, optimizing resolver functions, and monitoring query execution times. By optimizing GraphQL performance, organizations can minimize latency, improve scalability, and enhance user experience.  Data Modeling and Schema Design: Successful GraphQL implementations emphasize the importance of thoughtful data modeling and schema design. Designing a clear and intuitive schema, defining relationships between types, and normalizing data structures are critical for building scalable and maintainable GraphQL APIs. Companies like PayPal and Coursera have documented their approaches to schema design, highlighting best practices for organizing and structuring data in GraphQL schemas.  Developer Experience: Prioritizing developer experience (DX) is essential for driving adoption and success with GraphQL. Providing comprehensive documentation, offering interactive tooling (e.g., GraphQL playgrounds), and fostering a supportive developer community are key factors in promoting GraphQL adoption. Companies like Apollo and Prisma have contributed to the GraphQL ecosystem by developing tools, libraries, and educational resources to empower developers and simplify GraphQL development workflows.  By studying real-world examples of GraphQL implementation and learning from industry use cases, organizations can gain valuable insights into the benefits, challenges, and best practices associated with adopting GraphQL. Whether optimizing performance, designing schemas, or enhancing developer experience, GraphQL offers compelling advantages for building modern, data-driven applications in various domains and industries.  ","version":"Next","tagName":"h3"},{"title":"Future Trends in GraphQL​","type":1,"pageTitle":"The Comprehensive Guide to GraphQL","url":"/graphql/#future-trends-in-graphql","content":" As GraphQL continues to evolve and gain traction in the developer community, several emerging trends and advancements are shaping the future of GraphQL adoption and implementation. Here are some key areas to watch:  ","version":"Next","tagName":"h2"},{"title":"1. GraphQL in the Context of Microservices​","type":1,"pageTitle":"The Comprehensive Guide to GraphQL","url":"/graphql/#1-graphql-in-the-context-of-microservices","content":" Service Mesh Integration: With the increasing adoption of microservices architectures, GraphQL is poised to play a significant role in service mesh environments. Integrating GraphQL with service mesh technologies such as Istio and Linkerd enables developers to build distributed, resilient, and scalable applications with enhanced API management, observability, and security capabilities.  GraphQL Federation: GraphQL federation, an architectural pattern for composing distributed GraphQL schemas, is gaining momentum as a preferred approach for building scalable and modular microservices architectures. By federating multiple GraphQL APIs into a unified graph, organizations can achieve greater agility, autonomy, and composability across their microservices ecosystem.  ","version":"Next","tagName":"h3"},{"title":"2. Integration with Emerging Technologies​","type":1,"pageTitle":"The Comprehensive Guide to GraphQL","url":"/graphql/#2-integration-with-emerging-technologies","content":" Serverless Computing: The integration of GraphQL with serverless computing platforms such as AWS Lambda, Azure Functions, and Google Cloud Functions enables developers to build event-driven, scalable APIs with minimal operational overhead. Serverless GraphQL functions provide dynamic data resolution, auto-scaling, and cost-efficient execution, making them ideal for modern, cloud-native applications.  Edge Computing: GraphQL is increasingly being adopted in edge computing scenarios, where low-latency data access and distributed processing are critical requirements. By deploying GraphQL at the edge using technologies like Cloudflare Workers and AWS CloudFront, organizations can deliver performant, responsive APIs to edge locations worldwide, improving user experience and application performance.  ","version":"Next","tagName":"h3"},{"title":"3. Potential Advancements in the GraphQL Ecosystem​","type":1,"pageTitle":"The Comprehensive Guide to GraphQL","url":"/graphql/#3-potential-advancements-in-the-graphql-ecosystem","content":" GraphQL Standardization: As GraphQL adoption continues to grow, efforts to standardize GraphQL specifications, tooling, and best practices are gaining momentum. Organizations such as the GraphQL Foundation and the GraphQL Working Group are leading initiatives to define and maintain GraphQL standards, promote interoperability, and drive innovation in the GraphQL ecosystem.  Schema Stitching and Composition: Advanced schema stitching and composition techniques are emerging to address the challenges of building and managing complex GraphQL schemas. Tools and libraries for schema stitching, such as Apollo Federation and GraphQL Mesh, enable developers to compose distributed schemas, federate data sources, and implement cross-cutting concerns like authentication, authorization, and caching.  Advanced Query Optimization: Ongoing research and development efforts are focused on advancing query optimization techniques for GraphQL APIs. Innovations in query planning, execution, and caching are enhancing the performance, scalability, and efficiency of GraphQL queries, enabling organizations to deliver real-time, data-intensive applications with low latency and high throughput.  As GraphQL continues to mature and expand its capabilities, organizations can expect to see further integration with microservices, emerging technologies, and advancements in the GraphQL ecosystem. By staying informed about these future trends and embracing GraphQL as a strategic technology, organizations can unlock new opportunities for innovation, agility, and growth in the rapidly evolving landscape of modern software development.  ","version":"Next","tagName":"h3"},{"title":"Conclusion​","type":1,"pageTitle":"The Comprehensive Guide to GraphQL","url":"/graphql/#conclusion","content":" In conclusion, GraphQL has emerged as a powerful query language and runtime for building efficient, flexible, and scalable APIs. Throughout this guide, we've explored the fundamental concepts, advantages, implementation strategies, optimization techniques, and future trends of GraphQL. Here's a summary of the key takeaways:  ","version":"Next","tagName":"h2"},{"title":"Key Takeaways​","type":1,"pageTitle":"The Comprehensive Guide to GraphQL","url":"/graphql/#key-takeaways","content":" GraphQL offers several advantages over traditional REST APIs, including efficient data fetching, a strongly-typed schema, and reduced network requests. By enabling clients to request precisely the data they need, GraphQL optimizes data transfer and improves application performance.Understanding the fundamental concepts of GraphQL, such as schema, queries, mutations, and subscriptions, is essential for effectively designing, implementing, and consuming GraphQL APIs. With a clear understanding of these concepts, developers can leverage the full potential of GraphQL to build sophisticated and intuitive APIs.Implementing GraphQL in your API involves setting up a GraphQL server, defining a schema, and attaching resolvers to fetch and manipulate data. By structuring queries and mutations, developers can design APIs that are easy to understand, maintain, and evolve over time.  ","version":"Next","tagName":"h3"},{"title":"Recap of Fundamental Concepts​","type":1,"pageTitle":"The Comprehensive Guide to GraphQL","url":"/graphql/#recap-of-fundamental-concepts","content":" Schema: Defines the types of data available in the API and the relationships between them.Queries: Retrieve data from the API.Mutations: Modify data in the API.Subscriptions: Listen for real-time updates from the API.  By embracing GraphQL and leveraging its capabilities to build modern, data-driven APIs, developers can unlock new opportunities for innovation, collaboration, and growth. Whether you're a beginner exploring the basics of GraphQL or an experienced developer optimizing complex APIs, the comprehensive guide to GraphQL provides valuable insights and guidance to support your journey in mastering this powerful technology.  Thank you for joining us on this exploration of GraphQL. We hope this guide has equipped you with the knowledge and tools needed to build successful GraphQL APIs and navigate the evolving landscape of modern software development.  ","version":"Next","tagName":"h3"},{"title":"Next Steps​","type":1,"pageTitle":"The Comprehensive Guide to GraphQL","url":"/graphql/#next-steps","content":" Congratulations on completing this comprehensive guide to GraphQL! As you continue your journey to mastering GraphQL and building powerful APIs, here are some recommended next steps to further enhance your skills and stay updated on GraphQL developments:  ","version":"Next","tagName":"h2"},{"title":"Further Resources for Mastering GraphQL​","type":1,"pageTitle":"The Comprehensive Guide to GraphQL","url":"/graphql/#further-resources-for-mastering-graphql","content":" Explore in-depth tutorials, documentation, and guides provided by GraphQL's official website (https://graphql.org/). The website offers a wealth of resources, including tutorials, specifications, and best practices for implementing GraphQL APIs.Dive deeper into GraphQL concepts and techniques with online courses and tutorials available on platforms like Udemy, Coursera, and Pluralsight. These courses cover a wide range of topics, from GraphQL fundamentals to advanced topics like schema stitching and federation.Check out books and ebooks on GraphQL, such as &quot;Learning GraphQL: Declarative Data Fetching for Modern Web Apps&quot; by Eve Porcello and &quot;The GraphQL Guide&quot; by John Resig and Loren Sands-Ramshaw. These resources provide comprehensive coverage of GraphQL concepts, best practices, and real-world examples.  ","version":"Next","tagName":"h2"},{"title":"Community Forums and Events for Staying Updated on GraphQL Developments​","type":1,"pageTitle":"The Comprehensive Guide to GraphQL","url":"/graphql/#community-forums-and-events-for-staying-updated-on-graphql-developments","content":" Join the GraphQL community on platforms like GitHub, Reddit, and Stack Overflow to connect with other developers, ask questions, and share insights and experiences. These forums are valuable resources for staying updated on the latest trends, tools, and techniques in the GraphQL ecosystem.Attend GraphQL meetups, conferences, and workshops to network with industry experts, learn from experienced developers, and gain hands-on experience with GraphQL. Events like GraphQL Summit, GraphQL Europe, and GraphQL Asia offer opportunities to connect with the broader GraphQL community and stay informed about emerging trends and best practices. ","version":"Next","tagName":"h2"},{"title":"Comprehensive Guide to GraphQL Fragments","type":0,"sectionRef":"#","url":"/graphql/graphql-fragments/","content":"","keywords":"","version":"Next"},{"title":"Introduction to GraphQL Fragments​","type":1,"pageTitle":"Comprehensive Guide to GraphQL Fragments","url":"/graphql/graphql-fragments/#introduction-to-graphql-fragments","content":" GraphQL fragments are reusable units that allow developers to define a piece of a query once and use it in multiple places. This capability not only reduces redundancy but also improves the maintainability of your GraphQL code. By using fragments, you can streamline your queries and mutations, ensuring a more efficient and organized approach to handling data.  ","version":"Next","tagName":"h2"},{"title":"Why Use GraphQL Fragments?​","type":1,"pageTitle":"Comprehensive Guide to GraphQL Fragments","url":"/graphql/graphql-fragments/#why-use-graphql-fragments","content":" ","version":"Next","tagName":"h2"},{"title":"Code Reusability and Maintainability​","type":1,"pageTitle":"Comprehensive Guide to GraphQL Fragments","url":"/graphql/graphql-fragments/#code-reusability-and-maintainability","content":" GraphQL fragments enable the reuse of common fields across different queries and mutations. This reduces the need for writing the same field selections repeatedly, making your code more DRY (Don't Repeat Yourself). Consequently, maintaining and updating your codebase becomes more manageable.  ","version":"Next","tagName":"h3"},{"title":"Improved Query Performance​","type":1,"pageTitle":"Comprehensive Guide to GraphQL Fragments","url":"/graphql/graphql-fragments/#improved-query-performance","content":" Using fragments can lead to optimized queries by minimizing the amount of bytes sent in the query. This can result in faster query responses and a more efficient use of network resources.  ","version":"Next","tagName":"h3"},{"title":"Consistent Data Fetching​","type":1,"pageTitle":"Comprehensive Guide to GraphQL Fragments","url":"/graphql/graphql-fragments/#consistent-data-fetching","content":" With fragments, you ensure that your application fetches consistent sets of fields across various parts of your UI. This consistency can reduce bugs and discrepancies in your data representation, leading to a more stable and reliable application.  ","version":"Next","tagName":"h3"},{"title":"How to Define and Use GraphQL Fragments​","type":1,"pageTitle":"Comprehensive Guide to GraphQL Fragments","url":"/graphql/graphql-fragments/#how-to-define-and-use-graphql-fragments","content":" ","version":"Next","tagName":"h2"},{"title":"Defining a Fragment​","type":1,"pageTitle":"Comprehensive Guide to GraphQL Fragments","url":"/graphql/graphql-fragments/#defining-a-fragment","content":" To define a fragment, use the fragment keyword followed by the fragment name and the type it belongs to. Here’s an example:  fragment UserDetails on User { id name email }   ","version":"Next","tagName":"h3"},{"title":"Using a Fragment in Queries​","type":1,"pageTitle":"Comprehensive Guide to GraphQL Fragments","url":"/graphql/graphql-fragments/#using-a-fragment-in-queries","content":" Once defined, you can include the fragment in any query that requires the same set of fields. This is done using the ...FragmentName syntax. Here’s how you can use the UserDetails fragment in a query:  query GetUser { user(id: &quot;1&quot;) { ...UserDetails } }   ","version":"Next","tagName":"h3"},{"title":"Combining Multiple Fragments​","type":1,"pageTitle":"Comprehensive Guide to GraphQL Fragments","url":"/graphql/graphql-fragments/#combining-multiple-fragments","content":" You can also combine multiple fragments in a single query to fetch different sets of fields. Here’s an example:  fragment PostDetails on Post { id title content } query GetUserWithPosts { user(id: &quot;1&quot;) { ...UserDetails posts { ...PostDetails } } }   ","version":"Next","tagName":"h3"},{"title":"Best Practices for Using GraphQL Fragments​","type":1,"pageTitle":"Comprehensive Guide to GraphQL Fragments","url":"/graphql/graphql-fragments/#best-practices-for-using-graphql-fragments","content":" ","version":"Next","tagName":"h2"},{"title":"Keep Fragments Small and Specific​","type":1,"pageTitle":"Comprehensive Guide to GraphQL Fragments","url":"/graphql/graphql-fragments/#keep-fragments-small-and-specific","content":" Avoid creating overly large fragments that include too many fields. Instead, create small, focused fragments that serve a specific purpose. This makes your fragments more reusable and easier to manage.  ","version":"Next","tagName":"h3"},{"title":"Use Descriptive Names​","type":1,"pageTitle":"Comprehensive Guide to GraphQL Fragments","url":"/graphql/graphql-fragments/#use-descriptive-names","content":" Name your fragments descriptively to indicate their purpose and the type of data they fetch. This enhances the readability of your code and makes it easier for other developers to understand the intent of each fragment.  ","version":"Next","tagName":"h3"},{"title":"Organize Fragments Logically​","type":1,"pageTitle":"Comprehensive Guide to GraphQL Fragments","url":"/graphql/graphql-fragments/#organize-fragments-logically","content":" Group related fragments together in your codebase to keep things organized. For instance, you might have a folder dedicated to user-related fragments and another for post-related fragments. This logical organization aids in the maintainability of your project.  ","version":"Next","tagName":"h3"},{"title":"Advanced Fragment Usage​","type":1,"pageTitle":"Comprehensive Guide to GraphQL Fragments","url":"/graphql/graphql-fragments/#advanced-fragment-usage","content":" ","version":"Next","tagName":"h2"},{"title":"Nested Fragments​","type":1,"pageTitle":"Comprehensive Guide to GraphQL Fragments","url":"/graphql/graphql-fragments/#nested-fragments","content":" Fragments can be nested within other fragments to build more complex data structures. Here’s an example:  fragment CommentDetails on Comment { id text author { ...UserDetails } } fragment PostWithComments on Post { id title content comments { ...CommentDetails } }   ","version":"Next","tagName":"h3"},{"title":"Fragments on Interfaces and Unions​","type":1,"pageTitle":"Comprehensive Guide to GraphQL Fragments","url":"/graphql/graphql-fragments/#fragments-on-interfaces-and-unions","content":" You can also define fragments on interfaces and unions to handle various data types. This is particularly useful when working with polymorphic data structures. Here’s an example:  fragment MediaFields on Media { ... on Photo { url height width } ... on Video { url duration } } query GetMedia { media { ...MediaFields } }   ","version":"Next","tagName":"h3"},{"title":"Example Diagram​","type":1,"pageTitle":"Comprehensive Guide to GraphQL Fragments","url":"/graphql/graphql-fragments/#example-diagram","content":" Below is an example diagram to illustrate the relationship between different fragments and their usage in queries:    ","version":"Next","tagName":"h2"},{"title":"Conclusion​","type":1,"pageTitle":"Comprehensive Guide to GraphQL Fragments","url":"/graphql/graphql-fragments/#conclusion","content":" GraphQL fragments are a powerful feature that enhances the reusability, maintainability, and performance of your GraphQL queries. By defining and using fragments effectively, you can create a more efficient and organized codebase, ensuring that your application fetches data consistently and optimally. Follow best practices such as keeping fragments small, using descriptive names, and organizing them logically to make the most out of this feature. ","version":"Next","tagName":"h2"},{"title":"Unlocking the Power of GraphQL Directives: A Comprehensive Guide","type":0,"sectionRef":"#","url":"/graphql/graphql-directives/","content":"","keywords":"","version":"Next"},{"title":"Introduction to GraphQL Directives​","type":1,"pageTitle":"Unlocking the Power of GraphQL Directives: A Comprehensive Guide","url":"/graphql/graphql-directives/#introduction-to-graphql-directives","content":" GraphQL directives are a vital component of the GraphQL specification. These powerful tools may be less familiar due to their limited mandatory usage in API compliance. However, their ability to extend the functionality of a GraphQL API and server is crucial, especially in advanced tools like Tailcall Graph Server.  This article will explore the nature of GraphQL directives, their applications, and provide examples of their usage.  ","version":"Next","tagName":"h2"},{"title":"What Are GraphQL Directives?​","type":1,"pageTitle":"Unlocking the Power of GraphQL Directives: A Comprehensive Guide","url":"/graphql/graphql-directives/#what-are-graphql-directives","content":" GraphQL directives serve as annotations within a GraphQL schema, indicating that the annotated element requires special evaluation. They enable modifications in runtime execution and type validation within a GraphQL document.  Directives allow the alteration of GraphQL execution behavior by providing options beyond those available through field arguments. For example, directives can conditionally include or exclude fields.  Both built-in and custom directives can be utilized when building or consuming a GraphQL API. Built-in directives are defined by the GraphQL specification, while custom directives are created by the GraphQL service or tool being used.  ","version":"Next","tagName":"h2"},{"title":"Built-in GraphQL Directives​","type":1,"pageTitle":"Unlocking the Power of GraphQL Directives: A Comprehensive Guide","url":"/graphql/graphql-directives/#built-in-graphql-directives","content":" The GraphQL specification includes several built-in directives with specific names and argument values of any input type. These directives can be applied to types, fields, fragments, and operations. Here are the built-in directives:  @skip: Conditionally excludes fields from a query operation.@include: Conditionally includes fields in a query operation, opposite of @skip.@deprecated: Marks a field or enum value as deprecated and provides a reason for deprecation.  As GraphQL evolves, new directives like @defer and @stream may be introduced by the GraphQL Working Group. Additionally, GraphQL services and tools can provide custom directives.  ","version":"Next","tagName":"h3"},{"title":"Custom GraphQL Directives​","type":1,"pageTitle":"Unlocking the Power of GraphQL Directives: A Comprehensive Guide","url":"/graphql/graphql-directives/#custom-graphql-directives","content":" Custom directives enhance GraphQL's functionality, allowing the addition of bespoke behaviors to a GraphQL API. Various GraphQL server and client implementations use custom directives to extend functionality.  For instance, Tailcall uses custom directives like @http, @grpc and, @graphql to connect with data sources. These directives enable interfacing with REST APIs, gRPC APIs, and other GraphQL APIs. Directives like @call help combine data from multiple sources into a single type.  ","version":"Next","tagName":"h3"},{"title":"Directive Locations in GraphQL​","type":1,"pageTitle":"Unlocking the Power of GraphQL Directives: A Comprehensive Guide","url":"/graphql/graphql-directives/#directive-locations-in-graphql","content":" Directives can be applied to different locations within GraphQL. The GraphQL specification differentiates between type system directive locations and executable directive locations. The location determines how a GraphQL implementation handles them.  For example, @include and @skip can be used in queries passed to the GraphQL server, affecting query processing based on an argument. Conversely, @deprecated is only added to a schema definition.  ","version":"Next","tagName":"h3"},{"title":"Type System Directives​","type":1,"pageTitle":"Unlocking the Power of GraphQL Directives: A Comprehensive Guide","url":"/graphql/graphql-directives/#type-system-directives","content":" Type system directives annotate a schema, object type, or field definition in GraphQL SDL (Schema Definition Language) when building a GraphQL server. Both built-in and custom directives can be used in type system directive locations, allowing GraphQL server implementations to take additional actions.  Type system directive locations, also known as &quot;schema directives,&quot; include:  SCHEMASCALAROBJECTFIELD_DEFINITIONARGUMENT_DEFINITIONINTERFACEUNIONENUM &amp; ENUM_VALUEINPUT_OBJECT &amp; INPUT_FIELD_DEFINITION  For example, the @deprecated directive marks a field as deprecated:  type User { id: ID! name: String! @deprecated( reason: &quot;Use the firstName and lastName fields instead&quot; ) firstName: String! lastName: String! email: String! }   The @deprecated directive provides a reason for deprecation, which is available to services that introspect the schema. Clients can then warn users about the deprecated field.  ","version":"Next","tagName":"h3"},{"title":"Example of @http Directive​","type":1,"pageTitle":"Unlocking the Power of GraphQL Directives: A Comprehensive Guide","url":"/graphql/graphql-directives/#example-of-http-directive","content":" The @http directive, a custom directive in Tailcall Graph Server, fetches data for the User type from a REST API:  type User { id: ID! name: String! email: String! } type Query { user(id: ID!): User @http( baseURL: &quot;https://jsonplaceholder.typicode.com&quot; path: &quot;/users/{{.args.id)}&quot; ) }   When executing an operation that includes the user field, the Tailcall GraphQL API fetches data from the REST API and returns it to the client. The @http directive, applied to a type system location, annotates the user field and defines how data should be fetched.  ","version":"Next","tagName":"h3"},{"title":"Execution Directives​","type":1,"pageTitle":"Unlocking the Power of GraphQL Directives: A Comprehensive Guide","url":"/graphql/graphql-directives/#execution-directives","content":" Execution directives modify the behavior of an operation, field, or fragment during runtime execution. They can include or exclude fields or perform additional data processing before returning a response.  Executable directive locations in GraphQL include:  QUERYMUTATIONSUBSCRIPTIONFIELDFRAGMENT_DEFINITION &amp; FRAGMENT_SPREADINLINE_FRAGMENTVARIABLE_DEFINITION  Both built-in and custom directives can be applied to executable locations. Most built-in directives are executable, such as @skip and @include, used to conditionally include or exclude fields in an operation.  ","version":"Next","tagName":"h3"},{"title":"Example of @include Directive​","type":1,"pageTitle":"Unlocking the Power of GraphQL Directives: A Comprehensive Guide","url":"/graphql/graphql-directives/#example-of-include-directive","content":" The @include directive conditionally includes fields in a query operation:  query me($showName: Boolean!) { me { id firstName @include(if: $showName) lastName @include(if: $showName) email } }   The @include directive conditionally includes the firstName and lastName fields in the response. The if argument specifies a boolean value determining whether to include the field. The if argument is set to the variable $showName, allowing for conditional inclusion based on its value.  ","version":"Next","tagName":"h3"},{"title":"Conclusion and Next Steps​","type":1,"pageTitle":"Unlocking the Power of GraphQL Directives: A Comprehensive Guide","url":"/graphql/graphql-directives/#conclusion-and-next-steps","content":" GraphQL directives, though initially complex, are a powerful tool within the GraphQL ecosystem. This article explained built-in and custom directives and their application across type system and executable locations in GraphQL. Type system directives apply to GraphQL SDL, while executable directives modify GraphQL responses during runtime execution.  Understanding directives is crucial when working with GraphQL APIs, whether using tools like Tailcall GraphQL server or manually implementing. ","version":"Next","tagName":"h2"},{"title":"Comprehensive Guide to GraphQL Introspection","type":0,"sectionRef":"#","url":"/graphql/graphql-introspection/","content":"","keywords":"","version":"Next"},{"title":"Introduction to GraphQL Introspection​","type":1,"pageTitle":"Comprehensive Guide to GraphQL Introspection","url":"/graphql/graphql-introspection/#introduction-to-graphql-introspection","content":" GraphQL introspection is a powerful feature that allows developers to query details about the GraphQL schema itself. This capability is essential for understanding the structure and capabilities of an API, enabling dynamic querying, generating documentation, and creating development tools. In this article, we delve deeply into the mechanics and benefits of GraphQL introspection, providing you with a thorough understanding of its uses and implementations.  ","version":"Next","tagName":"h2"},{"title":"Understanding GraphQL Introspection Queries​","type":1,"pageTitle":"Comprehensive Guide to GraphQL Introspection","url":"/graphql/graphql-introspection/#understanding-graphql-introspection-queries","content":" ","version":"Next","tagName":"h2"},{"title":"What Are Introspection Queries?​","type":1,"pageTitle":"Comprehensive Guide to GraphQL Introspection","url":"/graphql/graphql-introspection/#what-are-introspection-queries","content":" Introspection queries in GraphQL are specialized queries that allow clients to retrieve information about the schema, types, fields, and operations available in a GraphQL API. These queries can be used to dynamically understand what a server can do, which is particularly useful for tools like GraphiQL and Apollo Client.  ","version":"Next","tagName":"h3"},{"title":"Basic Introspection Query Example​","type":1,"pageTitle":"Comprehensive Guide to GraphQL Introspection","url":"/graphql/graphql-introspection/#basic-introspection-query-example","content":" Here is an example of a simple introspection query that retrieves the schema's types:  { __schema { types { name kind description } } }   ","version":"Next","tagName":"h3"},{"title":"Advanced Introspection Queries​","type":1,"pageTitle":"Comprehensive Guide to GraphQL Introspection","url":"/graphql/graphql-introspection/#advanced-introspection-queries","content":" For more detailed insights, advanced introspection queries can be used to fetch information about specific types, fields, and their arguments. An example is querying the fields of a particular type:  { __type(name: &quot;Query&quot;) { name fields { name description args { name type { name kind } defaultValue } type { name kind } } } }   ","version":"Next","tagName":"h3"},{"title":"Benefits of GraphQL Introspection​","type":1,"pageTitle":"Comprehensive Guide to GraphQL Introspection","url":"/graphql/graphql-introspection/#benefits-of-graphql-introspection","content":" ","version":"Next","tagName":"h2"},{"title":"Enhancing Developer Experience​","type":1,"pageTitle":"Comprehensive Guide to GraphQL Introspection","url":"/graphql/graphql-introspection/#enhancing-developer-experience","content":" GraphQL introspection greatly enhances the developer experience by providing a clear and comprehensive view of the API capabilities. This transparency reduces the learning curve and allows for more efficient development and debugging processes.  ","version":"Next","tagName":"h3"},{"title":"Dynamic Documentation Generation​","type":1,"pageTitle":"Comprehensive Guide to GraphQL Introspection","url":"/graphql/graphql-introspection/#dynamic-documentation-generation","content":" With introspection, tools can automatically generate up-to-date documentation. This dynamic documentation ensures that developers always have access to the latest API information without manual updates.  ","version":"Next","tagName":"h3"},{"title":"Enabling Smart Clients​","type":1,"pageTitle":"Comprehensive Guide to GraphQL Introspection","url":"/graphql/graphql-introspection/#enabling-smart-clients","content":" Introspection allows the creation of smart clients that can adapt to schema changes. This adaptability ensures that clients can continue to function correctly even when the server's schema evolves.  ","version":"Next","tagName":"h3"},{"title":"Implementing Introspection in GraphQL​","type":1,"pageTitle":"Comprehensive Guide to GraphQL Introspection","url":"/graphql/graphql-introspection/#implementing-introspection-in-graphql","content":" ","version":"Next","tagName":"h2"},{"title":"Enabling Introspection in Your GraphQL Server​","type":1,"pageTitle":"Comprehensive Guide to GraphQL Introspection","url":"/graphql/graphql-introspection/#enabling-introspection-in-your-graphql-server","content":" Most GraphQL servers have introspection enabled by default. However, it is crucial to ensure that this feature is properly configured and secured, especially in production environments. Here is an example of enabling introspection in Tailcall GraphQL server configuration:  schema @server(introspection: true) { query: Query mutation: Mutation }   ","version":"Next","tagName":"h3"},{"title":"Security Considerations​","type":1,"pageTitle":"Comprehensive Guide to GraphQL Introspection","url":"/graphql/graphql-introspection/#security-considerations","content":" While introspection is incredibly useful, it can also expose potentially sensitive information about your schema. It is advisable to disable introspection in production environments or to secure it through proper authentication and authorization mechanisms.  ","version":"Next","tagName":"h3"},{"title":"GraphQL Introspection and Tooling​","type":1,"pageTitle":"Comprehensive Guide to GraphQL Introspection","url":"/graphql/graphql-introspection/#graphql-introspection-and-tooling","content":" ","version":"Next","tagName":"h2"},{"title":"GraphiQL and Introspection​","type":1,"pageTitle":"Comprehensive Guide to GraphQL Introspection","url":"/graphql/graphql-introspection/#graphiql-and-introspection","content":" GraphiQL, an in-browser IDE for exploring GraphQL, heavily relies on introspection queries to provide a rich interface for querying and visualizing data. By utilizing introspection, GraphiQL can offer autocompletion, error highlighting, and real-time query feedback.  ","version":"Next","tagName":"h3"},{"title":"Apollo Client and Schema Awareness​","type":1,"pageTitle":"Comprehensive Guide to GraphQL Introspection","url":"/graphql/graphql-introspection/#apollo-client-and-schema-awareness","content":" Apollo Client leverages introspection to manage local state and remote data seamlessly. By understanding the schema, Apollo Client can perform efficient queries and handle schema changes gracefully.  ","version":"Next","tagName":"h3"},{"title":"GraphQL Introspection Workflow​","type":1,"pageTitle":"Comprehensive Guide to GraphQL Introspection","url":"/graphql/graphql-introspection/#graphql-introspection-workflow","content":"   ","version":"Next","tagName":"h2"},{"title":"Conclusion​","type":1,"pageTitle":"Comprehensive Guide to GraphQL Introspection","url":"/graphql/graphql-introspection/#conclusion","content":" GraphQL introspection is a cornerstone feature that significantly improves the development lifecycle by providing real-time insights into API schemas. It empowers developers with dynamic documentation, enhances development tools, and enables adaptive client applications. By leveraging introspection effectively, you can ensure a robust and flexible GraphQL implementation. ","version":"Next","tagName":"h2"},{"title":"GraphQL Mutations: Techniques and Best Practices","type":0,"sectionRef":"#","url":"/graphql/graphql-mutations/","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"GraphQL Mutations: Techniques and Best Practices","url":"/graphql/graphql-mutations/#introduction","content":" GraphQL has revolutionized the way we interact with APIs by offering a flexible and efficient approach to querying and manipulating data. Among its powerful features, mutations stand out as the key mechanism for creating, updating, and deleting data. In this article, we delve into the intricacies of GraphQL mutations, providing a detailed guide to mastering this essential component.  ","version":"Next","tagName":"h2"},{"title":"Understanding GraphQL Mutations​","type":1,"pageTitle":"GraphQL Mutations: Techniques and Best Practices","url":"/graphql/graphql-mutations/#understanding-graphql-mutations","content":" ","version":"Next","tagName":"h2"},{"title":"What are GraphQL Mutations?​","type":1,"pageTitle":"GraphQL Mutations: Techniques and Best Practices","url":"/graphql/graphql-mutations/#what-are-graphql-mutations","content":" GraphQL mutations are the counterpart to queries, designed specifically for writing data rather than reading it. While queries fetch data, mutations allow you to modify server-side data. Think of queries as a way to ask questions and get answers, while mutations are more like giving commands to change things.  ","version":"Next","tagName":"h3"},{"title":"Why Use Mutations?​","type":1,"pageTitle":"GraphQL Mutations: Techniques and Best Practices","url":"/graphql/graphql-mutations/#why-use-mutations","content":" Mutations are essential because they enable dynamic interactions with your data. Instead of just looking at information, you can add new entries, update existing ones, or even remove data that's no longer needed. This ability to change data makes your application more interactive and responsive.  ","version":"Next","tagName":"h3"},{"title":"Mutation Structure​","type":1,"pageTitle":"GraphQL Mutations: Techniques and Best Practices","url":"/graphql/graphql-mutations/#mutation-structure","content":" ","version":"Next","tagName":"h2"},{"title":"Mutation Type​","type":1,"pageTitle":"GraphQL Mutations: Techniques and Best Practices","url":"/graphql/graphql-mutations/#mutation-type","content":" A GraphQL mutation typically involves several key components. First, you have the Mutation Type, which defines the action to be performed, such as creating, updating, or deleting data.  ","version":"Next","tagName":"h3"},{"title":"Input Arguments​","type":1,"pageTitle":"GraphQL Mutations: Techniques and Best Practices","url":"/graphql/graphql-mutations/#input-arguments","content":" Next, you have Input Arguments, which specify the data required for the mutation. These are like the ingredients you need to perform the mutation.  ","version":"Next","tagName":"h3"},{"title":"Return Fields​","type":1,"pageTitle":"GraphQL Mutations: Techniques and Best Practices","url":"/graphql/graphql-mutations/#return-fields","content":" Finally, you have Return Fields, which indicate the data returned after the mutation is executed. This is what you get back after the mutation has done its job.  Here’s a basic example of a mutation to create a new user:  mutation { createUser( input: {name: &quot;John Doe&quot;, email: &quot;john.doe@example.com&quot;} ) { id name email } }   ","version":"Next","tagName":"h3"},{"title":"Defining Mutations in the Schema​","type":1,"pageTitle":"GraphQL Mutations: Techniques and Best Practices","url":"/graphql/graphql-mutations/#defining-mutations-in-the-schema","content":" To implement mutations, you must define them in your GraphQL schema. This involves specifying the mutation type and the fields it supports. Here’s an example schema definition:  type Mutation { createUser(input: CreateUserInput!): User updateUser(id: ID!, input: UpdateUserInput!): User deleteUser(id: ID!): User } input CreateUserInput { name: String! email: String! } input UpdateUserInput { name: String email: String }   ","version":"Next","tagName":"h2"},{"title":"Executing Mutations​","type":1,"pageTitle":"GraphQL Mutations: Techniques and Best Practices","url":"/graphql/graphql-mutations/#executing-mutations","content":" Executing mutations in GraphQL involves sending a mutation request with the necessary input data. The response typically includes the newly modified data, confirming the mutation's success.  ","version":"Next","tagName":"h2"},{"title":"Example: Creating a User​","type":1,"pageTitle":"GraphQL Mutations: Techniques and Best Practices","url":"/graphql/graphql-mutations/#example-creating-a-user","content":" Here's how you can create a new user named Jane Doe:  mutation { createUser( input: {name: &quot;Jane Doe&quot;, email: &quot;jane.doe@example.com&quot;} ) { id name email } }   The response might look like this:  { &quot;data&quot;: { &quot;createUser&quot;: { &quot;id&quot;: &quot;1&quot;, &quot;name&quot;: &quot;Jane Doe&quot;, &quot;email&quot;: &quot;jane.doe@example.com&quot; } } }   ","version":"Next","tagName":"h3"},{"title":"Handling Errors in Mutations​","type":1,"pageTitle":"GraphQL Mutations: Techniques and Best Practices","url":"/graphql/graphql-mutations/#handling-errors-in-mutations","content":" Error handling is crucial for robust GraphQL APIs. Mutations should provide meaningful error messages and handle various scenarios gracefully.  ","version":"Next","tagName":"h2"},{"title":"Example: Handling Validation Errors​","type":1,"pageTitle":"GraphQL Mutations: Techniques and Best Practices","url":"/graphql/graphql-mutations/#example-handling-validation-errors","content":" If you try to create a user without a name or with an invalid email, you'll get a validation error:  mutation { createUser(input: {name: &quot;&quot;, email: &quot;invalid-email&quot;}) { id name email } }   The response will be:  { &quot;errors&quot;: [ { &quot;message&quot;: &quot;Validation error: Name is required, Email is invalid&quot;, &quot;locations&quot;: [ { &quot;line&quot;: 2, &quot;column&quot;: 3 } ], &quot;path&quot;: [&quot;createUser&quot;] } ], &quot;data&quot;: { &quot;createUser&quot;: null } }   ","version":"Next","tagName":"h3"},{"title":"Advanced Mutation Techniques​","type":1,"pageTitle":"GraphQL Mutations: Techniques and Best Practices","url":"/graphql/graphql-mutations/#advanced-mutation-techniques","content":" ","version":"Next","tagName":"h2"},{"title":"Nested Mutations​","type":1,"pageTitle":"GraphQL Mutations: Techniques and Best Practices","url":"/graphql/graphql-mutations/#nested-mutations","content":" Nested mutations allow you to perform multiple related operations in a single mutation. This can be particularly useful for complex data relationships. For example, you can create a user and their associated posts in one go:  mutation { createUser( input: { name: &quot;Alice&quot; email: &quot;alice@example.com&quot; posts: [{title: &quot;First Post&quot;}, {title: &quot;Second Post&quot;}] } ) { id name email posts { id title } } }   caution Performing Nested Mutations is possible but caution is advised as by default, GraphQL mutations are not transactional. This means that if one part of the mutation fails, the other parts will still be executed. You may need to implement custom logic to handle this.  ","version":"Next","tagName":"h3"},{"title":"Optimizing Mutations for Performance​","type":1,"pageTitle":"GraphQL Mutations: Techniques and Best Practices","url":"/graphql/graphql-mutations/#optimizing-mutations-for-performance","content":" Efficiently designed mutations are essential for maintaining performance and scalability in your GraphQL API. Consider the following techniques:  ","version":"Next","tagName":"h2"},{"title":"Optimistic UI Updates​","type":1,"pageTitle":"GraphQL Mutations: Techniques and Best Practices","url":"/graphql/graphql-mutations/#optimistic-ui-updates","content":" Enhance user experience by updating the UI optimistically before the mutation response is received. This makes the app feel faster and more responsive.  ","version":"Next","tagName":"h3"},{"title":"Input Validation​","type":1,"pageTitle":"GraphQL Mutations: Techniques and Best Practices","url":"/graphql/graphql-mutations/#input-validation","content":" Perform thorough validation on the client-side to minimize server-side processing. This helps ensure that only valid data reaches your server, reducing the risk of errors and improving performance.  ","version":"Next","tagName":"h3"},{"title":"Example Diagram: Mutation Lifecycle​","type":1,"pageTitle":"GraphQL Mutations: Techniques and Best Practices","url":"/graphql/graphql-mutations/#example-diagram-mutation-lifecycle","content":" Here’s a simple diagram to illustrate the lifecycle of a mutation:    ","version":"Next","tagName":"h2"},{"title":"Conclusion​","type":1,"pageTitle":"GraphQL Mutations: Techniques and Best Practices","url":"/graphql/graphql-mutations/#conclusion","content":" Mastering GraphQL mutations is fundamental for any developer working with GraphQL APIs. By understanding their structure, implementing them effectively, and optimizing for performance, you can leverage the full potential of GraphQL for dynamic and efficient data manipulation.    ","version":"Next","tagName":"h2"},{"title":"FAQs​","type":1,"pageTitle":"GraphQL Mutations: Techniques and Best Practices","url":"/graphql/graphql-mutations/#faqs","content":" ","version":"Next","tagName":"h2"},{"title":"What is a GraphQL mutation?​","type":1,"pageTitle":"GraphQL Mutations: Techniques and Best Practices","url":"/graphql/graphql-mutations/#what-is-a-graphql-mutation","content":" A GraphQL mutation is an operation that allows clients to modify server-side data, including creating, updating, and deleting records.  ","version":"Next","tagName":"h3"},{"title":"How do I handle errors in GraphQL mutations?​","type":1,"pageTitle":"GraphQL Mutations: Techniques and Best Practices","url":"/graphql/graphql-mutations/#how-do-i-handle-errors-in-graphql-mutations","content":" GraphQL responses include both data and errors. Clients can handle partial successes by checking the presence of errors in the response and taking appropriate actions.  ","version":"Next","tagName":"h3"},{"title":"What is the difference between queries and mutations in GraphQL?​","type":1,"pageTitle":"GraphQL Mutations: Techniques and Best Practices","url":"/graphql/graphql-mutations/#what-is-the-difference-between-queries-and-mutations-in-graphql","content":" Queries are used to fetch data, while mutations are used to modify data. Queries are typically idempotent, while mutations change the state of the server.  ","version":"Next","tagName":"h3"},{"title":"How do I secure GraphQL mutations?​","type":1,"pageTitle":"GraphQL Mutations: Techniques and Best Practices","url":"/graphql/graphql-mutations/#how-do-i-secure-graphql-mutations","content":" Secure GraphQL mutations by implementing authentication to verify user identity and authorization to ensure users have the correct permissions to perform the mutation. ","version":"Next","tagName":"h3"},{"title":"Mastering GraphQL Queries: Comprehensive Guide","type":0,"sectionRef":"#","url":"/graphql/graphql-queries/","content":"","keywords":"","version":"Next"},{"title":"Introduction to GraphQL Queries​","type":1,"pageTitle":"Mastering GraphQL Queries: Comprehensive Guide","url":"/graphql/graphql-queries/#introduction-to-graphql-queries","content":" In GraphQL, queries are the primary method to fetch data from a server. A GraphQL query allows you to specify exactly what data you need, making data retrieval both precise and efficient.  ","version":"Next","tagName":"h2"},{"title":"What is a GraphQL Query?​","type":1,"pageTitle":"Mastering GraphQL Queries: Comprehensive Guide","url":"/graphql/graphql-queries/#what-is-a-graphql-query","content":" A GraphQL query is a read operation that allows clients to specify precisely which data they need from the server. Unlike traditional REST APIs, where endpoints define the structure of responses, GraphQL queries let clients dictate the shape and size of the response. This flexibility reduces over-fetching and under-fetching of data, optimizing both server and client performance.  ","version":"Next","tagName":"h2"},{"title":"Basic Structure of a GraphQL Query​","type":1,"pageTitle":"Mastering GraphQL Queries: Comprehensive Guide","url":"/graphql/graphql-queries/#basic-structure-of-a-graphql-query","content":" GraphQL queries are written in a declarative syntax, resembling the structure of the requested data. Here is an example of a simple query to fetch user information:  { user(id: &quot;1&quot;) { id name email } }   ","version":"Next","tagName":"h2"},{"title":"Components of a Query​","type":1,"pageTitle":"Mastering GraphQL Queries: Comprehensive Guide","url":"/graphql/graphql-queries/#components-of-a-query","content":" Field: The basic unit of a query. In the example, id, name, and email are fields.Arguments: Parameters passed to fields to specify or filter data. id: &quot;1&quot; is an argument to the user field.Aliases: Alternative names for fields to avoid conflicts and improve readability.  ","version":"Next","tagName":"h3"},{"title":"Advanced Query Features​","type":1,"pageTitle":"Mastering GraphQL Queries: Comprehensive Guide","url":"/graphql/graphql-queries/#advanced-query-features","content":" ","version":"Next","tagName":"h2"},{"title":"Nested Queries​","type":1,"pageTitle":"Mastering GraphQL Queries: Comprehensive Guide","url":"/graphql/graphql-queries/#nested-queries","content":" GraphQL queries support nesting, allowing clients to request related data in a single query. This feature is particularly useful for fetching hierarchical data structures.  { user(id: &quot;1&quot;) { id name posts { title content } } }   ","version":"Next","tagName":"h3"},{"title":"Fragments​","type":1,"pageTitle":"Mastering GraphQL Queries: Comprehensive Guide","url":"/graphql/graphql-queries/#fragments","content":" Fragments allow the reuse of common field selections across multiple queries, mutations, or subscriptions. They help in maintaining a DRY (Don't Repeat Yourself) approach in GraphQL queries. We will cover fragments in detail in the GraphQL Fragments section.  fragment userFields on User { id name email } { user(id: &quot;1&quot;) { ...userFields } }   ","version":"Next","tagName":"h3"},{"title":"Variables​","type":1,"pageTitle":"Mastering GraphQL Queries: Comprehensive Guide","url":"/graphql/graphql-queries/#variables","content":" Variables enable dynamic queries, where the arguments can be passed externally, making the queries more flexible and reusable.  query getUser($userId: ID!) { user(id: $userId) { id name email } }   { &quot;userId&quot;: &quot;1&quot; }   ","version":"Next","tagName":"h3"},{"title":"Directives​","type":1,"pageTitle":"Mastering GraphQL Queries: Comprehensive Guide","url":"/graphql/graphql-queries/#directives","content":" Directives are used to modify the behavior of queries at runtime. Common directives include @include and @skip for conditional field inclusion.  { user(id: &quot;1&quot;) { id name email @include(if: $includeEmail) } }   ","version":"Next","tagName":"h3"},{"title":"Error Handling in Queries​","type":1,"pageTitle":"Mastering GraphQL Queries: Comprehensive Guide","url":"/graphql/graphql-queries/#error-handling-in-queries","content":" GraphQL provides a standardized way to handle errors. The response includes both data and errors, allowing clients to handle partial success scenarios gracefully.  { &quot;data&quot;: { &quot;user&quot;: null }, &quot;errors&quot;: [ { &quot;message&quot;: &quot;User not found&quot;, &quot;locations&quot;: [ { &quot;line&quot;: 2, &quot;column&quot;: 3 } ], &quot;path&quot;: [&quot;user&quot;] } ] }   ","version":"Next","tagName":"h2"},{"title":"Best Practices for Writing GraphQL Queries​","type":1,"pageTitle":"Mastering GraphQL Queries: Comprehensive Guide","url":"/graphql/graphql-queries/#best-practices-for-writing-graphql-queries","content":" Fetch Only Necessary Data: Always request the minimum required fields to reduce the payload and improve performance.Use Aliases and Fragments: To avoid naming conflicts and promote reuse of common field selections.Implement Pagination: For queries that return large lists, use pagination techniques like first, last, before, and after.Handle Errors Gracefully: Ensure your client can handle partial successes and provide useful feedback to users.  ","version":"Next","tagName":"h2"},{"title":"Example Diagram: GraphQL Query Structure​","type":1,"pageTitle":"Mastering GraphQL Queries: Comprehensive Guide","url":"/graphql/graphql-queries/#example-diagram-graphql-query-structure","content":"   ","version":"Next","tagName":"h2"},{"title":"Conclusion​","type":1,"pageTitle":"Mastering GraphQL Queries: Comprehensive Guide","url":"/graphql/graphql-queries/#conclusion","content":" You now have the skills to write simple and nested GraphQL queries, pass arguments, use variables for dynamic queries, paginate results, and filter queries. Mastering these concepts will enable you to fetch data efficiently and effectively using GraphQL.    ","version":"Next","tagName":"h2"},{"title":"Frequently Asked Questions (FAQs)​","type":1,"pageTitle":"Mastering GraphQL Queries: Comprehensive Guide","url":"/graphql/graphql-queries/#frequently-asked-questions-faqs","content":" ","version":"Next","tagName":"h2"},{"title":"How do I handle errors in GraphQL queries?​","type":1,"pageTitle":"Mastering GraphQL Queries: Comprehensive Guide","url":"/graphql/graphql-queries/#how-do-i-handle-errors-in-graphql-queries","content":" GraphQL responses include both data and errors. Clients can handle partial successes by checking the presence of errors in the response and taking appropriate actions.  ","version":"Next","tagName":"h3"},{"title":"What are GraphQL fragments?​","type":1,"pageTitle":"Mastering GraphQL Queries: Comprehensive Guide","url":"/graphql/graphql-queries/#what-are-graphql-fragments","content":" Fragments are reusable units of query logic that help maintain a DRY approach in your GraphQL queries. They allow you to define common field selections and use them across multiple queries, mutations, or subscriptions.  ","version":"Next","tagName":"h3"},{"title":"Can I use GraphQL with existing REST APIs?​","type":1,"pageTitle":"Mastering GraphQL Queries: Comprehensive Guide","url":"/graphql/graphql-queries/#can-i-use-graphql-with-existing-rest-apis","content":" Yes, you can use GraphQL as a layer on top of existing REST APIs to provide a more flexible and efficient way to query your data. For quickly creating a GraphQL server that converts REST APIs to GraphQL, check out Getting Started with Tailcall.  ","version":"Next","tagName":"h3"},{"title":"What are GraphQL directives?​","type":1,"pageTitle":"Mastering GraphQL Queries: Comprehensive Guide","url":"/graphql/graphql-queries/#what-are-graphql-directives","content":" Directives are used to modify the behavior of queries at runtime. Common directives like @include and @skip allow you to conditionally include or exclude fields from the query based on dynamic conditions.  ","version":"Next","tagName":"h3"},{"title":"How does GraphQL handle nested queries?​","type":1,"pageTitle":"Mastering GraphQL Queries: Comprehensive Guide","url":"/graphql/graphql-queries/#how-does-graphql-handle-nested-queries","content":" GraphQL allows you to fetch related data in a single request using nested queries. This is particularly useful for hierarchical data structures where you need to retrieve parent and child data together.  ","version":"Next","tagName":"h3"},{"title":"What is GraphiQL?​","type":1,"pageTitle":"Mastering GraphQL Queries: Comprehensive Guide","url":"/graphql/graphql-queries/#what-is-graphiql","content":" GraphiQL is an open-source in-browser IDE for exploring GraphQL APIs. You can use GraphiQL to interact with GraphQL servers and visualize query results.  ","version":"Next","tagName":"h3"},{"title":"What is the benefit of using aliases in GraphQL?​","type":1,"pageTitle":"Mastering GraphQL Queries: Comprehensive Guide","url":"/graphql/graphql-queries/#what-is-the-benefit-of-using-aliases-in-graphql","content":" Aliases allow you to rename fields in the response, avoiding conflicts and improving readability. This is useful when querying the same field multiple times with different arguments. ","version":"Next","tagName":"h3"},{"title":"Using GraphQL Variables for Type-Safe Queries","type":0,"sectionRef":"#","url":"/graphql/graphql-variables/","content":"","keywords":"","version":"Next"},{"title":"What Are GraphQL Variables?​","type":1,"pageTitle":"Using GraphQL Variables for Type-Safe Queries","url":"/graphql/graphql-variables/#what-are-graphql-variables","content":" GraphQL variables are similar to variables in any other programming language. They store values that you can access using their names. These variables are used to pass data from your application to your GraphQL queries and mutations. For instance, take a look at this example where a GraphQL query uses a variable to fetch data:  query GetUserByName($name: String!) { user(name: $name) { name email age } }   In this query, the $name variable in the query helps find a user by its name. You can easily spot GraphQL variables because they always start with a dollar sign ($). Here’s how a response might look:  { &quot;data&quot;: { &quot;user&quot;: { &quot;name&quot;: &quot;John&quot;, &quot;email&quot;: &quot;john@example.com&quot;, &quot;age&quot;: 10 } } }   ","version":"Next","tagName":"h2"},{"title":"Defining and Using Variables in GraphQL Queries​","type":1,"pageTitle":"Using GraphQL Variables for Type-Safe Queries","url":"/graphql/graphql-variables/#defining-and-using-variables-in-graphql-queries","content":" GraphQL variables are defined separately from the query string itself. When you run the query, these variables are inserted into it, and the API responds with matching results. Here’s how you define a query and its variables in a JSON object for HTTP requests:  { &quot;query&quot;: &quot;query GetUserByName($name: String!) { user(name: $name) { name email age } }&quot;, &quot;variables&quot;: { &quot;name&quot;: &quot;John&quot; } }   Separating the query from the variables makes it easy to write reusable queries. When making requests, you send the query and variables as separate objects.  ","version":"Next","tagName":"h2"},{"title":"Default Values for GraphQL Variables​","type":1,"pageTitle":"Using GraphQL Variables for Type-Safe Queries","url":"/graphql/graphql-variables/#default-values-for-graphql-variables","content":" You can set default values for variables, which allows queries to run even without input. Here’s how you do it:  query GetUserByName($name: String = &quot;Jack&quot;) { user(name: $name) { name email age } }   In this example, if no value is provided for $name, the query uses &quot;Jack&quot; as the default.  ","version":"Next","tagName":"h2"},{"title":"Using Variables in GraphQL Mutations​","type":1,"pageTitle":"Using GraphQL Variables for Type-Safe Queries","url":"/graphql/graphql-variables/#using-variables-in-graphql-mutations","content":" GraphQL mutations can create, update, or delete data on the server. Variables work the same way in mutations as they do in queries.  ","version":"Next","tagName":"h2"},{"title":"Example:​","type":1,"pageTitle":"Using GraphQL Variables for Type-Safe Queries","url":"/graphql/graphql-variables/#example","content":" mutation UpdateUserName($id: ID!, $new_name: String!) { updateUserName(id: $id, name: $new_name) { id name email age } }   ","version":"Next","tagName":"h3"},{"title":"Passing Variables:​","type":1,"pageTitle":"Using GraphQL Variables for Type-Safe Queries","url":"/graphql/graphql-variables/#passing-variables","content":" { &quot;id&quot;: &quot;1&quot;, &quot;new_name&quot;: &quot;Johnny&quot; }   ","version":"Next","tagName":"h3"},{"title":"Using JavaScript to Make a GraphQL Request:​","type":1,"pageTitle":"Using GraphQL Variables for Type-Safe Queries","url":"/graphql/graphql-variables/#using-javascript-to-make-a-graphql-request","content":" const query = ` query GetUserByName($name: String!) { user(name: $name) { name email age } } ` const variables = {name: &quot;John&quot;} fetch(&quot;https://YOUR_GRAPHQL_SERVER_URL&quot;, { method: &quot;POST&quot;, headers: { &quot;Content-Type&quot;: &quot;application/json&quot;, }, body: JSON.stringify({query, variables}), }) .then((response) =&gt; response.json()) .then((data) =&gt; console.log(data))   Replace YOUR_GRAPHQL_SERVER_URL with your GraphQL Server url.  ","version":"Next","tagName":"h3"},{"title":"GraphQL Variable Types and Type Safety​","type":1,"pageTitle":"Using GraphQL Variables for Type-Safe Queries","url":"/graphql/graphql-variables/#graphql-variable-types-and-type-safety","content":" GraphQL variables come with types to ensure type safety. For example, String! indicates that the variable must be a string GraphQL type and is required. If you pass a different type, an error occurs, and the query won’t run. This type safety prevents unexpected inputs and results, ensuring your application runs smoothly.  GraphQL has several built-in types: String, Int, Float, Boolean, and ID. These types form the foundation for input object types. You can also define custom input types to model your data.  ","version":"Next","tagName":"h2"},{"title":"Building Apps with GraphQL​","type":1,"pageTitle":"Using GraphQL Variables for Type-Safe Queries","url":"/graphql/graphql-variables/#building-apps-with-graphql","content":" TailCall’s CLI tool can generate GraphQL configurations from various sources, such as protobuf files and REST endpoints. This tool simplifies the process of creating GraphQL configurations, enabling you to build powerful applications with ease. To know more about the gen command in the TailCall CLI, check out the documentation.  ","version":"Next","tagName":"h2"},{"title":"Conclusion​","type":1,"pageTitle":"Using GraphQL Variables for Type-Safe Queries","url":"/graphql/graphql-variables/#conclusion","content":" GraphQL variables and type safety ensure consistent data across your applications. By leveraging these features, you can build flexible, reliable, and scalable applications. TailCall’s CLI tool simplifies the process of generating GraphQL configurations, enabling you to create powerful applications with ease. To learn more about building apps with GraphQL, explore the TailCall documentation. ","version":"Next","tagName":"h2"},{"title":"Is GraphQL Better Than REST? GraphQL vs REST Comparison","type":0,"sectionRef":"#","url":"/graphql/graphql-vs-rest-api-comparison/","content":"","keywords":"","version":"Next"},{"title":"What is REST?​","type":1,"pageTitle":"Is GraphQL Better Than REST? GraphQL vs REST Comparison","url":"/graphql/graphql-vs-rest-api-comparison/#what-is-rest","content":" REST (Representational State Transfer) is an architectural style for designing networked applications. It uses HTTP requests to perform CRUD (Create, Read, Update, Delete) operations. Each resource in a RESTful system is identified by a unique URI and can be manipulated using standard HTTP methods: GET, POST, PUT, DELETE. It is designed to utilize the stateless operations of the HTTP protocol for web communications. REST has been widely adopted due to its simplicity and effectiveness in designing scalable web services. Detailed Example: Consider a typical social media platform:  GET /users // Retrieves a list of users POST /users // Creates a new user GET /users/123 // Retrieves details about user 123 GET /users/123/posts // Retrieves posts by user 123 GET /users/123/followers // Retrieves followers of user 123 GET /posts // Retrieves posts POST /posts // Creates a post GET /posts/123/comments // Retrieves comments for post 123   REST API Design  Each endpoint is designed to handle a specific type of resource and HTTP method (GET, POST, PUT, DELETE). This design allows clients to interact with the server in a predictable manner.  ","version":"Next","tagName":"h2"},{"title":"Key Characteristics of REST:​","type":1,"pageTitle":"Is GraphQL Better Than REST? GraphQL vs REST Comparison","url":"/graphql/graphql-vs-rest-api-comparison/#key-characteristics-of-rest","content":" Stateless Operations: Each API request from a client to server must contain all the information needed to understand and process the request. The server side should not store any client context between requests, which is beneficial for scalability and reliability.Cacheable: Responses must explicitly define themselves as cacheable or non cacheable. If a response is defined as cacheable, the client can reuse the response for similar requests in future. Server also defines when to invalidate the cached response for a particular request to prevent clients from reusing stale or inappropriate data. A good caching practice can significantly reduce the number of requests to the server.Layered System: A client cannot ordinarily tell whether it is connected directly to the end server or an intermediary along the way. A REST API can consist of multiple layers of servers, each with its own specific functionality. For example, an E-Commerce application might have a server for authentication, another for product information, and a third for payment processing. A client can interact with a layer without knowing the details of other layers.  ","version":"Next","tagName":"h3"},{"title":"Advantages of REST​","type":1,"pageTitle":"Is GraphQL Better Than REST? GraphQL vs REST Comparison","url":"/graphql/graphql-vs-rest-api-comparison/#advantages-of-rest","content":" Simplicity and Maturity: REST has been widely used and is supported by most platforms and programming languages (Frontend and Backend), making it a mature choice for building APIs. Its principles are well-understood, and there is a wealth of documentation and tooling available.Scalability: Due to its stateless nature and ability to handle requests independently, REST can scale effectively. Each request is treated as an independent transaction, allowing for easy distribution across multiple servers.Caching: REST can efficiently leverage web infrastructure for caching requests, reducing the load on the backend and improving performance. HTTP caching mechanisms, such as ETags and cache-control headers, can be used to cache responses and minimize redundant data transfers.  ","version":"Next","tagName":"h3"},{"title":"What is GraphQL?​","type":1,"pageTitle":"Is GraphQL Better Than REST? GraphQL vs REST Comparison","url":"/graphql/graphql-vs-rest-api-comparison/#what-is-graphql","content":" GraphQL is a query language for APIs and a runtime for executing those queries. It allows clients to request exactly the data they need, avoiding over-fetching and under-fetching issues common with REST. Developed by Facebook in 2012 and open-sourced in 2015, GraphQL provides a more flexible and efficient approach to API design.  ","version":"Next","tagName":"h2"},{"title":"Key Characteristics of GraphQL:​","type":1,"pageTitle":"Is GraphQL Better Than REST? GraphQL vs REST Comparison","url":"/graphql/graphql-vs-rest-api-comparison/#key-characteristics-of-graphql","content":" Strongly Typed: GraphQL APIs are defined by a schema using the GraphQL Schema Definition Language (SDL).Single Endpoint: GraphQL APIs have a single endpoint, making it simpler to manage than multiple REST endpoints. This single endpoint can handle a wide variety of queries and mutations, streamlining the client-server interaction.Declarative Data Fetching: Clients can specify exactly what data they need, even to the level of specifying individual fields in a single query. This allows for highly efficient data fetching and reduces the likelihood of over-fetching or under-fetching. Example Query:  { user(id: &quot;1&quot;) { name email posts { title comments { content author { name } } } } }   In this example, the client requests specific fields (name, email, posts, comments) in a single query, avoiding unnecessary data transfers.  GraphQL Efficiency: This image shows how clients specify data needs, ensuring the server response matches the query structure precisely.  ","version":"Next","tagName":"h3"},{"title":"Advantages of GraphQL​","type":1,"pageTitle":"Is GraphQL Better Than REST? GraphQL vs REST Comparison","url":"/graphql/graphql-vs-rest-api-comparison/#advantages-of-graphql","content":" Efficient Data Loading: Reduces over-fetching and under-fetching, leading to more efficient data transfers. Clients can request exactly the data they need, minimizing network traffic and improving performance.Strongly Typed: Every GraphQL service defines a set of types which completely describe the set of possible data you can query on that service. This strong typing helps prevent errors and improves the development experience.Rapid Development: GraphQL's declarative nature and strong type system make it easier to develop and maintain APIs. The self-documenting nature of GraphQL schemas also aids in understanding the API structure.  ","version":"Next","tagName":"h2"},{"title":"Limitations of GraphQL​","type":1,"pageTitle":"Is GraphQL Better Than REST? GraphQL vs REST Comparison","url":"/graphql/graphql-vs-rest-api-comparison/#limitations-of-graphql","content":" Complexity in Client: The flexibility of GraphQL requires clients to handle more complexity in data handling and state management. Clients must be capable of constructing complex queries and managing the resulting data structures.Caching: Unlike REST, which can leverage HTTP caching mechanisms, caching a GraphQL API can be more involved because each query can be unique. This requires more sophisticated caching strategies, such as field-level caching or custom cache management solutions.  ","version":"Next","tagName":"h2"},{"title":"Similarities Between GraphQL and REST​","type":1,"pageTitle":"Is GraphQL Better Than REST? GraphQL vs REST Comparison","url":"/graphql/graphql-vs-rest-api-comparison/#similarities-between-graphql-and-rest","content":" Both GraphQL and REST facilitate data exchange between client and server in a client-server model, using HTTP as the underlying communication protocol. Here are some similarities:  Resource-Based Design: Both treat data as resources with unique identifiers. In REST, these are represented by URIs, while in GraphQL, they are defined in the schema and identified by the entities.Stateless: Both are stateless architectures, where each request is independent.Support for JSON: Both can use JSON for data format, although REST can also support XML and other formats.  ","version":"Next","tagName":"h2"},{"title":"Key Differences Between GraphQL and REST​","type":1,"pageTitle":"Is GraphQL Better Than REST? GraphQL vs REST Comparison","url":"/graphql/graphql-vs-rest-api-comparison/#key-differences-between-graphql-and-rest","content":" ","version":"Next","tagName":"h2"},{"title":"Data Fetching​","type":1,"pageTitle":"Is GraphQL Better Than REST? GraphQL vs REST Comparison","url":"/graphql/graphql-vs-rest-api-comparison/#data-fetching","content":" REST: Multiple endpoints can lead to inefficiencies when fetching related data across several resources. For example, fetching a user and their posts might require multiple requests. GraphQL: A single query can retrieve all related data, significantly reducing the need for multiple network requests. This leads to more efficient data fetching and improved performance.  Example: In REST, to fetch a user and their posts, you might need:  GET /users/123 GET /users/123/posts   In GraphQL, you can fetch the same data with a single query:  { user(id: &quot;123&quot;) { name email posts { title } } }   ","version":"Next","tagName":"h3"},{"title":"Performance and Flexibility​","type":1,"pageTitle":"Is GraphQL Better Than REST? GraphQL vs REST Comparison","url":"/graphql/graphql-vs-rest-api-comparison/#performance-and-flexibility","content":" REST: While REST APIs are generally easy to cache and scale, they can suffer from latency issues in complex systems due to multiple round trips. Each endpoint is designed to serve a specific resource, which can lead to inefficient data fetching.GraphQL: By reducing the number of requests and allowing for precise data retrieval, GraphQL can offer performance benefits, especially in systems with complex data relationships. It enables clients to specify their exact data needs, reducing the likelihood of redundant data transfers. Code Snippet Example (GraphQL):  { user(id: &quot;1&quot;) { name email posts { title comments { content author { name } } } } }   ","version":"Next","tagName":"h3"},{"title":"Error Handling​","type":1,"pageTitle":"Is GraphQL Better Than REST? GraphQL vs REST Comparison","url":"/graphql/graphql-vs-rest-api-comparison/#error-handling","content":" REST: Error handling needs to be implemented by developers. Responses typically include http status codes and error messages, but the structure can vary across different APIs.GraphQL: Inbuilt error handling and detailed error messages due to its strong type system. Errors are returned alongside the data, making it easier for clients to understand and handle exceptions.  ","version":"Next","tagName":"h3"},{"title":"Versioning​","type":1,"pageTitle":"Is GraphQL Better Than REST? GraphQL vs REST Comparison","url":"/graphql/graphql-vs-rest-api-comparison/#versioning","content":" REST: Often uses versioned endpoints to handle changes, which can be cumbersome.GraphQL: No need for versioning; deprecated fields are marked and can be handled gracefully.  ","version":"Next","tagName":"h3"},{"title":"When to Use GraphQL vs. REST​","type":1,"pageTitle":"Is GraphQL Better Than REST? GraphQL vs REST Comparison","url":"/graphql/graphql-vs-rest-api-comparison/#when-to-use-graphql-vs-rest","content":" ","version":"Next","tagName":"h2"},{"title":"Use GraphQL if:​","type":1,"pageTitle":"Is GraphQL Better Than REST? GraphQL vs REST Comparison","url":"/graphql/graphql-vs-rest-api-comparison/#use-graphql-if","content":" You need to reduce the number of API calls without creating new REST APIs for different data compositions.You have various frontend clients with different data requirements.Your application has a rich user interface that changes frequently.You want to expose your API to third-party developers for building external applications.  ","version":"Next","tagName":"h3"},{"title":"Use REST if:​","type":1,"pageTitle":"Is GraphQL Better Than REST? GraphQL vs REST Comparison","url":"/graphql/graphql-vs-rest-api-comparison/#use-rest-if","content":" You are building simple APIs with well-defined endpoints.Your application has low complexity and data interrelations.You prefer the simplicity and familiarity of REST.  ","version":"Next","tagName":"h3"},{"title":"Summary of Differences: REST vs GraphQL​","type":1,"pageTitle":"Is GraphQL Better Than REST? GraphQL vs REST Comparison","url":"/graphql/graphql-vs-rest-api-comparison/#summary-of-differences-rest-vs-graphql","content":" Aspect\tREST\tGraphQLEndpoint Structure\tMultiple endpoints for different resources\tSingle endpoint for all operations Data Fetching\tFixed data structure, may lead to overfetching or underfetching\tClients specify exact data needed, avoiding overfetching Performance\tPotentially slower for complex queries due to multiple requests\tMore efficient for complex queries, reduces multiple requests Flexibility\tLess flexible, relies on predefined endpoints\tHigh flexibility in querying data Data Types\tWeakly typed, depends on documentation for consistency\tStrongly typed schema, ensures consistency Real-Time Data\tRequires additional mechanisms for real-time updates\tSupports subscriptions for real-time updates Caching\tLeverages HTTP caching mechanisms (e.g., ETags, cache-control headers)\tMore complex, requires custom caching strategies Versioning\tOften requires versioned endpoints to manage changes\tNo versioning needed, handles changes through schema updates Error Handling\tRequires custom error handling in the API implementation\tBuilt-in error handling with detailed messages Development Speed\tSlower for complex queries, requires managing multiple endpoints\tFaster development for complex queries and rapid iterations Learning Curve\tEasier to learn and implement, well-documented and supported\tSteeper learning curve due to its flexibility and complexity Tooling and Ecosystem\tMature ecosystem with extensive tooling and documentation\tGrowing ecosystem with strong community support Use Cases\tBest for simple, well-defined data structures and public APIs\tIdeal for complex, interrelated data, and real-time applications Scalability\tInherently scalable due to statelessness and simplicity\tRequires careful management of queries and resolvers  ","version":"Next","tagName":"h2"},{"title":"GraphQL complements REST​","type":1,"pageTitle":"Is GraphQL Better Than REST? GraphQL vs REST Comparison","url":"/graphql/graphql-vs-rest-api-comparison/#graphql-complements-rest","content":" While GraphQL offers an excellent developer experience for frontend developers, it should primarily be used for composing data from multiple sources. REST/RPC APIs remain the best choice for implementing your business logic. Additionally, you should not hand-write your GraphQL layer. Instead, you can use Tailcall to automatically convert your REST APIs to GraphQL.  If you want to know why we think this way, you can read more about it in the article Writing a GraphQL Backend by Hand is Long Gone.  ","version":"Next","tagName":"h2"},{"title":"Conclusion​","type":1,"pageTitle":"Is GraphQL Better Than REST? GraphQL vs REST Comparison","url":"/graphql/graphql-vs-rest-api-comparison/#conclusion","content":" REST is ideal for simple data sources with well-defined resources, using multiple endpoints in the form of URLs. GraphQL excels in handling large, complex, and interrelated data sources with a single URL endpoint, providing flexibility and efficiency.  Choosing between GraphQL and REST depends on the specific needs of your application. While REST offers simplicity and scalability, GraphQL provides unparalleled flexibility and efficiency for complex and dynamic data needs. For many applications, using a hybrid approach that leverages the strengths of both technologies might be the most effective strategy.  Understanding the differences between GraphQL and REST, along with their appropriate use cases, allows you to make informed decisions that optimize both the development process and the end-user experience. By considering the structure of your data, the nature of your client applications, and the specific requirements of your use cases, you can select the API design that best fits your needs.  Explore both GraphQL and REST by implementing them in small projects or integrating them into different parts of a larger system. Experience firsthand how each handles real-world data scenarios to better understand their operational benefits and limitations. This hands-on approach will provide deeper insights into their practical applications and help you make more informed decisions about which technology to adopt for various aspects of your projects.  For quickly creating a GraphQL server that converts REST APIs to GraphQL, check out Getting Started with Tailcall.  By immersing yourself in both the theory and practice of GraphQL and REST, you can develop a robust understanding of how to effectively design and implement APIs that meet the evolving demands of modern applications. ","version":"Next","tagName":"h2"},{"title":"Problem Statement","type":0,"sectionRef":"#","url":"/graphql/problem-statement/","content":"","keywords":"","version":"Next"},{"title":"Problem Space​","type":1,"pageTitle":"Problem Statement","url":"/graphql/problem-statement/#problem-space","content":" Two of the major paradigm shifts happening in the technology industry over the past few years are:  Complex User Interfaces: Responsive websites that worked on desktop and mobile are dead. To build a successful B2C business, you need to build for all three platforms viz. Android, iOS, and Web (Desktop/PWA). The applications need to look slick, rich in information, and have snappy response times.Microservice Proliferation: Many companies these days bootstrap themselves on microservices instead of monoliths. This is because the tooling has gotten a lot better, and reusable components are available either in open-source or as a fully managed SAAS solution. This allows developers to focus on their core business logic and move fast.  ","version":"Next","tagName":"h2"},{"title":"Microservice Architecture​","type":1,"pageTitle":"Problem Statement","url":"/graphql/problem-statement/#microservice-architecture","content":" This is what a typical microservices architecture looks like today:  The clients (Mobile/Web) make requests to the microservices through an API gateway. An API gateway is a server that acts as a single point of entry for any type of request, responsible for routing requests to the appropriate backend service and forwarding the response to the client. An API gateway can also perform common tasks such as authentication, rate limiting, and caching, making it a useful component in a microservices architecture: each service exposes an API to the gateway, and the gateway acts as the &quot;front desk&quot; for clients to access the services.  ","version":"Next","tagName":"h2"},{"title":"API Orchestration​","type":1,"pageTitle":"Problem Statement","url":"/graphql/problem-statement/#api-orchestration","content":" API orchestration refers to the process of combining one or more APIs to create a new API. This can be done by creating a new API that either acts as a facade for the underlying APIs, or splits up incoming requests, delegates to the underlying APIs, and combines the results back together. Consider a scenario where a social media client application wants to display a timeline of posts with the author's profile information next to each one. In this case, the client can send two separate requests to two different APIs and combine them as follows: First to /posts to retrieve recent posts, with the following response:  type Post { id: ID! title: String! body: String! userId: ID! # Reference to the user by its id. }   Second, with the userId from the above post response, make a request to /users to retrieve the user's profile information, with the following response:  type User { id: ID! name: String! email: String! }   The client can then combine the results from these two APIs to create a single response that contains all the required information. This new response can be considered as the output of the composed API.  type Post { id: ID! title: String! body: String! user: User # Reference to the complete user object }   Orchestration is not limited to stitching APIs, here are some other use cases where having an API Orchestrator is of significant value:  Access Control: Instead of building an “admin” API and a “customer” API, you could create a set of basic CRUD endpoints and build access control on top of the orchestrator. Localization: Adding support for language translations can be moved to an orchestration layer instead of embedding into the application layer. Batching: An orchestrator can intelligently leverage batch APIs automatically without the consumer making any change, thus drastically reducing load. Obfuscation: An orchestrator can precisely control which field needs to be obfuscated, how, and when. Protocol Translation: An orchestrator can very efficiently convert between protocols. Validations: An orchestrator could filter out invalid requests up front, reducing unnecessary work on the underlying services. Type Safe SDK: Orchestration engines can generate type safe client SDKs to consume APIs. Discoverability: Orchestrators can provide detailed &amp; up-to-date documentation of the APIs that are exposed. Collaboration: Allows consumers and producers of APIs to move at different speeds via &quot;mocking&quot;. Optimize APIs based on usage patterns. Breaking Changes: Identify breaking changes, performance degradations and other potential issues even before deployment. Business Logic: Logic controlling the flow of requests based on business conditions is best suited to execution within the gateway or orchestration layer. Distributed Management: Instead of giving control of all APIs to one team, each team can manage their part of the API and seamlessly compose with the existing API network.  important API Orchestration is distinct from Microservice Orchestration, the latter relates to managing multiple micro-services working together to perform a larger task or workflow.  ","version":"Next","tagName":"h2"},{"title":"Composition on Clients​","type":1,"pageTitle":"Problem Statement","url":"/graphql/problem-statement/#composition-on-clients","content":" Composition on the client side remains unstandardized. Common problems include over-fetching and under-fetching. Over-fetching is where the server responds to a client request with more data than is required to render the screen. Under fetching is where the client needs to make multiple, often chained, API requests to get relevant data for a particular screen because the server couldn't provide all the required data in a single request. These two problems in conjunction with modest hardware and an unreliable network connection can make the overall solution unreliable, slow, and frustrating.  tip When composing on the client side, modest hardware and unfavorable network conditions often result in poor user-experience.  Increased Complexity:To build a rich user interface, API composition is necessary. One of the main challenges with API composition on the client side is that it can lead to increased complexity in the client application: the client needs to handle sending requests to multiple APIs and combining the results, adding to the overall size and complexity of the client code. Reduced Performance:Another challenge with API composition on the client side is that it can result in reduced performance and increased latency:the client often needs to make multiple requests to different APIs, taking more time and resulting in a slower response from the composed API. Increased Risk:In addition, API composition on the client side can also lead to increased security risks:the client needs to handle sensitive information such as API keys and authentication credentials for multiple APIs. These critical security tokens can be vulnerable to attacks if not properly secured, and many clients lack access to powerful CPUs and reliable network connections.  ","version":"Next","tagName":"h2"},{"title":"Backend For Frontend (BFF)​","type":1,"pageTitle":"Problem Statement","url":"/graphql/problem-statement/#backend-for-frontend-bff","content":" A BFF layer can help to solve the challenges of API composition mentioned above by providing a separate backend service that is optimized for each specific frontend client. This can enable the BFF to perform API composition on behalf of the client, which can help to improve the performance and reliability of the composed API. The BFF layer typically sits as a separate component in the overall architecture, between the frontend client and the microservices. It can communicate with both the frontend client and the microservices using well-defined interfaces and protocols, such as REST or gRPC.  tip BFFs can dramatically improve the reliability and performance of the system, thereby having a direct positive impact on user-experience.  The BFF can take advantage of a powerful CPU and access to a fast network to improve the performance and reliability of the composed API. It can also provide added flexibility and control over the composition process. This can make it a useful tool for developers who want to create new APIs by combining the functionality of multiple underlying APIs. However, there are a few challenges with a BFF layer:  Highly Specialized:BFF layers are highly specialized solutions that require a significant amount of hand-written code. Unlike an API gateway, there is no standard BFF solution that can be deployed out-of-the-box, and each BFF implementation must be custom-tailored to the specific requirements of the frontend client. This lack of standardization and reusability can make the BFF solution more complex and difficult to maintain. Fragile:Fragile and susceptible to failure, the BFF solution is dependent on the developers to follow best practices and handle all error scenarios. If these steps are not taken, the solution can be prone to bugs and performance issues. Additionally, the BFF solution must be thoroughly tested, including performance testing, unit testing, and integration testing, to ensure that it is reliable and performs well in production. This can require significant effort and expertise, and if these steps are not properly followed, the resulting BFF solution will likely be fragile and prone to failure. Since the BFF layer is the client's sole entry point to your backend, it becoming unavailable translates into a complete service outage for the user - it is therefore essential this layer be robust and resilient to exceptions. Speculative Performance:Because BFF layers are typically custom-written for each use case, it can be difficult to predict the performance impact of a small code change. Issues such as unoptimized algorithms, inefficient caching, and unnecessary downstream requests can go unnoticed and only be discovered very late in the development cycle. Typically this means companies must perform thorough benchmarking and load testing before anything goes to production, resulting in a high time to market even for minor changes. Monolithic:This layer frequently becomes quite comprehensive, intertwining with numerous backend services. It's not unusual for it to include a significant amount of complex, manually written code that can be challenging to manage. These issues can make it more difficult for new engineers to get up to speed, and can increase the time and cost associated with updating libraries or making architectural enhancements. Even small changes might necessitate large scale deployments across your infrastructure. Canary Support (Lack thereof):Every change that happens in the backend requires the deployment of the BFF layer. Any feature that is built on the client also requires changes on the BFF layer. Such frequent changes can not be exposed to 100% of users because the reliability and performance of this system are unknown. A common way to solve this problem is to use Blue-Green deployments. This requires additional infrastructure and complex routing mechanisms. First-class support to do canary releases is very important and should be part of a modern BFF layer, however, most companies rely on DevOps for its support. Coupled Releases:Since the BFF layer acts as a bridge between clients and services it serves as the middle link in the dependency chain: the client depends on the BFF, which in turn depends on the services. When it's time to deploy new features, first you must deploy the new services (which must support the existing and new BFF), then the new BFF layer is deployed (which must support existing and new clients), and finally the client can be deployed. If, due to a bug in a microservice, you need to revert the services, then you'd also need to replace the BFF layer with one that supports the new client calls even though the services have (temporarily) lost support for these calls. This coupling makes for expensive operational management. Composability:Traditional APIs, such as REST, work well when interacting directly with a single data source. REST benefits from a mature infrastructure that handles various cross-cutting concerns, such as routing, load balancing, caching, rate limiting, authentication, and authorization. However, the semantics of these capabilities start to break down when we consider API composition. For example, imagine an API composed of two other APIs, where one is highly cacheable and the other is not well defined. The same issues arise with authorization, authentication, and rate limiting. This inherent lack of composability makes REST challenging to use in scenarios requiring API composition.  note Presentation Layer, Facade, Middleware, Frontend Layer, Orchestration Layer, API Adapter — these are all terms that are sometimes used to refer to the BFF layer ","version":"Next","tagName":"h2"},{"title":"Understanding GraphQL Schemas and Types","type":0,"sectionRef":"#","url":"/graphql/schemas-and-types/","content":"","keywords":"","version":"Next"},{"title":"What is GraphQL Schema?​","type":1,"pageTitle":"Understanding GraphQL Schemas and Types","url":"/graphql/schemas-and-types/#what-is-graphql-schema","content":" In GraphQL, schemas act as a bridge between the client and the owner of the data, i.e., the Data Source. A schema defines a contract between the client and the server, providing a clear understanding of the data that can be queried. Upon receiving a query, the server validates the query against the GraphQL schema, then executes it and sends back the response in the requested shape.  In simple terms, a schema is a comprehensive description of the data that clients can query. It outlines the types of objects, the relationships between them, and the operations available for querying and mutating data. It is defined using the GraphQL Schema Definition Language (SDL), a human-readable syntax that describes the capabilities of the API.  ","version":"Next","tagName":"h2"},{"title":"The Importance of Schemas in GraphQL​","type":1,"pageTitle":"Understanding GraphQL Schemas and Types","url":"/graphql/schemas-and-types/#the-importance-of-schemas-in-graphql","content":" Schemas in GraphQL are vital because they:  Specify the available data types.Define relationships between different data entities.Enforce data validation rules.Provide a clear contract between the server and the client.  ","version":"Next","tagName":"h2"},{"title":"GraphQL Type System​","type":1,"pageTitle":"Understanding GraphQL Schemas and Types","url":"/graphql/schemas-and-types/#graphql-type-system","content":" As discussed above, GraphQL defines various types that we can utilize to build our schema. Here are the different available types.  Scalar TypeObject TypeInput TypesEnum TypeInterface and Union TypesLists and Non-Null  ","version":"Next","tagName":"h2"},{"title":"Defining Types in GraphQL​","type":1,"pageTitle":"Understanding GraphQL Schemas and Types","url":"/graphql/schemas-and-types/#defining-types-in-graphql","content":" ","version":"Next","tagName":"h2"},{"title":"Scalar Types​","type":1,"pageTitle":"Understanding GraphQL Schemas and Types","url":"/graphql/schemas-and-types/#scalar-types","content":" Scalar types are primitive data types that resolve to a single value. Common scalar types in GraphQL include Int, Float, String, Boolean, and ID.  type Post { id: ID! title: String! content: String! }   ","version":"Next","tagName":"h3"},{"title":"Object Types​","type":1,"pageTitle":"Understanding GraphQL Schemas and Types","url":"/graphql/schemas-and-types/#object-types","content":" Object types represent a collection of fields, each with a specific type. For example, a User object type might have fields like id, name, and email and their corresponding types.  type User { id: ID! name: String! email: String! }   ","version":"Next","tagName":"h3"},{"title":"Input Types​","type":1,"pageTitle":"Understanding GraphQL Schemas and Types","url":"/graphql/schemas-and-types/#input-types","content":" Input types are used for complex mutations, allowing clients to pass structured objects as arguments.  input PostInput { title: String! content: String! authorId: ID! }   ","version":"Next","tagName":"h3"},{"title":"Enum Types​","type":1,"pageTitle":"Understanding GraphQL Schemas and Types","url":"/graphql/schemas-and-types/#enum-types","content":" Enumeration types restrict a field to a set of predefined values, enhancing type safety and validation.  enum Role { ADMIN EDITOR USER }   ","version":"Next","tagName":"h3"},{"title":"Interface and Union Types​","type":1,"pageTitle":"Understanding GraphQL Schemas and Types","url":"/graphql/schemas-and-types/#interface-and-union-types","content":" Interfaces and unions enable polymorphic queries by allowing fields to return different types under a common interface or union.  interface Node { id: ID! } type User implements Node { id: ID! name: String! } type Post implements Node { id: ID! title: String! }   ","version":"Next","tagName":"h3"},{"title":"Lists and Non-Null Types​","type":1,"pageTitle":"Understanding GraphQL Schemas and Types","url":"/graphql/schemas-and-types/#lists-and-non-null-types","content":" In defining your schema, you will utilize object, scalar, input, and enum types. GraphQL also offers modifiers that enable quick validations within type definitions and arguments of queries and mutations. The available modifiers include:  Exclamation Mark (!) for Non-NullSquare Brackets ([]) for List  ","version":"Next","tagName":"h3"},{"title":"Relationships Between Types​","type":1,"pageTitle":"Understanding GraphQL Schemas and Types","url":"/graphql/schemas-and-types/#relationships-between-types","content":" GraphQL schema can represent relationships between types using references. For instance, a User can have multiple posts, and each Post can reference its author.  type User { id: ID! name: String! posts: [Post!]! } type Post { id: ID! title: String! content: String! author: User! }   ","version":"Next","tagName":"h2"},{"title":"Schema, Query and Mutation Types​","type":1,"pageTitle":"Understanding GraphQL Schemas and Types","url":"/graphql/schemas-and-types/#schema-query-and-mutation-types","content":" ","version":"Next","tagName":"h2"},{"title":"Schema Type​","type":1,"pageTitle":"Understanding GraphQL Schemas and Types","url":"/graphql/schemas-and-types/#schema-type","content":" Schema type is a special Object type which is the entry point for all GraphQL operations. It defines the queries, mutations, and subscriptions available in the schema.  schema { query: Query mutation: Mutation subscription: Subscription }   ","version":"Next","tagName":"h3"},{"title":"Query Type​","type":1,"pageTitle":"Understanding GraphQL Schemas and Types","url":"/graphql/schemas-and-types/#query-type","content":" The query type defines the entry point for read operations in a GraphQL schema. It specifies what data clients can fetch.  type Query { users: [User!]! user(id: ID!): User posts: [Post!]! post(id: ID!): Post }   ","version":"Next","tagName":"h3"},{"title":"Mutation Type​","type":1,"pageTitle":"Understanding GraphQL Schemas and Types","url":"/graphql/schemas-and-types/#mutation-type","content":" The mutation type defines the entry point for write operations, allowing clients to modify data.  type Mutation { createUser(name: String!, email: String!): User! createPost(input: PostInput!): Post! }   ","version":"Next","tagName":"h3"},{"title":"Subscriptions in GraphQL​","type":1,"pageTitle":"Understanding GraphQL Schemas and Types","url":"/graphql/schemas-and-types/#subscriptions-in-graphql","content":" Subscriptions allow clients to receive real-time updates when data changes. They are defined similarly to queries and mutations.  type Subscription { postAdded: Post! }   ","version":"Next","tagName":"h2"},{"title":"Best Practices for Designing GraphQL Schemas​","type":1,"pageTitle":"Understanding GraphQL Schemas and Types","url":"/graphql/schemas-and-types/#best-practices-for-designing-graphql-schemas","content":" Use Descriptive Naming Conventions: Ensure that type and field names are intuitive and descriptive.Leverage Scalar and Enum Types: Use scalar and enum types to enforce data validation.Design for Performance: Minimize nested queries and optimize resolver functions.Modularize Schemas: Break down large schemas into smaller, reusable modules.Documentation: Annotate schemas with comments for better maintainability and clarity.  ","version":"Next","tagName":"h2"},{"title":"Example Diagram: GraphQL Schema Structure​","type":1,"pageTitle":"Understanding GraphQL Schemas and Types","url":"/graphql/schemas-and-types/#example-diagram-graphql-schema-structure","content":"   ","version":"Next","tagName":"h2"},{"title":"Conclusion​","type":1,"pageTitle":"Understanding GraphQL Schemas and Types","url":"/graphql/schemas-and-types/#conclusion","content":" A robust and well-defined GraphQL schema is essential for building scalable and efficient APIs. By understanding the core concepts and best practices for defining schemas and types, developers can create powerful and flexible GraphQL servers that meet the needs of their clients. ","version":"Next","tagName":"h2"},{"title":"What is GraphQL?: A Simple Introduction","type":0,"sectionRef":"#","url":"/graphql/what-is-graphql/","content":"","keywords":"","version":"Next"},{"title":"GraphQL over HTTP​","type":1,"pageTitle":"What is GraphQL?: A Simple Introduction","url":"/graphql/what-is-graphql/#graphql-over-http","content":" ","version":"Next","tagName":"h2"},{"title":"Client Side​","type":1,"pageTitle":"What is GraphQL?: A Simple Introduction","url":"/graphql/what-is-graphql/#client-side","content":" GraphQL is a query language for your APIs. It gives clients the capability to ask for exactly what they need without worrying about where to get it from.  Instead of making traditional GET, POST, PUT, and DELETE requests to different endpoints, GraphQL needs only one endpoint to interact, typically using the POST method. The client sends queries in the body of the POST request. The request will look something like this:The query is sent as a string inside a JSON object.  ","version":"Next","tagName":"h3"},{"title":"Server Side​","type":1,"pageTitle":"What is GraphQL?: A Simple Introduction","url":"/graphql/what-is-graphql/#server-side","content":" On the server side, It is a runtime that understands these &quot;queries,&quot; fetches data from various data sources, bundles it in the shape that the client requested, and sends it back in an HTTP response.The response object is inside the data key of the JSON object.  The GraphQL server is responsible for exposing the schema, which is a strongly typed contract between the client and the server. It defines what queries clients can make, what types of data can be fetched, and what mutations can be performed.  For GraphQL, the origin of the data is irrelevant—it could come from a database, a microservice, or even a RESTful API. In essence, GraphQL is not concerned with the source of the data.  Check out the diagram below to get a better understanding of how GraphQL is used in your stack.  ","version":"Next","tagName":"h3"},{"title":"Client-Server Interaction:​","type":1,"pageTitle":"What is GraphQL?: A Simple Introduction","url":"/graphql/what-is-graphql/#client-server-interaction","content":" The client sends a query to the server. Note that the query is not in JSON format, but it looks like the shape of the JSON data the client needs. So when the POST request is made, the query is sent as a string inside a JSON object.The server receives the JSON object, extracts the query string from it, parses the query to check for proper syntax, and validates it against the Schema (the contract between the client and the server).Based on the query, the server fetches the data from the data sources and bundles it in the JSON object in the shape that the client requested.  ","version":"Next","tagName":"h3"},{"title":"GraphQL Adoption​","type":1,"pageTitle":"What is GraphQL?: A Simple Introduction","url":"/graphql/what-is-graphql/#graphql-adoption","content":" Due to this flexibility, the adoption of GraphQL has been increasing rapidly. There are many implementations available in various languages like JavaScript, Python, Ruby, Java, Rust, and more.  Starting off as a &quot;hobbyist&quot; stack, It has now been adopted by many big companies like Netflix, GitHub, Twitter, Pinterest, Shopify, and more.  ","version":"Next","tagName":"h2"},{"title":"Frequently Asked Questions​","type":1,"pageTitle":"What is GraphQL?: A Simple Introduction","url":"/graphql/what-is-graphql/#frequently-asked-questions","content":" ","version":"Next","tagName":"h2"},{"title":"Is GraphQL frontend or backend?​","type":1,"pageTitle":"What is GraphQL?: A Simple Introduction","url":"/graphql/what-is-graphql/#is-graphql-frontend-or-backend","content":" GraphQL has two parts: the client-side and the server-side. On the client side, It is a query language that allows you to ask for the data you need. On the server side, It is a runtime for executing those queries using a type system you define for your data.  ","version":"Next","tagName":"h3"},{"title":"Is GraphQL an API Gateway?​","type":1,"pageTitle":"What is GraphQL?: A Simple Introduction","url":"/graphql/what-is-graphql/#is-graphql-an-api-gateway","content":" GraphQL is not an API Gateway. However, it can be used as a layer between your client and your existing APIs to provide a more flexible and efficient way to interact with your data.  ","version":"Next","tagName":"h3"},{"title":"Is GraphQL a Database?​","type":1,"pageTitle":"What is GraphQL?: A Simple Introduction","url":"/graphql/what-is-graphql/#is-graphql-a-database","content":" GraphQL is not a database. It is a query language for your API and a server-side runtime for executing queries using a type system you define for your data. It can be used to query data from databases, REST/gRPC APIs, and other data sources.  ","version":"Next","tagName":"h3"},{"title":"Is GraphQL better than REST?​","type":1,"pageTitle":"What is GraphQL?: A Simple Introduction","url":"/graphql/what-is-graphql/#is-graphql-better-than-rest","content":" It depends on your use case. Since there is more efficiency associated with working with GraphQL, development is much faster with it than with REST.  ","version":"Next","tagName":"h3"},{"title":"How can I convert my REST APIs to GraphQL?​","type":1,"pageTitle":"What is GraphQL?: A Simple Introduction","url":"/graphql/what-is-graphql/#how-can-i-convert-my-rest-apis-to-graphql","content":" You can use tools like Tailcall, which is the simplest way to convert your REST APIs to GraphQL APIs. You can find more details here. ","version":"Next","tagName":"h3"},{"title":"GraphQL in React: 5 Best Approaches for Data Fetching.","type":0,"sectionRef":"#","url":"/graphql/graphql-react-client/","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"GraphQL in React: 5 Best Approaches for Data Fetching.","url":"/graphql/graphql-react-client/#introduction","content":" React developers often need to fetch data from GraphQL APIs. This comprehensive guide explores five effective methods for querying GraphQL data in React applications, using the SpaceX GraphQL API to demonstrate fetching and displaying data about recent space missions. We'll cover full-featured client libraries to lightweight solutions, providing a detailed comparison table and specific use cases for each method.  ","version":"Next","tagName":"h2"},{"title":"1. Apollo Client: The Comprehensive Solution​","type":1,"pageTitle":"GraphQL in React: 5 Best Approaches for Data Fetching.","url":"/graphql/graphql-react-client/#1-apollo-client-the-comprehensive-solution","content":" Apollo Client is the most feature-rich GraphQL library for React, widely adopted for its robust data fetching, built-in caching, and integrated state management.  Getting started with Apollo Client​  Install dependencies: npm install @apollo/client graphql Set up the Apollo Provider and client: import { ApolloProvider, ApolloClient, InMemoryCache, } from &quot;@apollo/client&quot; const client = new ApolloClient({ uri: &quot;https://api.spacex.land/graphql/&quot;, cache: new InMemoryCache(), }) ReactDOM.render( &lt;ApolloProvider client={client}&gt; &lt;App /&gt; &lt;/ApolloProvider&gt;, rootElement, ) Use the useQuery hook to fetch data: import {useQuery, gql} from &quot;@apollo/client&quot; const LAUNCHES_QUERY = gql` { launchesPast(limit: 10) { id mission_name } } ` function SpaceXLaunches() { const {data, loading, error} = useQuery(LAUNCHES_QUERY) if (loading) return &quot;Loading...&quot; if (error) return &lt;pre&gt;{error.message}&lt;/pre&gt; return ( &lt;ul&gt; {data.launchesPast.map((launch) =&gt; ( &lt;li key={launch.id}&gt;{launch.mission_name}&lt;/li&gt; ))} &lt;/ul&gt; ) }   ","version":"Next","tagName":"h3"},{"title":"2. Urql: The Lightweight Contender​","type":1,"pageTitle":"GraphQL in React: 5 Best Approaches for Data Fetching.","url":"/graphql/graphql-react-client/#2-urql-the-lightweight-contender","content":" Urql provides a streamlined alternative to Apollo, with a smaller bundle size, simpler setup process, and optional caching.  To use Urql​  Install dependencies: npm install urql graphql Set up the Urql Provider and client: import {createClient, Provider} from &quot;urql&quot; const client = createClient({ url: &quot;https://api.spacex.land/graphql/&quot;, }) ReactDOM.render( &lt;Provider value={client}&gt; &lt;App /&gt; &lt;/Provider&gt;, rootElement, ) Implement data fetching with the useQuery hook: import {useQuery} from &quot;urql&quot; const LAUNCHES_QUERY = ` { launchesPast(limit: 10) { id mission_name } } ` function SpaceXLaunches() { const [result] = useQuery({query: LAUNCHES_QUERY}) const {data, fetching, error} = result if (fetching) return &quot;Loading...&quot; if (error) return &lt;pre&gt;{error.message}&lt;/pre&gt; return ( &lt;ul&gt; {data.launchesPast.map((launch) =&gt; ( &lt;li key={launch.id}&gt;{launch.mission_name}&lt;/li&gt; ))} &lt;/ul&gt; ) }   ","version":"Next","tagName":"h3"},{"title":"3. React Query + GraphQL Request: The Flexible Duo​","type":1,"pageTitle":"GraphQL in React: 5 Best Approaches for Data Fetching.","url":"/graphql/graphql-react-client/#3-react-query--graphql-request-the-flexible-duo","content":" This combination pairs a lightweight GraphQL client with a powerful data-fetching library.  Install dependencies: npm install react-query graphql-request Set up React Query's QueryClientProvider: import { QueryClient, QueryClientProvider, } from &quot;react-query&quot; const queryClient = new QueryClient() ReactDOM.render( &lt;QueryClientProvider client={queryClient}&gt; &lt;App /&gt; &lt;/QueryClientProvider&gt;, rootElement, ) Implement data fetching: import {request, gql} from &quot;graphql-request&quot; import {useQuery} from &quot;react-query&quot; const endpoint = &quot;https://api.spacex.land/graphql/&quot; const LAUNCHES_QUERY = gql` { launchesPast(limit: 10) { id mission_name } } ` function SpaceXLaunches() { const {data, isLoading, error} = useQuery( &quot;launches&quot;, () =&gt; request(endpoint, LAUNCHES_QUERY), ) if (isLoading) return &quot;Loading...&quot; if (error) return &lt;pre&gt;{error.message}&lt;/pre&gt; return ( &lt;ul&gt; {data.launchesPast.map((launch) =&gt; ( &lt;li key={launch.id}&gt;{launch.mission_name}&lt;/li&gt; ))} &lt;/ul&gt; ) }   ","version":"Next","tagName":"h3"},{"title":"4. React Query + Axios: Leveraging a Popular HTTP Client​","type":1,"pageTitle":"GraphQL in React: 5 Best Approaches for Data Fetching.","url":"/graphql/graphql-react-client/#4-react-query--axios-leveraging-a-popular-http-client","content":" Combines React Query with Axios for familiar HTTP handling.  Install dependencies: npm install react-query axios Implement data fetching: import axios from &quot;axios&quot; import {useQuery} from &quot;react-query&quot; const endpoint = &quot;https://api.spacex.land/graphql/&quot; const LAUNCHES_QUERY = ` { launchesPast(limit: 10) { id mission_name } } ` function SpaceXLaunches() { const {data, isLoading, error} = useQuery( &quot;launches&quot;, () =&gt; axios .post(endpoint, {query: LAUNCHES_QUERY}) .then((response) =&gt; response.data.data), ) if (isLoading) return &quot;Loading...&quot; if (error) return &lt;pre&gt;{error.message}&lt;/pre&gt; return ( &lt;ul&gt; {data.launchesPast.map((launch) =&gt; ( &lt;li key={launch.id}&gt;{launch.mission_name}&lt;/li&gt; ))} &lt;/ul&gt; ) }   ","version":"Next","tagName":"h3"},{"title":"5. React Query + Fetch API: The Minimalist Approach​","type":1,"pageTitle":"GraphQL in React: 5 Best Approaches for Data Fetching.","url":"/graphql/graphql-react-client/#5-react-query--fetch-api-the-minimalist-approach","content":" Utilizes the browser's Fetch API with React Query for a minimalistic approach.  Install React Query: npm install react-query Implement data fetching: import { useQuery } from &quot;react-query&quot;; const endpoint = &quot;https://api.spacex.land/graphql/&quot;; const LAUNCHES_QUERY = ` { launchesPast(limit: 10) { id mission_name } } `; function SpaceXLaunches() { the { data, isLoading, error } = useQuery(&quot;launches&quot;, () =&gt; fetch(endpoint, { method: &quot;POST&quot;, headers: { &quot;Content-Type&quot;: &quot;application/json&quot; }, body: JSON.stringify({ query: LAUNCHES_QUERY }) }) .then(response =&gt; { if (!response.ok) throw new Error(&quot;Network response was not ok&quot;); return response.json(); }) .then(result =&gt; result.data) ); if (isLoading) return &quot;Loading...&quot;; if (error) return &lt;pre&gt;{error.message}&lt;/pre&gt;; return ( &lt;ul&gt; {data.launchesPast.map((launch) =&gt; ( &lt;li key={launch.id}&gt;{launch.mission_name}&lt;/li&gt; ))} &lt;/ul&gt; ); }   ","version":"Next","tagName":"h3"},{"title":"Detailed Comparison Table​","type":1,"pageTitle":"GraphQL in React: 5 Best Approaches for Data Fetching.","url":"/graphql/graphql-react-client/#detailed-comparison-table","content":" Here’s a comparison table to help choose the right method based on specific needs:  Method\tBundle Size (minified + gzip)*\tLearning Curve\tCaching Capabilities\tCommunity Support\tAdditional FeaturesApollo Client\t~47.04 KB\tModerate\tExtensive (InMemoryCache, customizable)\tHigh\tState management, optimistic UI updates Urql\t~2.18 KB\tLow\tModerate (Document caching)\tModerate\tExtensible architecture React Query + GraphQL Request\t~13 KB + ~185.8 KB\tLow\tBasic (Managed by React Query)\tGrowing\tMinimal overhead React Query + Axios\t~13 KB + ~13.2 KB\tLow\tBasic (Managed by React Query)\tHigh\tFamiliar HTTP handling React Query + Fetch API\t~13 KB + ~152.4 KB\tLow\tBasic (Managed by React Query)\tModerate\tBrowser-native, minimal setup  (*) culled from *bundlephobia.com*  ","version":"Next","tagName":"h2"},{"title":"Caching Capabilities​","type":1,"pageTitle":"GraphQL in React: 5 Best Approaches for Data Fetching.","url":"/graphql/graphql-react-client/#caching-capabilities","content":" Apollo Client: Normalized caching (stores entities by ID)Automatic cache updatesManual cache manipulationPersistence and rehydrationOptimistic updates Urql: Document caching (stores full query responses)Customizable caching with exchangersPersistence support React Query (applies to all React Query combinations): Time-based cachingStale-while-revalidate strategyManual cache manipulationPersistence and rehydration  ","version":"Next","tagName":"h3"},{"title":"Error Handling​","type":1,"pageTitle":"GraphQL in React: 5 Best Approaches for Data Fetching.","url":"/graphql/graphql-react-client/#error-handling","content":" Proper error handling is crucial for creating robust GraphQL applications. This section provides a detailed discussion on error handling for each client, including code examples for different types of errors and guidance on displaying user-friendly error messages.  ","version":"Next","tagName":"h2"},{"title":"1. Apollo Client​","type":1,"pageTitle":"GraphQL in React: 5 Best Approaches for Data Fetching.","url":"/graphql/graphql-react-client/#1-apollo-client","content":" Apollo Client provides detailed error information through the error property returned by the useQuery hook. It distinguishes between GraphQL errors and network errors.  import {useQuery, gql} from &quot;@apollo/client&quot; const LAUNCHES_QUERY = gql` { launchesPast(limit: 10) { id mission_name } } ` function SpaceXLaunches() { const {data, loading, error} = useQuery(LAUNCHES_QUERY) if (loading) return &quot;Loading...&quot; if (error) { return &lt;ErrorDisplay error={error} /&gt; } // Render data... } function ErrorDisplay({error}) { // Function to generate a user-friendly error message const getUserFriendlyErrorMessage = (error) =&gt; { if (error.networkError) { return &quot;Unable to reach the server. Please check your internet connection and try again.&quot; } if (error.graphQLErrors.length &gt; 0) { // You might want to customize this based on specific error codes or messages return &quot;There was an issue processing your request. Please try again later.&quot; } return &quot;An unexpected error occurred. Please try again.&quot; } return ( &lt;div className=&quot;error-container&quot;&gt; &lt;h2&gt;Oops! Something went wrong&lt;/h2&gt; &lt;p&gt;{getUserFriendlyErrorMessage(error)}&lt;/p&gt; {process.env.NODE_ENV !== &quot;production&quot; &amp;&amp; ( &lt;details&gt; &lt;summary&gt;Technical Details&lt;/summary&gt; {error.graphQLErrors.map( ({message, locations, path}, index) =&gt; ( &lt;div key={index}&gt; &lt;p&gt;GraphQL error: {message}&lt;/p&gt; &lt;p&gt;Location: {JSON.stringify(locations)}&lt;/p&gt; &lt;p&gt;Path: {JSON.stringify(path)}&lt;/p&gt; &lt;/div&gt; ), )} {error.networkError &amp;&amp; ( &lt;p&gt; Network error: {error.networkError.message} &lt;/p&gt; )} &lt;/details&gt; )} &lt;/div&gt; ) }   This example demonstrates how to:  Display a user-friendly error message based on the type of errorShow technical details only in non-production environmentsHandle both GraphQL and network errors  ","version":"Next","tagName":"h3"},{"title":"2. Urql​","type":1,"pageTitle":"GraphQL in React: 5 Best Approaches for Data Fetching.","url":"/graphql/graphql-react-client/#2-urql","content":" Urql provides error information through the error property in the result object.  import {useQuery} from &quot;urql&quot; const LAUNCHES_QUERY = ` { launchesPast(limit: 10) { id mission_name } } ` function SpaceXLaunches() { const [result] = useQuery({query: LAUNCHES_QUERY}) const {data, fetching, error} = result if (fetching) return &quot;Loading...&quot; if (error) { return &lt;ErrorDisplay error={error} /&gt; } // Render data... } function ErrorDisplay({error}) { const getUserFriendlyErrorMessage = (error) =&gt; { if (error.networkError) { return &quot;Unable to reach the server. Please check your internet connection and try again.&quot; } if (error.graphQLErrors.length &gt; 0) { // Customize based on specific error types if needed return &quot;There was an issue processing your request. Please try again later.&quot; } return &quot;An unexpected error occurred. Please try again.&quot; } return ( &lt;div className=&quot;error-container&quot;&gt; &lt;h2&gt;Oops! Something went wrong&lt;/h2&gt; &lt;p&gt;{getUserFriendlyErrorMessage(error)}&lt;/p&gt; {process.env.NODE_ENV !== &quot;production&quot; &amp;&amp; ( &lt;details&gt; &lt;summary&gt;Technical Details&lt;/summary&gt; {error.graphQLErrors.map( (graphQLError, index) =&gt; ( &lt;div key={index}&gt; &lt;p&gt;GraphQL error: {graphQLError.message}&lt;/p&gt; {graphQLError.locations &amp;&amp; ( &lt;p&gt; Location:{&quot; &quot;} {JSON.stringify(graphQLError.locations)} &lt;/p&gt; )} {graphQLError.path &amp;&amp; ( &lt;p&gt; Path:{&quot; &quot;} {JSON.stringify(graphQLError.path)} &lt;/p&gt; )} &lt;/div&gt; ), )} {error.networkError &amp;&amp; ( &lt;p&gt; Network error: {error.networkError.message} &lt;/p&gt; )} &lt;/details&gt; )} &lt;/div&gt; ) }   ","version":"Next","tagName":"h3"},{"title":"React Query (applies to all React Query examples)​","type":1,"pageTitle":"GraphQL in React: 5 Best Approaches for Data Fetching.","url":"/graphql/graphql-react-client/#react-query-applies-to-all-react-query-examples","content":" React Query provides error information through the error property returned by the useQuery hook.  When using React Query with GraphQL Request, you need to handle errors from both libraries. This approach requires more manual error handling but offers fine-grained control.  import {useQuery} from &quot;react-query&quot; import {request, gql} from &quot;graphql-request&quot; const endpoint = &quot;https://api.spacex.land/graphql/&quot; const LAUNCHES_QUERY = gql` { launchesPast(limit: 10) { id mission_name } } ` function SpaceXLaunches() { const {data, isLoading, error} = useQuery( &quot;launches&quot;, async () =&gt; { try { return await request(endpoint, LAUNCHES_QUERY) } catch (error) { // GraphQL Request wraps GraphQL errors in a ClientError if (error.response) { throw new Error( JSON.stringify(error.response.errors), ) } else { // Network error throw new Error(`Network error: ${error.message}`) } } }, ) if (isLoading) return &quot;Loading...&quot; if (error) { return &lt;ErrorDisplay error={error} /&gt; } // Render data... } function ErrorDisplay({error}) { const getUserFriendlyErrorMessage = (error) =&gt; { try { const parsedError = JSON.parse(error.message) if (Array.isArray(parsedError)) { // GraphQL errors return &quot;There was an issue processing your request. Please try again later.&quot; } } catch { // Network error or other non-GraphQL error return &quot;Unable to reach the server. Please check your internet connection and try again.&quot; } return &quot;An unexpected error occurred. Please try again.&quot; } return ( &lt;div className=&quot;error-container&quot;&gt; &lt;h2&gt;Oops! Something went wrong&lt;/h2&gt; &lt;p&gt;{getUserFriendlyErrorMessage(error)}&lt;/p&gt; {process.env.NODE_ENV !== &quot;production&quot; &amp;&amp; ( &lt;details&gt; &lt;summary&gt;Technical Details&lt;/summary&gt; &lt;pre&gt;{error.message}&lt;/pre&gt; &lt;/details&gt; )} &lt;/div&gt; ) }   ","version":"Next","tagName":"h3"},{"title":"Common Issues and Resolutions​","type":1,"pageTitle":"GraphQL in React: 5 Best Approaches for Data Fetching.","url":"/graphql/graphql-react-client/#common-issues-and-resolutions","content":" ","version":"Next","tagName":"h2"},{"title":"1. Apollo Client​","type":1,"pageTitle":"GraphQL in React: 5 Best Approaches for Data Fetching.","url":"/graphql/graphql-react-client/#1-apollo-client-1","content":" Issue: Cache inconsistencies Resolution: Use refetchQueries option when mutating data or implement cache update functions.Issue: Overfeching data Resolution: Utilize fragment colocation and implement proper query splitting.  ","version":"Next","tagName":"h3"},{"title":"2. Urql​","type":1,"pageTitle":"GraphQL in React: 5 Best Approaches for Data Fetching.","url":"/graphql/graphql-react-client/#2-urql-1","content":" Issue: Stale data after mutations Resolution: Use the cache-and-network fetch policy or implement manual cache updates.Issue: SSR hydration mismatches Resolution: Ensure consistent query variables between server and client, or use the ssrExchange.  ","version":"Next","tagName":"h3"},{"title":"3. React Query (all combinations)​","type":1,"pageTitle":"GraphQL in React: 5 Best Approaches for Data Fetching.","url":"/graphql/graphql-react-client/#3-react-query-all-combinations","content":" Issue: Stale data displayed briefly before refetch Resolution: Adjust staleTime and cacheTime options to fine-tune caching behavior.Issue: Unnecessary refetches on component remount Resolution: Implement proper query keys and adjust refetchOnMount option.  ","version":"Next","tagName":"h3"},{"title":"Use Cases for Each Method​","type":1,"pageTitle":"GraphQL in React: 5 Best Approaches for Data Fetching.","url":"/graphql/graphql-react-client/#use-cases-for-each-method","content":" Apollo Client: Best for large-scale applications needing complex state management and data synchronization.Urql: Suitable for medium-sized projects where simplicity and performance are prioritized.React Query + GraphQL Request: Ideal for projects requiring high flexibility with minimal GraphQL-specific setup.React Query + Axios: Preferred when developers are already familiar with Axios and need robust HTTP capabilities.React Query + Fetch API: Optimal for projects that require a minimalistic approach with no additional dependencies.  ","version":"Next","tagName":"h3"},{"title":"Conclusion​","type":1,"pageTitle":"GraphQL in React: 5 Best Approaches for Data Fetching.","url":"/graphql/graphql-react-client/#conclusion","content":" By understanding the distinct features and use cases of each method, developers can select the most appropriate GraphQL fetching technique for their React projects. This guide aims to equip developers with the knowledge to efficiently integrate GraphQL data fetching into their applications, regardless of scale or complexity. ","version":"Next","tagName":"h2"},{"title":"GraphQL Configuration","type":0,"sectionRef":"#","url":"/docs/tailcall-dsl-graphql-custom-directives/","content":"","keywords":"","version":"Next"},{"title":"@addField Directive​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#addfield-directive","content":" The @addField directive simplifies data structures and queries by adding a field that inline or flattens a nested field or node within your schema. It modifies the schema and the data transformation process, making nested data more accessible and straightforward to present.  For instance, consider a schema:  schema { query: Query } type User @addField(name: &quot;street&quot;, path: [&quot;address&quot;, &quot;street&quot;]) { id: Int! name: String! username: String! email: String! phone: String website: String address: Address @modify(omit: true) } type Address { street: String! city: String! state: String! } type Query { user(id: Int!): User @http(path: &quot;/users/{{.args.id}}&quot;) }   Suppose we focus on the street field in Address.  In this case, applying the @addField directive to the User type creates a street field within the User type. It uses a path argument to specify the sequence of fields from a declared field (address), leading to the Address field to add. We also can apply @modify(omit: true) to remove the address field from the schema, as the street field from Address is now directly accessible on the User type.  Post application, the schema becomes:  schema { query: Query } type User { id: Int! name: String! username: String! email: String! phone: String website: String street: String } type Query { user(id: Int): Post! }   In the above example, since we added a @modify(omit: true) on the address field, the schema no longer includes the Address type.  The @addField directive also take cares of nullablity of the fields. If any of the fields in the path is nullable, the resulting type will be nullable.  @addField also supports indexing, allowing for the specification of an array index for inline inclusion. For instance, if a field posts is of type [Post], and the goal is to access the title of the first post, specify the path as [&quot;posts&quot;,&quot;0&quot;,&quot;title&quot;].  type User @addField( name: &quot;firstPostTitle&quot; path: [&quot;posts&quot;, &quot;0&quot;, &quot;title&quot;] ) { id: Int! name: String! username: String! email: String! phone: String website: String posts: Post @http(path: &quot;/users/{{.value.id}}/posts&quot;) } type Post { id: Int! userId: Int! title: String! body: String! }   In conclusion, the @addField directive helps tidy up your schema and streamline data fetching by reducing query depth, promoting better performance and simplicity.  ","version":"Next","tagName":"h2"},{"title":"@cache Directive​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#cache-directive","content":" The @cache directive provides a protocol agnostic mechanism for caching the results of fields within a GraphQL schema. Like any other cache implementation, this feature is useful for optimizing performance by reducing the need to fetch data that doesn't change frequently.  ","version":"Next","tagName":"h2"},{"title":"maxAge​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#maxage","content":" @cache(maxAge: Int)   This parameter is a non-zero unsigned integer specifying the duration, in milliseconds, that retains the cached value.  ","version":"Next","tagName":"h3"},{"title":"Usage​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#usage","content":" Consider the following GraphQL schema example:  type Query { posts: [Post] @http(path: &quot;/posts&quot;) } type Post { id: Int title: String userId: Int @cache(maxAge: 100) user: User @http(path: &quot;/user/{{.value.userId}}&quot;) @cache(maxAge: 200) } type User { id: Int name: String email: String }   In this configuration, the system caches the result of the user field due to its association with an HTTP resolver. But it does not cache the values of userId and title because they lack individual resolvers; the resolver for the posts field retrieves their values, employing the @http(path: &quot;/posts&quot;) directive.  Applying the @cache directive at the type level affects all fields within that type. For example:  type Query { posts: [Post] @http(path: &quot;/posts&quot;) } type Post @cache(maxAge: 100) { id: Int title: String userId: Int user: User @http(path: &quot;/user/{{.value.userId}}&quot;) } type User { id: Int name: String email: String }   You can simplify this configuration to show that applying the @cache directive to a type means every field within that type inherits it:  type Query { posts: [Post] @http(path: &quot;/posts&quot;) } type Post { id: Int @cache(maxAge: 100) title: String @cache(maxAge: 100) userId: Int @cache(maxAge: 100) user: User @http(path: &quot;/user/{{.value.userId}}&quot;) @cache(maxAge: 100) } type User { id: Int name: String email: String }   Since the @cache directive does not affect fields without resolvers, the effective configuration can be further reduced as follows:  type Query { posts: [Post] @http(path: &quot;/posts&quot;) } type Post { id: Int title: String userId: Int user: User @http(path: &quot;/user/{{.value.userId}}&quot;) @cache(maxAge: 100) } type User { id: Int name: String email: String }   When applying the @cache directive both at the type level and on individual fields within that type, the field-level directive takes precedence:  type Query { posts: [Post] @http(path: &quot;/posts&quot;) } type Post @cache(maxAge: 200) { id: Int title: String userId: Int user: User @http(path: &quot;/user/{{.value.userId}}&quot;) @cache(maxAge: 100) } type User { id: Int name: String email: String }   Thus, in the configuration above, while all fields inherit the @cache(maxAge: 200) directive at the type level, the user field's explicit @cache(maxAge: 100) directive takes precedence.  ","version":"Next","tagName":"h3"},{"title":"Cache Key​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#cache-key","content":" The caching mechanism generates a hash based on information related to the applied query to serve as the cache key for the corresponding value.  For instance, the system caches the user field in the following configuration, using the hash of the interpolated string &quot;/user/{{.value.userId}}&quot; as the cache key. For example, if Post.userId equals 1, the system generates the cache key by hashing the string &quot;/users/1&quot;.  ","version":"Next","tagName":"h3"},{"title":"@call Directive​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#call-directive","content":" The @call directive in GraphQL signifies a shift towards more efficient configuration management by introducing a methodology akin to function invocations in conventional programming. This directive is pivotal for developers navigating the intricacies of elaborate GraphQL schemas, where minimizing redundancy and adhering to the DRY (Don't Repeat Yourself) principle are paramount. Consider the following schema example:  schema @upstream( baseURL: &quot;https://jsonplaceholder.typicode.com&quot; ) { query: Query } type Query { user(id: Int!): User @http(path: &quot;/users/{{.args.id}}&quot;) posts: [Post] @http(path: &quot;/posts&quot;) } type Post { id: Int! userId: Int! title: String! body: String! user: User @http(path: &quot;/users/{{.value.userId}}&quot;) } type User { id: Int! name: String! email: String! }   In this schema, at lines 9 and 18, a pattern of configuration duplication emerges when fetching user's data by its id, demonstrating a prime use case for the @call directive. Through refactoring the Post type to incorporate the @call directive, we can eliminate this redundancy.  type Post { id: Int! userId: Int! title: String! body: String! user: User @call( steps: [ {query: &quot;user&quot;, args: {id: &quot;{{.value.userId}}&quot;}} ] ) }   Here, the @call directive invokes the user query from the Query type, leveraging the data-fetching process that's already defined in the root query. The query parameter specifies the target field, while the args parameter delineates the arguments to be passed.  ","version":"Next","tagName":"h2"},{"title":"steps​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#steps","content":" @call directive can compose together other resolvers, allowing to create a chain of resolvers that can be executed in sequence. This is done by using the steps parameter, which is an array of objects that define the operations to be executed.  ","version":"Next","tagName":"h3"},{"title":"query​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#query","content":" Specify the root query field to invoke, alongside the requisite arguments, using the @call directive for a concise and efficient query structure.  type Post { userId: Int! user: User @call( steps: [ {query: &quot;user&quot;, args: {id: &quot;{{.value.userId}}&quot;}} ] ) }   ","version":"Next","tagName":"h3"},{"title":"mutation​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#mutation","content":" Similarly, the @call directive can facilitate calling a mutation from another mutation field, employing the mutation parameter for field specification and the args parameter for argument delineation.  type Mutation { insertPost(input: PostInput, overwrite: Boolean): Post @http( body: &quot;{{.args.input}}&quot; method: &quot;POST&quot; path: &quot;/posts&quot; query: {overwrite: &quot;{{.args.overwrite}}&quot;} ) upsertPost(input: PostInput): Post @call( steps: [ { mutation: &quot;insertPost&quot; args: {input: &quot;{{.args.input}}&quot;, overwrite: true} } ] ) }   ","version":"Next","tagName":"h3"},{"title":"args​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#args","content":" The args parameter in the @call directive facilitates passing arguments to the targeted query or mutation, represented as a key-value mapping where each key corresponds to an argument name and its associated value.  type Post { userId: Int! user: User @call( steps: [ {query: &quot;user&quot;, args: {id: &quot;{{.value.userId}}&quot;}} ] ) }   tip The @call directive is predominantly advantageous in complex, large-scale configurations. For those new to GraphQL or Tailcall, it may be beneficial to explore this directive after familiarizing yourself with the foundational aspects of GraphQL.  ","version":"Next","tagName":"h3"},{"title":"Composition​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#composition","content":" @call directive provides the ability to express a sequence of steps that one might need to compose. These steps are executed such that the result of each step is passed as an argument to the next step. The query and mutation parameters are used to specify the target field, while the args parameter is used to pass arguments to the target field.  Let's explain this with an example:  schema @server { query: Query } type Query { a(input: JSON): JSON @expr(body: {value: &quot;{{.args.input.a}}&quot;}) b(input: JSON): JSON @expr(body: {value: &quot;{{.args.input.b}}&quot;}) c(input: JSON): JSON @expr(body: {value: &quot;{{.args.input.c}}&quot;}) }   Here we have defined there operations viz. a, b &amp; c each of them pluck their respective keys from the given input value. Let's run this query with some test input:  { a(input: {a: 100}) b(input: {b: 200}) c(input: {c: 300}) }   Here is how the response would look like:  { &quot;data&quot;: { &quot;a&quot;: { &quot;value&quot;: 100 }, &quot;b&quot;: { &quot;value&quot;: 200 }, &quot;c&quot;: { &quot;value&quot;: 300 } } }   As you can see the @expr directive plucks the inner value and returns the result. How about we implement an abc operation that could leverage the existing operations and unwrap the following input value:  {&quot;a&quot;: {&quot;b&quot;: {&quot;c&quot;: {&quot;d&quot;: 1000}}}}   Given the above input if we wish to extract the last inner number 1000 then we could define a new operation as follows  schema @server { query: Query } type Query { a(input: JSON): JSON @expr(body: {value: &quot;{{.args.input.a}}&quot;}) b(input: JSON): JSON @expr(body: {value: &quot;{{.args.input.b}}&quot;}) c(input: JSON): JSON @expr(body: {value: &quot;{{.args.input.c}}&quot;}) abc(input: JSON): JSON @call( steps: [ {query: &quot;a&quot;, args: {input: &quot;{{.args.input}}&quot;}} {query: &quot;b&quot;, args: {input: &quot;{{.args.value}}&quot;}} {query: &quot;c&quot;, args: {input: &quot;{{.args.value}}&quot;}} ] ) }   We use the @call directive to compose the operations together. The args specify how we would like to pass the arguments to the operation and the result of that operation is passed to the next step. We can test the new abc operation with the following query:  query { abc(input: {a: {b: {c: 1000}}}) }   The server returns the response that we expected:  { &quot;data&quot;: { &quot;abc&quot;: { &quot;value&quot;: 100 } } }   This way you can compose combine multiple operations can compose them together using the @call directive.  note We use JSON scalar here because we don't care about the type safety of this option. In a real world example you might want to use proper input and output types.  ","version":"Next","tagName":"h3"},{"title":"@expr Directive​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#expr-directive","content":" The @expr directive in GraphQL is a powerful tool for embedding data directly into your schema, offering two primary functionalities:  ","version":"Next","tagName":"h2"},{"title":"Static​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#static","content":" This feature allows for the inclusion of a constant response within the schema definition itself. It is useful for scenarios where the response is unchanging. e.g:  schema { query: Query } type Query { user: User @expr(body: {name: &quot;John&quot;, age: 12}) } type User { name: String age: Int }   The @expr directive also checks the provided value at compile time to ensure it matches the field's schema. If not, the console displays a descriptive error message.  ","version":"Next","tagName":"h3"},{"title":"Dynamic​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#dynamic","content":" Beyond static data embedding, the @expr directive extends its utility to support dynamic data injection through Mustache template syntax. This feature enables the use of placeholders within the constant data, which are then dynamically replaced with actual values at runtime. It supports both scalar values and complex objects, including lists and nested objects, offering flexibility in tailoring responses to specific needs. e.g:  schema { query: Query } type Query { user: User @expr( body: { name: &quot;John&quot; workEmail: &quot;john@xyz.com&quot; personalEmail: &quot;john@xyz.com&quot; } ) } type User { name: String age: Int personalEmail: String workEmail: String emails: Emails @expr( body: { emails: { workEmail: &quot;{{.value.workEmail}}&quot; personalEmail: &quot;{{.value.personalEmail}}&quot; } } ) } type Emails { workEmail: String personalEmail: String }   In this example, the @expr directive dynamically generate an Emails object based on the provided template data. The placeholders within the template ({{.value.workEmail}} and {{.value.personalEmail}}) gets replaced with the actual values specified in the User type, allowing for dynamic content generation while still adhering to the schema's structure.  ","version":"Next","tagName":"h3"},{"title":"@graphQL Directive​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#graphql-directive","content":" The @graphQL directive allows to specify GraphQL API server request to fetch data from.  type Query { users: [User] @graphQL(name: &quot;userList&quot;) }   The @graphQL directive facilitates fetching a list of users from the GraphQL API upstream. The name argument specifies the root field's name on the upstream server. The upcoming request to the GraphQL server determines the User type's inner fields for the request. Depending on the operation type within which one finds the @graphQL directive, the GraphQL configuration determines the query's operation type.  For the next request with the config above:  query { users { id name } }   Tailcall will request the next query for the upstream:  query { userList { id name } }   ","version":"Next","tagName":"h2"},{"title":"baseURL​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#baseurl","content":" This refers to the base URL of the API. If not specified, the default base URL is the one specified in the @upstream directive.  type Query { users: [User] @graphQL( name: &quot;users&quot; baseURL: &quot;https://graphqlzero.almansi.me/api&quot; ) }   ","version":"Next","tagName":"h3"},{"title":"name​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#name","content":" The root field's name on the upstream to request data from. For example:  type Query { users: [User] @graphQL(name: &quot;userList&quot;) }   When Tailcall receives a query for the users field, it will request a query for userList from the upstream.  ","version":"Next","tagName":"h3"},{"title":"args​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#args-1","content":" Named arguments for the requested field. For example:  type Query { user: User @graphQL( name: &quot;user&quot; args: [{key: &quot;id&quot;, value: &quot;{{.value.userId}}&quot;}] ) }   Will request the next query from the upstream for the first user's name:  query { user(id: 1) { name } }   ","version":"Next","tagName":"h3"},{"title":"headers​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#headers","content":" The headers parameter allows customizing the headers of the GraphQL request made by the @graphQL directive. Specifying a key-value map of header names and their values achieves this.  For instance:  type Mutation { users: User @graphQL( name: &quot;users&quot; headers: [{key: &quot;X-Server&quot;, value: &quot;Tailcall&quot;}] ) }   In this example, a request to /users will include the HTTP header X-Server with the value Tailcall.  ","version":"Next","tagName":"h3"},{"title":"batch​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#batch","content":" In case the upstream GraphQL server supports request batching, we can specify the batch argument to batch requests to a single upstream into a single batch request. For example:  schema @upstream( batch: { maxSize: 1000 delay: 10 headers: [&quot;X-Server&quot;, &quot;Authorization&quot;] } ) { query: Query mutation: Mutation } type Query { users: [User] @graphQL(name: &quot;users&quot;, batch: true) posts: [Post] @graphQL(name: &quot;posts&quot;, batch: true) }   Make sure you have also specified batch settings to the @upstream and to the @graphQL directive.  ","version":"Next","tagName":"h3"},{"title":"@grpc Directive​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#grpc-directive","content":" The @grpc directive enables the resolution of GraphQL fields via gRPC services. Below is an illustrative example of how to apply this directive within a GraphQL schema:  schema @link(src: &quot;./users.proto&quot;, type: Protobuf) { query: Query } type Query { users: [User] @grpc(method: &quot;users.UserService.ListUsers&quot;) }   This schema snippet demonstrates the directive's application, where a query for users triggers a gRPC request to the UserService's ListUsers method, thereby fetching the user data.  The .proto file delineates the structure and methods of the gRPC service. A simplified example of such a file is as follows:  syntax = &quot;proto3&quot;; package users; service UserService { rpc ListUsers (UserListRequest) returns (UserListReply) {} rpc GetUser (UserGetRequest) returns (UserGetReply) {} } message UserListRequest { // Definitions of request parameters } message UserListReply { // Structure of the reply } message UserGetRequest { // Definitions of request parameters } message UserGetReply { // Structure of the reply }   important It is mandatory to have a package name in a protobuf file.  Linking this file within a GraphQL schema is facilitated by the @link directive, as shown below:  schema @link(src: &quot;./users.proto&quot;, type: Protobuf) { query: Query }   Tailcall automatically resolves the protobuf file for any methods referenced in the @grpc directive.  ","version":"Next","tagName":"h2"},{"title":"method​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#method","content":" This parameter specifies the gRPC service and method to be invoked, formatted as &lt;package&gt;.&lt;service&gt;.&lt;method&gt;:  type Query { users: [User] @grpc(method: &quot;proto.users.UserService.ListUsers&quot;) }   ","version":"Next","tagName":"h3"},{"title":"baseURL​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#baseurl-1","content":" Defines the base URL for the gRPC API. If not specified, the URL set in the @upstream directive is used by default:  type Query { users: [User] @grpc( baseURL: &quot;https://grpc-server.example.com&quot; method: &quot;proto.users.UserService.ListUsers&quot; ) }   ","version":"Next","tagName":"h3"},{"title":"body​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#body","content":" This parameter outlines the arguments for the gRPC call, allowing for both static and dynamic inputs:  type UserInput { id: ID } type Query { user(id: UserInput!): User @grpc( body: &quot;{{.args.id}}&quot; method: &quot;proto.users.UserService.GetUser&quot; ) }   ","version":"Next","tagName":"h3"},{"title":"headers​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#headers-1","content":" Custom headers for the gRPC request can be defined, facilitating the transmission of authentication tokens or other contextual data:  type Query { users: [User] @grpc( headers: [ {key: &quot;X-CUSTOM-HEADER&quot;, value: &quot;custom-value&quot;} ] method: &quot;proto.users.UserService.ListUsers&quot; ) }   ","version":"Next","tagName":"h3"},{"title":"batchKey​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#batchkey","content":" This argument is employed to optimize batch requests by grouping them based on specified response keys, enhancing performance in scenarios requiring multiple, similar requests:  type Query { users(id: UserInput!): [User] @grpc( batchKey: [&quot;id&quot;] method: &quot;proto.users.UserService.ListUsers&quot; baseURL: &quot;https://grpc-server.example.com&quot; ) }   info Read about n + 1 to learn how to use the batchKey setting.  ","version":"Next","tagName":"h3"},{"title":"@http Directive​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#http-directive","content":" The @http directive indicates a field or node relies on a REST API. For example:  type Query { users: [User] @http(path: &quot;/users&quot;) }   In this example, adding the @http directive to the users field of the Query type indicates reliance on a REST API for the users field. The path argument specifies the REST API's path, which is /users in this scenario.Querying the users field prompts the GraphQL server to issue a GET request to https://jsonplaceholder.typicode.com/users.  ","version":"Next","tagName":"h2"},{"title":"baseURL​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#baseurl-2","content":" Specifies the API's base URL. If unspecified, it defaults to the URL in the @upstream directive.  type Query { users: [User] @http( path: &quot;/users&quot; baseURL: &quot;https://jsonplaceholder.typicode.com&quot; ) }   ","version":"Next","tagName":"h3"},{"title":"path​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#path","content":" Refers to the API endpoint, for example, https://jsonplaceholder.typicode.com/users.  type Query { users: [User] @http(path: &quot;/users&quot;) }   If your API endpoint contains dynamic segments, you can substitute variables using Mustache templates. For example, to fetch a specific user, you can write the path as /users/{{.args.id}}.  type Query { user(id: ID!): User @http(path: &quot;/users/{{.args.id}}&quot;) }   ","version":"Next","tagName":"h3"},{"title":"method​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#method-1","content":" Specifies the HTTP method for the API call. The default method is GET if not specified.  type Mutation { createUser(input: UserInput!): User @http(method: &quot;POST&quot;, path: &quot;/users&quot;) }   ","version":"Next","tagName":"h3"},{"title":"query​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#query-1","content":" Represents the API call's query parameters, either as a static object or with dynamic parameters using Mustache templates. These parameters append to the URL.  type Query { userPosts(id: ID!): [Post] @http( path: &quot;/posts&quot; query: [{key: &quot;userId&quot;, value: &quot;{{.args.id}}&quot;}] ) }   ","version":"Next","tagName":"h3"},{"title":"body​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#body-1","content":" Defines the API call's body, necessary for methods like POST or PUT. Pass it as a static object or use Mustache templates for variable substitution from the GraphQL variables.  type Mutation { createUser(input: UserInput!): User @http( method: &quot;POST&quot; path: &quot;/users&quot; body: &quot;{{.args.input}}&quot; ) }   In the example above, the createUser mutation sends a POST request to /users, with the input object converted to JSON and included in the request body.  ","version":"Next","tagName":"h3"},{"title":"headers​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#headers-2","content":" Customizes the HTTP request headers made by the @http directive. Specify a key-value map of header names and values.  For instance:  type Mutation { createUser(input: UserInput!): User @http( path: &quot;/users&quot; headers: [{key: &quot;X-Server&quot;, value: &quot;Tailcall&quot;}] ) }   In this example, a request to /users will include a HTTP header X-Server with the value Tailcall.  You can make use of mustache templates to provide dynamic values for headers, derived from the arguments or context provided in the request. For example:  type Mutation { users(name: String): User @http( path: &quot;/users&quot; headers: [ {key: &quot;X-Server&quot;, value: &quot;Tailcall&quot;} {key: &quot;User-Name&quot;, value: &quot;{{.args.name}}&quot;} ] ) }   In this scenario, the User-Name header's value will dynamically adjust according to the name argument passed in the request.  ","version":"Next","tagName":"h3"},{"title":"batchKey​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#batchkey-1","content":" Groups data requests into a single call, enhancing efficiency. Refer to our n + 1 guide for more details.  type Post { id: Int! name: String! user: User @http( path: &quot;/users&quot; query: [{key: &quot;id&quot;, value: &quot;{{.value.userId}}&quot;}] batchKey: [&quot;id&quot;] ) }   query: {key: &quot;id&quot;, value: &quot;{{.value.userId}}&quot;}]: Instructs TailCall CLI to generate a URL aligning the user id with userId from the parent Post, compiling a single URL for a batch of posts, such as /users?id=1&amp;id=2&amp;id=3...id=10, consolidating requests into one.  ","version":"Next","tagName":"h3"},{"title":"onRequest​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#onrequest","content":" The onRequest property accepts a string value representing the remote function to be called every time an HTTP request is initiated. Typically the remote function is defined in a linked JavaScript worker file.  note For defining a request middleware globally for all requests, refer to the upstream directive documentation.  type Query { userPosts(id: ID!): [Post] @http( path: &quot;/posts&quot; query: [{key: &quot;userId&quot;, value: &quot;{{.args.id}}&quot;}] onRequest: &quot;someFunctionName&quot; ) }   ","version":"Next","tagName":"h3"},{"title":"@js Directive​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#js-directive","content":" The @js directive allows you to use JavaScript functions to resolve fields in your GraphQL schema. This can be useful for custom data transformations or complex field resolutions.  ","version":"Next","tagName":"h2"},{"title":"Usage​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#usage-1","content":" The @js directive is used to specify a JavaScript function that will resolve the value of a field. The directive takes a single argument, name, which is the name of the JavaScript function to be used.  ","version":"Next","tagName":"h3"},{"title":"Syntax​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#syntax","content":" fieldName: FieldType @js(name: &quot;functionName&quot;)   ","version":"Next","tagName":"h3"},{"title":"Example​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#example","content":" Let's consider a foo.js file which contains a resolve function:  function resolve(val) { let json = JSON.parse(val) return JSON.stringify(json.id) }   Here is an example of how the @js directive is used within a GraphQL schema:  schema @link(type: Script, src: &quot;./scripts/foo.js&quot;) @server(port: 8000) @upstream( baseURL: &quot;http://jsonplaceholder.typicode.com&quot; httpCache: true ) { query: Query } type Query { posts: [Post] @http(path: &quot;/posts&quot;) } type Post { id: Int! idx: Int! @js(name: &quot;resolve&quot;) userId: Int! title: String! body: String! }   ","version":"Next","tagName":"h3"},{"title":"Error Handling​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#error-handling","content":" When using the @js directive, it is important to handle errors within your JavaScript functions. For example, you can use try-catch blocks to catch and handle any errors that occur during the resolution process.  function resolve(val) { try { let json = JSON.parse(val) return JSON.stringify(json.id) } catch (error) { console.error(&quot;Error resolving value:&quot;, error) throw new Error(&quot;Failed to resolve value&quot;) } }   ","version":"Next","tagName":"h3"},{"title":"Performance Considerations​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#performance-considerations","content":" When using the @js directive, keep in mind that JavaScript functions can introduce performance overhead, especially if they perform complex operations or are called frequently. To minimize performance impact, ensure that your functions are optimized and avoid unnecessary computations.  ","version":"Next","tagName":"h3"},{"title":"@link Directive​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#link-directive","content":" The @link directive is used for bringing external resources into your GraphQL schema. It makes it easier to include configurations, .proto files for gRPC services, and other files into your schema. With this directive, external resources are either merged with or used effectively in the importing configuration.  ","version":"Next","tagName":"h2"},{"title":"How it Works​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#how-it-works","content":" The @link directive requires specifying a source src, the resource's type type, and an optional identifier id.  src: The source of the link is defined here. It can be either a URL or a file path. When a file path is given, it's relative to the file's location that is importing the link. type: This specifies the link's type, which determines how the imported resource is integrated into the schema. For a list of supported types, see the Supported Types section. id: This is an optional field that assigns a unique identifier to the link. It's helpful for referring to the link within the schema.  ","version":"Next","tagName":"h3"},{"title":"Example​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#example-1","content":" The following example illustrates how to utilize the @link directive to incorporate a Protocol Buffers (.proto) file for a gRPC service into your GraphQL schema.  schema @server(port: 8000) @upstream( baseURL: &quot;http://news.local&quot; httpCache: 42 batch: {delay: 10} ) @link( id: &quot;news&quot; src: &quot;./src/grpc/news.proto&quot; type: Protobuf ) { query: Query } type Query { news: NewsData! @grpc(method: &quot;news.NewsService.GetAllNews&quot;) } type News { id: Int title: String body: String postImage: String } type NewsData { news: [News]! }   ","version":"Next","tagName":"h3"},{"title":"Supported Types​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#supported-types","content":" The @link directive enriches your configuration by supporting the integration of external resources. Each link type is designed to serve a specific purpose, enhancing the functionality and flexibility of your schema. Below is a detailed overview of each supported link type:  ","version":"Next","tagName":"h3"},{"title":"Config​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#config","content":" The Config link type is essential for importing other configuration files. This feature enables a modular approach to schema management by allowing configurations from the imported file to override overlapping settings in the main schema. This functionality is useful in large projects, where maintaining a single monolithic schema file becomes impractical. By using Config, developers can split their schema configurations into manageable pieces, thus promoting better organization and scalability.  Example use case:  Modularizing schema configurations for different environments (development, staging, production).Reusing common configurations across multiple schema files.  ","version":"Next","tagName":"h3"},{"title":"Protobuf​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#protobuf","content":" The Protobuf link type integrates Protocol Buffers definitions by importing .proto files. This integration is crucial for Tailcall to communicate with gRPC services. By including .proto definitions, the GraphQL server can directly interact with gRPC services, allowing for efficient and type-safe communication.  For detailed integration steps and best practices, refer to the gRPC Integration Guide.  ","version":"Next","tagName":"h3"},{"title":"Script​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#script","content":" The Script link type allows the config to link to an external JavaScript file. This file can contain custom logic that is executed in response to HTTP request-response events. This feature enables developers to implement custom behaviors, such as adding headers to responses or filtering requests based on specific criteria.  Example script for adding a custom header to all outgoing requests:  function onRequest({request}) { // Add a custom header for all outgoing requests request.headers[&quot;X-Custom-Header&quot;] = &quot;Processed&quot; // Return the updated request return {request} }   ","version":"Next","tagName":"h3"},{"title":"Cert​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#cert","content":" The Cert link type is designed for importing SSL/TLS certificates, a crucial component for enabling HTTPS in your GraphQL server. This link type ensures that the server can expose connections over HTTPS.  tip When using the Cert link type, specify the path to the certificate file. Ensure the certificate is up-to-date and issued by a trusted certificate authority (CA) to avoid security warnings or connection issues.  Example use case:  Securing communication between the GraphQL server and clients.Enhancing privacy and security by encrypting data in transit.  ","version":"Next","tagName":"h3"},{"title":"Key​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#key","content":" The Key link type imports the private key associated with your SSL/TLS certificate, enabling HTTPS for your GraphQL server. The private key is a critical security element that decrypts information encrypted by the corresponding public key in the SSL/TLS certificate.  When configuring the Key link type, provide the path to your private key file. Ensure the private key matches the imported certificate specified by the Cert link above, and is protected by appropriate file permissions to maintain security.  ","version":"Next","tagName":"h3"},{"title":"Operation​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#operation","content":" The Operation link type connects your schema to a set of predefined, GraphQL spec-compliant queries and mutations. This functionality allows for the validation and optimization of these operations by the GraphQL server.  Each type serves a specific purpose, enabling the flexible integration of external resources into your GraphQL schema.  ","version":"Next","tagName":"h3"},{"title":"Htpasswd​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#htpasswd","content":" The Htpasswd link type allows the importation of an htpasswd file. This file is utilized to set up Basic authentication.  ","version":"Next","tagName":"h3"},{"title":"Jwks​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#jwks","content":" The Jwks link type enables the importation of a JWKS file. This file facilitates the provision of detailed access control through JWT authentication.  ","version":"Next","tagName":"h3"},{"title":"@modify Directive​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#modify-directive","content":" The @modify directive in GraphQL provides the flexibility to alter the attributes of a field or a node within your GraphQL schema. Here's how you can use this directive:  ","version":"Next","tagName":"h2"},{"title":"name​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#name-1","content":" You can rename a field or a node in your GraphQL schema using the name argument in the @modify directive. This can be helpful when the field name in your underlying data source doesn't match the desired field name in your schema. For instance:  type User { id: Int! @modify(name: &quot;userId&quot;) }   @modify(name: &quot;userId&quot;) informs GraphQL to present the field known as id in the underlying data source as userId in your schema.  ","version":"Next","tagName":"h3"},{"title":"omit​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#omit","content":" You can exclude a field or a node from your GraphQL schema using the omit argument in the @modify directive. This can be useful if you want to keep certain data hidden from the client. For instance:  type User { id: Int! @modify(omit: true) }   @modify(omit: true) instructs GraphQL to exclude the id field from the schema, making it inaccessible to the client.  tip @omit is a standalone directive and is an alias/shorthand for modify(omit: true) checkout documentation  ","version":"Next","tagName":"h3"},{"title":"@omit Directive​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#omit-directive","content":" Within a GraphQL schema, the @omit directive excludes fields or nodes from the generated schema, making them inaccessible through the GraphQL API. This directive is useful for hiding sensitive information or simplifying your API by removing unnecessary fields.  ","version":"Next","tagName":"h2"},{"title":"How it works​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#how-it-works-1","content":" When applied to a field or node, the @omit directive instructs the Tailcall not to include that field or node in the schema. This means that clients cannot query or mutate data in those fields.  ","version":"Next","tagName":"h3"},{"title":"Example​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#example-2","content":" Consider a scenario where you have a User type with an embedded Address type. If you want to exclude the Address type from the schema to simplify the API, you can use the @omit directive:  type Address { city: String street: String } type User { name: String address: Address @omit }   In this example, the address field will not be accessible or visible through the GraphQL API.  ","version":"Next","tagName":"h3"},{"title":"Comparison with modify​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#comparison-with-modify","content":" The @omit directive and @modify(omit: true) essentially serve the same purpose in excluding fields from the schema, but they differ in syntax and flexibility. In fact, one can consider @omit as a shorthand or alias for the more verbose @modify(omit: true).  @omit offers a concise way to directly exclude a field or node without additional arguments. @modify(omit: true), as part of the broader @modify directive, provides more options, such as field renaming through the name argument. This makes it a more flexible choice when you need more than field exclusion.  ","version":"Next","tagName":"h3"},{"title":"@protected Directive​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#protected-directive","content":" The @protected annotation designates a type or field as protected, meaning that a user must be authenticated to access that data.  type Query { protected: String! @protected protectedType: ProtectedType } type ProtectedType @protected { name: String! nested: String! }   important To utilize the @protected directive, you must link at least one authentication provider in the configuration using the @link directive (Htpasswd or Jwks).  ","version":"Next","tagName":"h2"},{"title":"How It Works​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#how-it-works-2","content":" When a field is annotated with @protected, an authentication check is performed upon receiving the request. Depending on the authentication result, either the requested data is provided in the response, or an authentication error is returned.If a type is annotated with @protected, all fields within that type inherit the protection, requiring user authentication for any field that's queried.  ","version":"Next","tagName":"h3"},{"title":"@rest Directive​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#rest-directive","content":" API orchestration is essential, yet not all can adopt GraphQL despite its benefits. The Tailcall DSL feature leverages GraphQL at compile time to generate REST endpoints, aligning with traditional API infrastructure like CDNs and Gateways.  ","version":"Next","tagName":"h2"},{"title":"Usage​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#usage-2","content":" method: Specifies the HTTP method (GET, POST, etc.).path: Sets the endpoint URL, with support for dynamic values from query arguments.query: Defines the query parameters as key-value pairs.  ","version":"Next","tagName":"h3"},{"title":"Example​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#example-3","content":" Define GraphQL types and queries, using the @rest directive to map fields to REST API endpoints.  schema.graphql  schema @upstream(baseURL: &quot;https://jsonplaceholder.typicode.com&quot;) @link(type: Operation, src: &quot;user-operation.graphql&quot;) { query: Query } type Query { user(id: Int!): User @rest(method: &quot;GET&quot;, path: &quot;/users/{{.args.id}}&quot;) } type User { id: Int! name: String! email: String! }   user-operation.graphql  query ($id: Int!) @rest(method: GET, path: &quot;/user/$id&quot;) { user(id: $id) { id name } }     This example demonstrates how to define a simple query to fetch user data from a REST endpoint using the @rest directive. By leveraging @rest, GraphQL can serve as a layer over RESTful services, combining REST's simplicity with GraphQL's flexibility.  ","version":"Next","tagName":"h3"},{"title":"@server Directive​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#server-directive","content":" The @server directive, applied at the schema level, provides a comprehensive set of server configurations. It dictates server behavior and helps tune Tailcall for a range of use-cases.  schema @server(...[ServerSettings]...){ query: Query mutation: Mutation }   In this templated structure, replace ...[ServerSettings]... with specific configurations tailored to your project's needs. Adjust and expand these settings as necessary.  The ServerSettings options and their details appear below.  ","version":"Next","tagName":"h2"},{"title":"workers​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#workers","content":" Setting workers to 32 means that the GraphQL server will use 32 worker threads.  schema @server(workers: 32) { query: Query mutation: Mutation }   This example sets the workers to 32, meaning the GraphQL server will use 32 worker threads.  ","version":"Next","tagName":"h3"},{"title":"port​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#port","content":" Setting the port to 8090 means that Tailcall will be accessible at http://localhost:8000.  schema @server(port: 8090) { query: Query mutation: Mutation }   This example sets the port to 8090, making Tailcall accessible at http://localhost:8090.  tip Always choose non-standard ports, avoiding typical ones like 80 or 8080. Make sure your chosen port is free.  ","version":"Next","tagName":"h3"},{"title":"headers​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#headers-3","content":" Allows intelligent configuration of the final response headers that's produced by Tailcall.  ","version":"Next","tagName":"h3"},{"title":"cacheControl​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#cachecontrol","content":" Activating the cacheControl configuration directs Tailcall to send Cache-Control headers in its responses. The max-age value in the header matches the lowest of the values in the responses that Tailcall receives from its upstream. By default, this is false, which means Tailcall does not set any header.  schema @server(headers: {cacheControl: true}) { query: Query mutation: Mutation }   ","version":"Next","tagName":"h3"},{"title":"custom​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#custom","content":" The custom is an array of key-value pairs. These headers get added to the response of every request made to the server. This can be useful for adding headers like Access-Control-Allow-Origin to allow cross-origin requests, or some headers like X-Allowed-Roles for use by downstream services.  schema @server( headers: { custom: [ {key: &quot;X-Allowed-Roles&quot;, value: &quot;admin,user&quot;} ] } ) { query: Query mutation: Mutation }   ","version":"Next","tagName":"h3"},{"title":"experimental​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#experimental","content":" When the experimental configuration is enabled, Tailcall can include headers starting with X- in its responses, which are sourced from its upstream. By default, this feature is disabled ([]), meaning Tailcall does not forward any such headers unless explicitly configured to do so.  schema @server( headers: {experimental: [&quot;X-Experimental-Header&quot;]} ) { query: Query mutation: Mutation }   ","version":"Next","tagName":"h3"},{"title":"setCookies​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#setcookies","content":" Enabling the setCookies option instructs Tailcall to include set-cookie headers in its responses, which are obtained from the headers of upstream responses.  schema @server(headers: {setCookies: true}) { query: Query mutation: Mutation }   ","version":"Next","tagName":"h3"},{"title":"cors​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#cors","content":" The cors configuration allows you to enable CORS on Tailcall. This is useful when you want to access Tailcall in the browser. Here is a simple configuration to get started with cors:  schema @server( headers: { cors: {allowHeaders: [&quot;*&quot;], allowOrigins: [&quot;*&quot;]} } ) { query: Query }   The above setting will enable CORS on the server for all headers, origins &amp; methods. You can further configure the cors settings to make it more secure with the following fields:  allowCredentials: Indicates whether the server allows credentials (e.g., cookies, authorization headers) to be sent in cross-origin requests.allowHeaders: A list of allowed headers in cross-origin requests. This can be used to specify custom headers that are allowed to be included in cross-origin requests.allowMethods: A list of allowed HTTP methods in cross-origin requests. These methods specify the actions that are permitted in cross-origin requests.allowOrigins: A list of origins that are allowed to access the server's resources in cross-origin requests. An origin can be a domain, a subdomain, or even 'null' for local file schemes.allowPrivateNetwork: Indicates whether requests from private network addresses are allowed in cross-origin requests. Private network addresses typically include IP addresses reserved for internal networks.exposeHeaders: A list of headers that the server exposes to the browser in cross-origin responses. Exposing certain headers allows client-side code to access them in the response.maxAge: The maximum time (in seconds) that the client should cache preflight OPTIONS requests to avoid sending excessive requests to the server.vary: A list of header names that indicate the values of which might cause the server's response to vary, potentially affecting caching.  schema @server( port: 8000 hostname: &quot;0.0.0.0&quot; headers: { cors: { allowCredentials: false allowHeaders: [&quot;Authorization&quot;] allowMethods: [POST, GET, OPTIONS] allowOrigins: [&quot;abc.xyz&quot;] allowPrivateNetwork: true exposeHeaders: [&quot;Content-Type&quot;] maxAge: 360 vary: [&quot;Origin&quot;] } } ) { query: Query }   ","version":"Next","tagName":"h3"},{"title":"vars​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#vars","content":" This configuration allows defining local variables for use during the server's operations. These variables are handy for storing constant configurations, secrets, or other shared information that operations might need.  schema @server( vars: {key: &quot;apiKey&quot;, value: &quot;YOUR_API_KEY_HERE&quot;} ) { query: Query mutation: Mutation } type Query { externalData: Data @http( path: &quot;/external-api/data&quot; headers: [ { key: &quot;Authorization&quot; value: &quot;Bearer {{.vars.apiKey}}&quot; } ] ) }   In the provided example, setting a variable named apiKey with a placeholder value of &quot;YOUR_API_KEY_HERE&quot; implies that whenever Tailcall fetches data from the externalData endpoint, it includes the apiKey in the Authorization header of the HTTP request.  tip Local variables, like apiKey, are instrumental in securing access to external services or providing a unified place for configurations. Ensure that sensitive information stored this way is well protected and not exposed unintentionally, if your GraphQL configuration is publicly accessible.  ","version":"Next","tagName":"h3"},{"title":"introspection​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#introspection","content":" This setting controls the server's allowance of introspection queries. Introspection, a core feature of GraphQL, allows clients to directly fetch schema information. This capability proves crucial for tools and client applications in comprehending the available types, fields, and operations. By default, the server enables this setting (true).  schema @server(introspection: false) { query: Query mutation: Mutation }   tip Although introspection is beneficial during development and debugging stages, consider disabling it in production environments. Turning off introspection in live deployments can enhance security by preventing potential attackers from discerning the schema and any associated business logic or data structures.  ","version":"Next","tagName":"h3"},{"title":"queryValidation​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#queryvalidation","content":" The queryValidation configuration determines if the server checks incoming GraphQL queries against the defined schema. Each query check ensures it matches the schema, preventing errors from incorrect or malformed queries. In some situations, you might want to disable it, notably to enhance server performance at the cost of these checks. This defaults to false if not specified.  schema @server(queryValidation: true) { query: Query mutation: Mutation }   The example above sets queryValidation to true, enabling the validation phase for incoming queries.  tip Enable this in the development environment to ensure the queries sent are correct and validated. In the production environment, consider disabling it for improved performance.  ","version":"Next","tagName":"h3"},{"title":"responseValidation​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#responsevalidation","content":" Tailcall can automatically infer the schema of the HTTP endpoints for you. This information can check responses received from the upstream services. Enabling this setting allows you to do that. If not specified, the default setting for responseValidation is false.  schema @server(responseValidation: true) { query: Query mutation: Mutation }   tip Disabling this setting will offer major performance improvements, but at the potential expense of data integrity.  ","version":"Next","tagName":"h3"},{"title":"globalResponseTimeout​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#globalresponsetimeout","content":" The globalResponseTimeout configuration sets the max duration a query can run before the server terminates it. Essentially, it acts as a safeguard against long-running queries that could strain resources or pose security concerns.  If not explicitly defined, there might be a system-specific or default value that applies.  schema @server(globalResponseTimeout: 5000) { query: Query mutation: Mutation }   In this given example, setting the globalResponseTimeout to 5000 milliseconds, or 5 seconds, means any query execution taking longer than this duration will be automatically terminated by  tip Setting an appropriate response timeout in production environments is crucial. This optimizes resource use and serves as a security measure against potential denial-of-service attacks, where adversaries might run complex queries to exhaust server resources.  ","version":"Next","tagName":"h3"},{"title":"version​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#version","content":" The server uses the HTTP version. If not specified, the default value is HTTP1. The available options are HTTP1 and HTTP2.  schema @server(version: HTTP2) { query: Query mutation: Mutation }   ","version":"Next","tagName":"h3"},{"title":"cert​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#cert-1","content":" The path to certificate(s) for running the server over HTTP2 (HTTPS). If not specified, the default value is null.  schema @server(cert: &quot;./cert.pem&quot;) { query: Query mutation: Mutation }   tip The certificate can be of any extension, but it's highly recommended to use standards (pem, crt, key).  ","version":"Next","tagName":"h3"},{"title":"key​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#key-1","content":" The path to the key for running the server over HTTP2 (HTTPS). If not specified, the default value is null.  schema @server(key: &quot;./key.pem&quot;) { query: Query mutation: Mutation }   tip The key can be of any extension, but it's highly recommended to use standards (pem, crt, key).  ","version":"Next","tagName":"h3"},{"title":"showcase​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#showcase","content":" The @server directive's showcase option allows for hands-on experimentation with server configurations in a controlled environment. This feature simplifies the process of exploring and testing different settings.  schema @server(showcase: true) { query: Query }   ","version":"Next","tagName":"h3"},{"title":"batchRequests​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#batchrequests","content":" Batching in GraphQL combines requests into one, reducing server round trips.  schema @server( port: 8000 batchRequests: true )   tip Batching can improve performance but may introduce latency if one request in the batch takes longer. It also makes network traffic debugging harder.  ","version":"Next","tagName":"h3"},{"title":"dedupe​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#dedupe","content":" A boolean flag, if set to true, will enable deduplication of IO operations to enhance performance. This flag prevents duplicate IO requests from being executed concurrently, reducing resource load. If not specified, this feature defaults to false.  schema @server( port: 8000 dedupe: true )   ","version":"Next","tagName":"h3"},{"title":"@telemetry Directive​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#telemetry-directive","content":" The @telemetry directive facilitates seamless integration with OpenTelemetry, enhancing the observability of your GraphQL services powered by Tailcall. By leveraging this directive, developers gain access to valuable insights into the performance and behavior of their applications.  ","version":"Next","tagName":"h2"},{"title":"Traces​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#traces","content":" Here are the traces that are captured by the @telemetry directive:  Trace Name\tDescriptionrequest\tCaptures the span for processing the HTTP request on the server side, providing foundational observability. graphQL\tOnly for GraphQL ingress. Span for processing GraphQL call REST &lt;http_method&gt; &lt;http_route&gt;\tOnly for REST ingress. Span for processing REST API call &lt;field_name&gt;\tDenotes spans for fields with defined resolvers, offering insights into field names and execution times for resolver logic. &lt;expr_name&gt;\tNested within the &lt;field_name&gt; spans, these granulated spans detail the execution of expressions in resolving a field, highlighting the hierarchical execution pattern of nested expressions. upstream_request\tRequest that were made from tailcall service to upstream  ","version":"Next","tagName":"h3"},{"title":"Metrics​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#metrics","content":" The @telemetry directive also captures the following metrics:  Metric\tDescriptioncache.hit_rate\tReflects the cache hit rate for the cache powered by the @cache directive http.server.request.count\tCounts the number of incoming requests made to specific route. Optionally enriched with selected headers by requestHeaders http.client.request.count\tCounts the number of outgoing requests to specific upstream  ","version":"Next","tagName":"h3"},{"title":"export​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#export","content":" The export field defines how the open-telemetry data should be exported and in which format. The following are the supported formats:  ","version":"Next","tagName":"h3"},{"title":"otlp​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#otlp","content":" Utilizes the OTLP format to export telemetry data to backend systems, supported by most modern tracing and analytics platforms. Here is an example using [honeycomb.io]:  schema @telemetry( export: { otlp: { url: &quot;https://api.honeycomb.io:443&quot; headers: [ { key: &quot;x-honeycomb-team&quot; value: &quot;{{.env.HONEYCOMB_API_KEY}}&quot; } {key: &quot;x-honeycomb-dataset&quot;, value: &quot;tailcall&quot;} ] } } ) { query: Query }   You can configure the OTLP exporter with the following options:  Field\tDescriptionurl\tDefines the URL for the OTLP Collector. headers\tSets additional headers for requests to the OTLP Collector.  ","version":"Next","tagName":"h3"},{"title":"prometheus​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#prometheus","content":" Facilitates metrics export in a Prometheus compatible format, providing a dedicated endpoint for metrics.  schema @telemetry(export: {prometheus: {path: &quot;/metrics&quot;}}) { query: Query }   You can configure the Prometheus exporter with the following options:  Field\tDescriptionpath\tDesignates the endpoint path for Prometheus metrics, defaulting to /metrics. format\tControls the format viz. text or protobuf, for sending data to Prometheus.  ","version":"Next","tagName":"h3"},{"title":"stdout​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#stdout","content":" Outputs all telemetry data to stdout, ideal for testing or local development environments.  schema @telemetry(export: {stdout: {pretty: true}}) { query: Query }   You can configure the stdout exporter with the following options:  Field\tDescriptionpretty\tEnables formatted output of telemetry data for enhanced readability.  ","version":"Next","tagName":"h3"},{"title":"requestHeaders​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#requestheaders","content":" Specifies list of headers of ingress request the value of which will be sent to the telemetry as attributes.  schema @telemetry(requestHeaders: [&quot;X-User-Id&quot;]) { query: Query }   ","version":"Next","tagName":"h3"},{"title":"apollo​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#apollo","content":" Facilitates seamless integration with Apollo Studio, enhancing the observability of GraphQL services. By leveraging this field, developers gain access to valuable insights into the performance and behavior of their GraphQL APIs.  schema @telemetry( export: { otlp: { api_key: &quot;{{.env.APOLLO_API_KEY}}&quot; graph_ref: &quot;graph-id@current&quot; platform: &quot;website.com&quot; version: &quot;1.0.0&quot; } } ) { query: Query }   You can configure the apollo exporter with the following options:  Field\tDescriptionapi_key\tThe API Key generated from Apollo Studio. graph_ref\tThe Graph Ref, which is the graph_id and the variant concatenated using @(i.e. &lt;graph_id&gt;@&lt;variant&gt;) platform\tAn arbitrary value which can contain the name of your website or some other value to identify your deployment uniqely, in case you have multiple deployments. version\tVersion of Apollo which is being used.  By integrating the @telemetry directive into your GraphQL schema, you empower your development teams with critical insights into application performance, enabling proactive optimization and maintenance.  ","version":"Next","tagName":"h3"},{"title":"@upstream Directive​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#upstream-directive","content":" The upstream directive enables control over specific aspects of the upstream server connection, including settings such as connection timeouts, keep-alive intervals, and more. The system applies default values if you do not specify them.  schema @upstream(...[UpstreamSetting]...){ query: Query mutation: Mutation }   The document below details the options for UpstreamSetting.  ","version":"Next","tagName":"h2"},{"title":"poolIdleTimeout​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#poolidletimeout","content":" The connection pool waits for this duration in seconds before closing idle connections.  schema @upstream( poolIdleTimeout: 60 baseURL: &quot;http://jsonplaceholder.typicode.com&quot; ) { query: Query mutation: Mutation }   ","version":"Next","tagName":"h3"},{"title":"poolMaxIdlePerHost​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#poolmaxidleperhost","content":" The max number of idle connections each host will maintain.  schema @upstream( poolMaxIdlePerHost: 60 baseURL: &quot;http://jsonplaceholder.typicode.com&quot; ) { query: Query mutation: Mutation }   ","version":"Next","tagName":"h3"},{"title":"keepAliveInterval​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#keepaliveinterval","content":" The time in seconds between each keep-alive message sent to maintain the connection.  schema @upstream( keepAliveInterval: 60 baseURL: &quot;http://jsonplaceholder.typicode.com&quot; ) { query: Query mutation: Mutation }   ","version":"Next","tagName":"h3"},{"title":"keepAliveTimeout​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#keepalivetimeout","content":" The time in seconds that the connection will wait for a keep-alive message before closing.  schema @upstream( keepAliveTimeout: 60 baseURL: &quot;http://jsonplaceholder.typicode.com&quot; ) { query: Query mutation: Mutation }   ","version":"Next","tagName":"h3"},{"title":"keepAliveWhileIdle​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#keepalivewhileidle","content":" A boolean value that determines whether to send keep-alive messages while the connection is idle.  schema @upstream( keepAliveWhileIdle: false baseURL: &quot;http://jsonplaceholder.typicode.com&quot; ) { query: Query mutation: Mutation }   ","version":"Next","tagName":"h3"},{"title":"proxy​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#proxy","content":" The proxy setting defines an intermediary server that routes upstream requests before they reach their intended endpoint. By specifying a proxy URL, you introduce a layer, enabling custom routing and security policies.  schema @upstream( proxy: {url: &quot;http://localhost:3000&quot;} baseURL: &quot;http://jsonplaceholder.typicode.com&quot; ) { query: Query mutation: Mutation }   In the provided example, we've set the proxy's url to &quot;http://localhost:3000&quot;. This configuration ensures that all requests aimed at the designated baseURL first go through this proxy. To illustrate, if the baseURL is &quot;http://jsonplaceholder.typicode.com&quot;, any request targeting it initially goes to &quot;http://localhost:3000&quot; before the proxy redirects it to its final destination.  ","version":"Next","tagName":"h3"},{"title":"connectTimeout​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#connecttimeout","content":" The time in seconds that the connection will wait for a response before timing out.  schema @upstream( connectTimeout: 60 baseURL: &quot;http://jsonplaceholder.typicode.com&quot; ) { query: Query mutation: Mutation }   ","version":"Next","tagName":"h3"},{"title":"timeout​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#timeout","content":" The max time in seconds that the connection will wait for a response.  schema @upstream( timeout: 60 baseURL: &quot;http://jsonplaceholder.typicode.com&quot; ) { query: Query mutation: Mutation }   ","version":"Next","tagName":"h3"},{"title":"tcpKeepAlive​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#tcpkeepalive","content":" The time in seconds between each TCP keep-alive message sent to maintain the connection.  schema @upstream( tcpKeepAlive: 60 baseURL: &quot;http://jsonplaceholder.typicode.com&quot; ) { query: Query mutation: Mutation }   ","version":"Next","tagName":"h3"},{"title":"userAgent​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#useragent","content":" The User-Agent header value for HTTP requests.  schema @upstream( userAgent: &quot;Tailcall/1.0&quot; baseURL: &quot;http://jsonplaceholder.typicode.com&quot; ) { query: Query mutation: Mutation }   ","version":"Next","tagName":"h3"},{"title":"allowedHeaders​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#allowedheaders","content":" The allowedHeaders configuration defines a set of whitelisted HTTP headers that can be forwarded to upstream services during requests. Without specifying allowedHeaders, the system will not forward any incoming headers to upstream services, offering an extra security layer but potentially limiting necessary data flow. Tailcall compares the provided whitelisted headers in a case-insensitive format.  schema @upstream( allowedHeaders: [&quot;Authorization&quot;, &quot;X-Api-Key&quot;] ) { query: Query mutation: Mutation }   In the example above, the configuration for allowedHeaders permits Authorization and X-Api-Key headers. Thus, requests with these headers will forward them to upstream services; the system ignores all others. This configuration ensures communication of the expected headers to dependent services, emphasizing security and consistency.  ","version":"Next","tagName":"h3"},{"title":"baseURL​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#baseurl-3","content":" This refers to the default base URL for your APIs. If it's not explicitly mentioned in the @upstream directive, then each @http directive must specify its own baseURL. If neither @upstream nor @http provides a baseURL, it results in a compilation error.  schema @upstream( baseURL: &quot;http://jsonplaceholder.typicode.com&quot; ) { query: Query mutation: Mutation }   In this representation, http://jsonplaceholder.typicode.com serves as the baseURL. Thus, all API calls made by @http prepend this URL to their respective paths.  tip Ensure that your base URL remains free from specific path segments. GOOD: @upstream(baseURL: http://jsonplaceholder.typicode.com)BAD: @upstream(baseURL: http://jsonplaceholder.typicode.com/api)  ","version":"Next","tagName":"h3"},{"title":"httpCache​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#httpcache","content":" When httpCache passed with value greater than 0 it directs Tailcall to use HTTP caching mechanisms, following the HTTP Caching RFC to enhance performance by minimizing unnecessary data fetches. If left unspecified, this feature defaults to 0 disabling the caching mechanism.  schema @upstream(httpCache: 42) { query: Query mutation: Mutation }   ","version":"Next","tagName":"h3"},{"title":"Tips​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#tips","content":" Use batching when other optimization techniques fail to resolve performance issues.Apply batching and thoroughly assess its impact.Understand that batching may make debugging more challenging.  ","version":"Next","tagName":"h3"},{"title":"batch​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#batch-1","content":" An object that specifies the batch settings, including maxSize (the max size of the batch), delay (the delay in milliseconds between each batch), and headers (an array of HTTP headers that the batch will include).  schema @upstream( batch: { maxSize: 1000 delay: 10 headers: [&quot;X-Server&quot;, &quot;Authorization&quot;] } ) { query: Query mutation: Mutation }   ","version":"Next","tagName":"h3"},{"title":"onRequest​","type":1,"pageTitle":"GraphQL Configuration","url":"/docs/tailcall-dsl-graphql-custom-directives/#onrequest-1","content":" Similar to the @http property, this accepts a string value representing a middleware function defined in a JavaScript file. It intercepts all outgoing HTTP requests from the server. This interceptor, written in JavaScript, can be used to modify outgoing requests and also generate artificial responses to customize the behavior of the GraphQL server.  schema @upstream(onRequest: 'someFunctionName') @link(type: Script, src: &quot;path_to/worker.js&quot;) { query: Query mutation: Mutation }  ","version":"Next","tagName":"h3"}],"options":{"highlightResult":true,"id":"default"}}