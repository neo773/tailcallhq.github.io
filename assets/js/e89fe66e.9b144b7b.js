"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[8967],{9632:(e,r,n)=>{n.r(r),n.d(r,{assets:()=>o,contentTitle:()=>c,default:()=>m,frontMatter:()=>t,metadata:()=>s,toc:()=>l});var i=n(4848),a=n(8453);const t={title:"Performing Micro Benchmarks in Tailcall",sidebar_label:"Micro Benchmarks",description:"Learn how to perform micro benchmarks for your APIs with Tailcall. Optimize performance with our detailed guides and tools."},c=void 0,s={id:"micro-benchmark",title:"Performing Micro Benchmarks in Tailcall",description:"Learn how to perform micro benchmarks for your APIs with Tailcall. Optimize performance with our detailed guides and tools.",source:"@site/developers/micro-benchmark.md",sourceDirName:".",slug:"/micro-benchmark",permalink:"/developers/micro-benchmark",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{title:"Performing Micro Benchmarks in Tailcall",sidebar_label:"Micro Benchmarks",description:"Learn how to perform micro benchmarks for your APIs with Tailcall. Optimize performance with our detailed guides and tools."},sidebar:"tutorialSidebar",previous:{title:"Integration Testing",permalink:"/developers/integration-testing"},next:{title:"Mutability",permalink:"/developers/mutability"}},o={},l=[{value:"Running Benchmarks",id:"running-benchmarks",level:2},{value:"Comparing Benchmarks",id:"comparing-benchmarks",level:2}];function h(e){const r={a:"a",code:"code",h2:"h2",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,a.R)(),...e.components},{Head:n}=r;return n||function(e,r){throw new Error("Expected "+(r?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Head",!0),(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n,{children:(0,i.jsx)("title",{children:"Micro Benchmarks | Tailcall"})}),"\n",(0,i.jsxs)(r.p,{children:["Benchmarking infrastructure is crucial for Tailcall. We run a suite of benchmarks on our continuous integration (CI) system for every commit made to the ",(0,i.jsx)(r.code,{children:"main"})," branch using ",(0,i.jsx)(r.a,{href:"https://bheisler.github.io/criterion.rs/book/",children:"Criterion"}),"."]}),"\n",(0,i.jsx)(r.h2,{id:"running-benchmarks",children:"Running Benchmarks"}),"\n",(0,i.jsxs)(r.ol,{children:["\n",(0,i.jsxs)(r.li,{children:["\n",(0,i.jsxs)(r.p,{children:["Install ",(0,i.jsx)(r.a,{href:"https://crates.io/crates/cargo-criterion",children:"cargo-criterion"})," and ",(0,i.jsx)(r.a,{href:"https://crates.io/crates/rust-script",children:"rust-script"}),":"]}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-bash",children:"cargo install cargo-criterion rust-script\n"})}),"\n"]}),"\n",(0,i.jsxs)(r.li,{children:["\n",(0,i.jsx)(r.p,{children:"Execute the benchmarks:"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-bash",children:"cargo bench\n"})}),"\n",(0,i.jsx)(r.p,{children:"This command will run all benchmarks and display the results. To run a specific benchmark you could modify the command and pass a pattern to the command:"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-bash",children:"cargo bench -- 'foo.*bar'\n"})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(r.h2,{id:"comparing-benchmarks",children:"Comparing Benchmarks"}),"\n",(0,i.jsx)(r.p,{children:"To facilitate benchmark comparison, we have developed a Rust script capable of contrasting the outcomes of two benchmarks."}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-bash",children:"# Checkout the base branch\ngit checkout main\n\n# Run the benchmarks for the main branch and store the result in a file\ncargo bench --message-format=json > main.json\n\n# Checkout the feature branch\ngit checkout feature\n\n# Run the benchmarks again in your feature branch\ncargo bench --message-format=json > feature.json\n\n# Perform a comparison check between the two branches\n./scripts/criterion_compare.rs main.json feature.json table\n"})}),"\n",(0,i.jsxs)(r.p,{children:["If the benchmarks indicate a degradation exceeding ",(0,i.jsx)(r.strong,{children:"10%"}),", the script will terminate with an error. You can refer to the automatically generated ",(0,i.jsx)(r.code,{children:"benches/benchmark.md"})," file to identify which benchmarks underperformed and investigate the corresponding code changes before submitting a pull request."]})]})}function m(e={}){const{wrapper:r}={...(0,a.R)(),...e.components};return r?(0,i.jsx)(r,{...e,children:(0,i.jsx)(h,{...e})}):h(e)}}}]);